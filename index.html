<!DOCTYPE html>
<html lang="en">

<head>
    <title>MyArxiv</title>
    <meta charset="utf-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="robots" content="noindex, nofollow"/>
    <meta name="viewport" content="width=device-width, initial-scale=1"/>
    <link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
    <link href="index.css" rel="stylesheet"/>
    <link href="https://cdn.jsdelivr.net/npm/remixicon@2.5.0/fonts/remixicon.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"
            integrity="sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx"
            crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js"
            integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR"
            crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function () {
            renderMathInElement(document.body, {
                // customised options
                // • auto-render specific keys, e.g.:
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '$', right: '$', display: false},
                    {left: '\\(', right: '\\)', display: false},
                    {left: '\\[', right: '\\]', display: true},
                    {left: "\\begin{equation}", right: "\\end{equation}", display: true},
                    {left: "\\begin{align}", right: "\\end{align}", display: true},
                    {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
                    {left: "\\begin{gather}", right: "\\end{gather}", display: true},
                    {left: "\\begin{CD}", right: "\\end{CD}", display: true},
                ],
                // • rendering keys, e.g.:
                throwOnError: false
            });
        });
    </script>
</head>

<body>
<section class="header-container">
    <div style="display:flex; justify-content:space-between; align-items:flex-end;">
        <div>
            <div class="header-title">
                MyArxiv
            </div>
        </div>

        <div class=icons>
            <label class="theme-switch" for="checkbox">
                <input type="checkbox" id="checkbox"/>
                <i id="theme-icon" class="ri-moon-line" style="font-size: 32px" rel="noopener noreferrer"></i>
            </label>
        </div>
    </div>
</section>

    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-26T00:00:00Z">2024-03-26</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LISA: Layerwise Importance Sampling for Memory-Efficient Large Language
  Model Fine-Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17919v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17919v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Pan, Xiang Liu, Shizhe Diao, Renjie Pi, Jipeng Zhang, Chi Han, Tong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The machine learning community has witnessed impressive advancements since
the first appearance of large language models (LLMs), yet their huge memory
consumption has become a major roadblock to large-scale training. Parameter
Efficient Fine-Tuning techniques such as Low-Rank Adaptation (LoRA) have been
proposed to alleviate this problem, but their performance still fails to match
full parameter training in most large-scale fine-tuning settings. Attempting to
complement this deficiency, we investigate layerwise properties of LoRA on
fine-tuning tasks and observe an uncommon skewness of weight norms across
different layers. Utilizing this key observation, a surprisingly simple
training strategy is discovered, which outperforms both LoRA and full parameter
training in a wide range of settings with memory costs as low as LoRA. We name
it Layerwise Importance Sampled AdamW (LISA), a promising alternative for LoRA,
which applies the idea of importance sampling to different layers in LLMs and
randomly freeze most middle layers during optimization. Experimental results
show that with similar or less GPU memory consumption, LISA surpasses LoRA or
even full parameter tuning in downstream fine-tuning tasks, where LISA
consistently outperforms LoRA by over $11\%$-$37\%$ in terms of MT-Bench
scores. On large models, specifically LLaMA-2-70B, LISA achieves on-par or
better performance than LoRA on MT-Bench, GSM8K, and PubMedQA, demonstrating
its effectiveness across different domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Unreasonable Ineffectiveness of the Deeper Layers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17887v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17887v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrey Gromov, Kushal Tirumala, Hassan Shapourian, Paolo Glorioso, Daniel A. Roberts
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We empirically study a simple layer-pruning strategy for popular families of
open-weight pretrained LLMs, finding minimal degradation of performance on
different question-answering benchmarks until after a large fraction (up to
half) of the layers are removed. To prune these models, we identify the optimal
block of layers to prune by considering similarity across layers; then, to
"heal" the damage, we perform a small amount of finetuning. In particular, we
use parameter-efficient finetuning (PEFT) methods, specifically quantization
and Low Rank Adapters (QLoRA), such that each of our experiments can be
performed on a single A100 GPU. From a practical perspective, these results
suggest that layer pruning methods can complement other PEFT strategies to
further reduce computational resources of finetuning on the one hand, and can
improve the memory and latency of inference on the other hand. From a
scientific perspective, the robustness of these LLMs to the deletion of layers
implies either that current pretraining methods are not properly leveraging the
parameters in the deeper layers of the network or that the shallow layers play
a critical role in storing knowledge.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 + 10 pages, 5 + 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring LLMs as a Source of Targeted Synthetic Textual Data to
  Minimize High Confidence Misclassifications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17860v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17860v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philip Lippmann, Matthijs Spaan, Jie Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural Language Processing (NLP) models optimized for predictive performance
often make high confidence errors and suffer from vulnerability to adversarial
and out-of-distribution data. Existing work has mainly focused on mitigation of
such errors using either humans or an automated approach. In this study, we
explore the usage of large language models (LLMs) for data augmentation as a
potential solution to the issue of NLP models making wrong predictions with
high confidence during classification tasks. We compare the effectiveness of
synthetic data generated by LLMs with that of human data obtained via the same
procedure. For mitigation, humans or LLMs provide natural language
characterizations of high confidence misclassifications to generate synthetic
data, which are then used to extend the training set. We conduct an extensive
evaluation of our approach on three classification tasks and demonstrate its
effectiveness in reducing the number of high confidence misclassifications
present in the model, all while maintaining the same level of accuracy.
Moreover, we find that the cost gap between humans and LLMs surpasses an order
of magnitude, as LLMs attain human-like performance while being more scalable.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ChroniclingAmericaQA: A Large-scale Question Answering <span class="highlight-title">Dataset</span> based on
  Historical American Newspaper Pages <span class="chip">SIGIR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17859v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17859v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bhawna Piryani, Jamshid Mozafari, Adam Jatowt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Question answering (QA) and Machine Reading Comprehension (MRC) tasks have
significantly advanced in recent years due to the rapid development of deep
learning techniques and, more recently, large language models. At the same
time, many benchmark datasets have become available for QA and MRC tasks.
However, most existing large-scale benchmark datasets have been created
predominantly using synchronous document collections like Wikipedia or the Web.
Archival document collections, such as historical newspapers, contain valuable
information from the past that is still not widely used to train large language
models. To further contribute to advancing QA and MRC tasks and to overcome the
limitation of previous datasets, we introduce ChroniclingAmericaQA, a
large-scale dataset with 485K question-answer pairs created based on the
historical newspaper collection Chronicling America. Our dataset is constructed
from a subset of the Chronicling America newspaper collection spanning 120
years. One of the significant challenges for utilizing digitized historical
newspaper collections is the low quality of OCR text. Therefore, to enable
realistic testing of QA models, our dataset can be used in three different
ways: answering questions from raw and noisy content, answering questions from
cleaner, corrected version of the content, as well as answering questions from
scanned images of newspaper pages. This and the fact that ChroniclingAmericaQA
spans the longest time period among available QA datasets make it quite a
unique and useful resource.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at SIGIR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Verbing Weirds Language (Models): Evaluation of English Zero-Derivation
  in Five LLMs <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17856v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17856v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David R. Mortensen, Valentina Izrailevitch, Yunze Xiao, Hinrich Schütze, Leonie Weissweiler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lexical-syntactic flexibility, in the form of conversion (or zero-derivation)
is a hallmark of English morphology. In conversion, a word with one part of
speech is placed in a non-prototypical context, where it is coerced to behave
as if it had a different part of speech. However, while this process affects a
large part of the English lexicon, little work has been done to establish the
degree to which language models capture this type of generalization. This paper
reports the first study on the behavior of large language models with reference
to conversion. We design a task for testing lexical-syntactic flexibility --
the degree to which models can generalize over words in a construction with a
non-prototypical part of speech. This task is situated within a natural
language inference paradigm. We test the abilities of five language models --
two proprietary models (GPT-3.5 and GPT-4), three open-source models (Mistral
7B, Falcon 40B, and Llama 2 70B). We find that GPT-4 performs best on the task,
followed by GPT-3.5, but that the open source language models are also able to
perform it and that the 7B parameter Mistral displays as little difference
between its baseline performance on the natural language inference task and the
non-prototypical syntactic category task, as the massive GPT-4.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Using Domain Knowledge to Guide Dialog Structure Induction via Neural
  Probabilistic Soft Logic 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17853v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17853v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Connor Pryor, Quan Yuan, Jeremiah Liu, Mehran Kazemi, Deepak Ramachandran, Tania Bedrax-Weiss, Lise Getoor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dialog Structure Induction (DSI) is the task of inferring the latent dialog
structure (i.e., a set of dialog states and their temporal transitions) of a
given goal-oriented dialog. It is a critical component for modern dialog system
design and discourse analysis. Existing DSI approaches are often purely
data-driven, deploy models that infer latent states without access to domain
knowledge, underperform when the training corpus is limited/noisy, or have
difficulty when test dialogs exhibit distributional shifts from the training
domain. This work explores a neural-symbolic approach as a potential solution
to these problems. We introduce Neural Probabilistic Soft Logic Dialogue
Structure Induction (NEUPSL DSI), a principled approach that injects symbolic
knowledge into the latent space of a generative neural model. We conduct a
thorough empirical investigation on the effect of NEUPSL DSI learning on hidden
representation quality, few-shot learning, and out-of-domain generalization
performance. Over three dialog structure induction datasets and across
unsupervised and semi-supervised settings for standard and cross-domain
generalization, the injection of symbolic knowledge using NEUPSL DSI provides a
consistent boost in performance over the canonical baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ArabicaQA: A Comprehensive <span class="highlight-title">Dataset</span> for Arabic Question Answering <span class="chip">SIGIR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17848v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17848v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdelrahman Abdallah, Mahmoud Kasem, Mahmoud Abdalla, Mohamed Mahmoud, Mohamed Elkasaby, Yasser Elbendary, Adam Jatowt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we address the significant gap in Arabic natural language
processing (NLP) resources by introducing ArabicaQA, the first large-scale
dataset for machine reading comprehension and open-domain question answering in
Arabic. This comprehensive dataset, consisting of 89,095 answerable and 3,701
unanswerable questions created by crowdworkers to look similar to answerable
ones, along with additional labels of open-domain questions marks a crucial
advancement in Arabic NLP resources. We also present AraDPR, the first dense
passage retrieval model trained on the Arabic Wikipedia corpus, specifically
designed to tackle the unique challenges of Arabic text retrieval. Furthermore,
our study includes extensive benchmarking of large language models (LLMs) for
Arabic question answering, critically evaluating their performance in the
Arabic language context. In conclusion, ArabicaQA, AraDPR, and the benchmarking
of LLMs in Arabic question answering offer significant advancements in the
field of Arabic NLP. The dataset and code are publicly accessible for further
research https://github.com/DataScienceUIBK/ArabicaQA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at SIGIR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hierarchical Open-Vocabulary 3D Scene Graphs for Language-Grounded Robot
  Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17846v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17846v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdelrhman Werby, Chenguang Huang, Martin Büchner, Abhinav Valada, Wolfram Burgard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent open-vocabulary robot mapping methods enrich dense geometric maps with
pre-trained visual-language features. While these maps allow for the prediction
of point-wise saliency maps when queried for a certain language concept,
large-scale environments and abstract queries beyond the object level still
pose a considerable hurdle, ultimately limiting language-grounded robotic
navigation. In this work, we present HOV-SG, a hierarchical open-vocabulary 3D
scene graph mapping approach for language-grounded robot navigation. Leveraging
open-vocabulary vision foundation models, we first obtain state-of-the-art
open-vocabulary segment-level maps in 3D and subsequently construct a 3D scene
graph hierarchy consisting of floor, room, and object concepts, each enriched
with open-vocabulary features. Our approach is able to represent multi-story
buildings and allows robotic traversal of those using a cross-floor Voronoi
graph. HOV-SG is evaluated on three distinct datasets and surpasses previous
baselines in open-vocabulary semantic accuracy on the object, room, and floor
level while producing a 75% reduction in representation size compared to dense
open-vocabulary maps. In order to prove the efficacy and generalization
capabilities of HOV-SG, we showcase successful long-horizon
language-conditioned robot navigation within real-world multi-storage
environments. We provide code and trial video data at http://hovsg.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code and video are available at http://hovsg.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Graph Language Model (GLM): A new graph-based approach to detect social
  instabilities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17816v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17816v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wallyson Lemes de Oliveira, Vahid Shamsaddini, Ali Ghofrani, Rahul Singh Inda, Jithendra Sai Veeramaneni, Étienne Voutaz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This scientific report presents a novel methodology for the early prediction
of important political events using News datasets. The methodology leverages
natural language processing, graph theory, clique analysis, and semantic
relationships to uncover hidden predictive signals within the data. Initially,
we designed a preliminary version of the method and tested it on a few events.
This analysis revealed limitations in the initial research phase. We then
enhanced the model in two key ways: first, we added a filtration step to only
consider politically relevant news before further processing; second, we
adjusted the input features to make the alert system more sensitive to
significant spikes in the data. After finalizing the improved methodology, we
tested it on eleven events including US protests, the Ukraine war, and French
protests. Results demonstrate the superiority of our approach compared to
baseline methods. Through targeted refinements, our model can now provide
earlier and more accurate predictions of major political events based on subtle
patterns in news data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Are Compressed Language Models Less Subgroup Robust? <span class="chip">EMNLP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17811v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17811v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leonidas Gee, Andrea Zugarini, Novi Quadrianto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To reduce the inference cost of large language models, model compression is
increasingly used to create smaller scalable models. However, little is known
about their robustness to minority subgroups defined by the labels and
attributes of a dataset. In this paper, we investigate the effects of 18
different compression methods and settings on the subgroup robustness of BERT
language models. We show that worst-group performance does not depend on model
size alone, but also on the compression method used. Additionally, we find that
model compression does not always worsen the performance on minority subgroups.
Altogether, our analysis serves to further research into the subgroup
robustness of model compression.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The 2023 Conference on Empirical Methods in Natural Language
  Processing (EMNLP 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Have Faith in Faithfulness: Going Beyond Circuit Overlap When Finding
  Model Mechanisms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17806v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17806v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Hanna, Sandro Pezzelle, Yonatan Belinkov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many recent language model (LM) interpretability studies have adopted the
circuits framework, which aims to find the minimal computational subgraph, or
circuit, that explains LM behavior on a given task. Most studies determine
which edges belong in a LM's circuit by performing causal interventions on each
edge independently, but this scales poorly with model size. Edge attribution
patching (EAP), gradient-based approximation to interventions, has emerged as a
scalable but imperfect solution to this problem. In this paper, we introduce a
new method - EAP with integrated gradients (EAP-IG) - that aims to better
maintain a core property of circuits: faithfulness. A circuit is faithful if
all model edges outside the circuit can be ablated without changing the model's
performance on the task; faithfulness is what justifies studying circuits,
rather than the full model. Our experiments demonstrate that circuits found
using EAP are less faithful than those found using EAP-IG, even though both
have high node overlap with circuits found previously using causal
interventions. We conclude more generally that when using circuits to compare
the mechanisms models use to solve tasks, faithfulness, not overlap, is what
should be measured.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Text-to-Image Consistency via Automatic <span class="highlight-title">Prompt</span> Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17804v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17804v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oscar Mañas, Pietro Astolfi, Melissa Hall, Candace Ross, Jack Urbanek, Adina Williams, Aishwarya Agrawal, Adriana Romero-Soriano, Michal Drozdzal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Impressive advances in text-to-image (T2I) generative models have yielded a
plethora of high performing models which are able to generate aesthetically
appealing, photorealistic images. Despite the progress, these models still
struggle to produce images that are consistent with the input prompt,
oftentimes failing to capture object quantities, relations and attributes
properly. Existing solutions to improve prompt-image consistency suffer from
the following challenges: (1) they oftentimes require model fine-tuning, (2)
they only focus on nearby prompt samples, and (3) they are affected by
unfavorable trade-offs among image quality, representation diversity, and
prompt-image consistency. In this paper, we address these challenges and
introduce a T2I optimization-by-prompting framework, OPT2I, which leverages a
large language model (LLM) to improve prompt-image consistency in T2I models.
Our framework starts from a user prompt and iteratively generates revised
prompts with the goal of maximizing a consistency score. Our extensive
validation on two datasets, MSCOCO and PartiPrompts, shows that OPT2I can boost
the initial consistency score by up to 24.9% in terms of DSG score while
preserving the FID and increasing the recall between generated and real data.
Our work paves the way toward building more reliable and robust T2I systems by
harnessing the power of LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SciNews: From Scholarly Complexities to Public Narratives -- A <span class="highlight-title">Dataset</span>
  for Scientific News Report Generation <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17768v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17768v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongqi Pu, Yifan Wang, Jia Loy, Vera Demberg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scientific news reports serve as a bridge, adeptly translating complex
research articles into reports that resonate with the broader public. The
automated generation of such narratives enhances the accessibility of scholarly
insights. In this paper, we present a new corpus to facilitate this paradigm
development. Our corpus comprises a parallel compilation of academic
publications and their corresponding scientific news reports across nine
disciplines. To demonstrate the utility and reliability of our dataset, we
conduct an extensive analysis, highlighting the divergences in readability and
brevity between scientific news narratives and academic manuscripts. We
benchmark our dataset employing state-of-the-art text generation models. The
evaluation process involves both automatic and human evaluation, which lays the
groundwork for future explorations into the automated generation of scientific
news reports. The dataset and code related to this work are available at
https://dongqi.me/projects/SciNews.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LREC-COLING 2024 Main Conference Paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Constructions Are So Difficult That Even Large Language Models Get Them
  Right for the Wrong Reasons <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17760v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17760v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shijia Zhou, Leonie Weissweiler, Taiqi He, Hinrich Schütze, David R. Mortensen, Lori Levin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we make a contribution that can be understood from two
perspectives: from an NLP perspective, we introduce a small challenge dataset
for NLI with large lexical overlap, which minimises the possibility of models
discerning entailment solely based on token distinctions, and show that GPT-4
and Llama 2 fail it with strong bias. We then create further challenging
sub-tasks in an effort to explain this failure. From a Computational
Linguistics perspective, we identify a group of constructions with three
classes of adjectives which cannot be distinguished by surface features. This
enables us to probe for LLM's understanding of these constructions in various
ways, and we find that they fail in a variety of ways to distinguish between
them, suggesting that they don't adequately represent their meaning or capture
the lexical properties of phrasal heads.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can multiple-choice questions really be useful in detecting the
  abilities of LLMs? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17752v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17752v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wangyue Li, Liangzhi Li, Tong Xiang, Xiao Liu, Wei Deng, Noa Garcia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multiple-choice questions (MCQs) are widely used in the evaluation of large
language models (LLMs) due to their simplicity and efficiency. However, there
are concerns about whether MCQs can truly measure LLM's capabilities,
particularly in knowledge-intensive scenarios where long-form generation (LFG)
answers are required. The misalignment between the task and the evaluation
method demands a thoughtful analysis of MCQ's efficacy, which we undertake in
this paper by evaluating nine LLMs on four question-answering (QA) datasets in
two languages: Chinese and English. We identify a significant issue: LLMs
exhibit an order sensitivity in bilingual MCQs, favoring answers located at
specific positions, i.e., the first position. We further quantify the gap
between MCQs and long-form generation questions (LFGQs) by comparing their
direct outputs, token logits, and embeddings. Our results reveal a relatively
low correlation between answers from MCQs and LFGQs for identical questions.
Additionally, we propose two methods to quantify the consistency and confidence
of LLMs' output, which can be generalized to other QA evaluation benchmarks.
Notably, our analysis challenges the idea that the higher the consistency, the
greater the accuracy. We also find MCQs to be less reliable than LFGQs in terms
of expected calibration error. Finally, the misalignment between MCQs and LFGQs
is not only reflected in the evaluation performance but also in the embedding
space. Our code and models can be accessed at
https://github.com/Meetyou-AI-Lab/Can-MC-Evaluate-LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UCxn: Typologically Informed Annotation of Constructions Atop Universal
  Dependencies <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17748v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17748v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leonie Weissweiler, Nina Böbel, Kirian Guiller, Santiago Herrera, Wesley Scivetti, Arthur Lorenzi, Nurit Melnik, Archna Bhatia, Hinrich Schütze, Lori Levin, Amir Zeldes, Joakim Nivre, William Croft, Nathan Schneider
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Universal Dependencies (UD) project has created an invaluable collection
of treebanks with contributions in over 140 languages. However, the UD
annotations do not tell the full story. Grammatical constructions that convey
meaning through a particular combination of several morphosyntactic elements --
for example, interrogative sentences with special markers and/or word orders --
are not labeled holistically. We argue for (i) augmenting UD annotations with a
'UCxn' annotation layer for such meaning-bearing grammatical constructions, and
(ii) approaching this in a typologically informed way so that morphosyntactic
strategies can be compared across languages. As a case study, we consider five
construction families in ten languages, identifying instances of each
construction in UD treebanks through the use of morphosyntactic patterns. In
addition to findings regarding these particular constructions, our study yields
important insights on methodology for describing and identifying constructions
in language-general and language-particular ways, and lays the foundation for
future constructional enrichment of UD treebanks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Continual Few-shot Event Detection via Hierarchical Augmentation
  Networks <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17733v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17733v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenlong Zhang, Pengfei Cao, Yubo Chen, Kang Liu, Zhiqiang Zhang, Mengshu Sun, Jun Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional continual event detection relies on abundant labeled data for
training, which is often impractical to obtain in real-world applications. In
this paper, we introduce continual few-shot event detection (CFED), a more
commonly encountered scenario when a substantial number of labeled samples are
not accessible. The CFED task is challenging as it involves memorizing previous
event types and learning new event types with few-shot samples. To mitigate
these challenges, we propose a memory-based framework: Hierarchical
Augmentation Networks (HANet). To memorize previous event types with limited
memory, we incorporate prototypical augmentation into the memory set. For the
issue of learning new event types in few-shot scenarios, we propose a
contrastive augmentation module for token representations. Despite comparing
with previous state-of-the-art methods, we also conduct comparisons with
ChatGPT. Experiment results demonstrate that our method significantly
outperforms all of these methods in multiple continual few-shot event detection
tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FastPerson: Enhancing Video Learning through Effective Video
  Summarization that Preserves Linguistic and Visual Contexts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17727v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17727v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kazuki Kawamura, Jun Rekimoto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quickly understanding lengthy lecture videos is essential for learners with
limited time and interest in various topics to improve their learning
efficiency. To this end, video summarization has been actively researched to
enable users to view only important scenes from a video. However, these studies
focus on either the visual or audio information of a video and extract
important segments in the video. Therefore, there is a risk of missing
important information when both the teacher's speech and visual information on
the blackboard or slides are important, such as in a lecture video. To tackle
this issue, we propose FastPerson, a video summarization approach that
considers both the visual and auditory information in lecture videos.
FastPerson creates summary videos by utilizing audio transcriptions along with
on-screen images and text, minimizing the risk of overlooking crucial
information for learners. Further, it provides a feature that allows learners
to switch between the summary and original videos for each chapter of the
video, enabling them to adjust the pace of learning based on their interests
and level of understanding. We conducted an evaluation with 40 participants to
assess the effectiveness of our method and confirmed that it reduced viewing
time by 53\% at the same level of comprehension as that when using traditional
video playback methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhanced Short Text Modeling: Leveraging Large Language Models for Topic
  Refinement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17706v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17706v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuyu Chang, Rui Wang, Peng Ren, Haiping Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Crafting effective topic models for brief texts, like tweets and news
headlines, is essential for capturing the swift shifts in social dynamics.
Traditional topic models, however, often fall short in accurately representing
the semantic intricacies of short texts due to their brevity and lack of
contextual data. In our study, we harness the advanced capabilities of Large
Language Models (LLMs) to introduce a novel approach termed "Topic Refinement".
This approach does not directly involve itself in the initial modeling of
topics but focuses on improving topics after they have been mined. By employing
prompt engineering, we direct LLMs to eliminate off-topic words within a given
topic, ensuring that only contextually relevant words are preserved or
substituted with ones that fit better semantically. This method emulates
human-like scrutiny and improvement of topics, thereby elevating the semantic
quality of the topics generated by various models. Our comprehensive evaluation
across three unique datasets has shown that our topic refinement approach
significantly enhances the semantic coherence of topics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Not All Similarities Are Created Equal: Leveraging Data-Driven Biases to
  Inform GenAI Copyright Disputes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17691v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17691v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Uri Hacohen, Adi Haviv, Shahar Sarfaty, Bruria Friedman, Niva Elkin-Koren, Roi Livni, Amit H Bermano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of Generative Artificial Intelligence (GenAI) models, including
GitHub Copilot, OpenAI GPT, and Stable Diffusion, has revolutionized content
creation, enabling non-professionals to produce high-quality content across
various domains. This transformative technology has led to a surge of synthetic
content and sparked legal disputes over copyright infringement. To address
these challenges, this paper introduces a novel approach that leverages the
learning capacity of GenAI models for copyright legal analysis, demonstrated
with GPT2 and Stable Diffusion models. Copyright law distinguishes between
original expressions and generic ones (Sc\`enes \`a faire), protecting the
former and permitting reproduction of the latter. However, this distinction has
historically been challenging to make consistently, leading to over-protection
of copyrighted works. GenAI offers an unprecedented opportunity to enhance this
legal analysis by revealing shared patterns in preexisting works. We propose a
data-driven approach to identify the genericity of works created by GenAI,
employing "data-driven bias" to assess the genericity of expressive
compositions. This approach aids in copyright scope determination by utilizing
the capabilities of GenAI to identify and prioritize expressive elements and
rank them according to their frequency in the model's dataset. The potential
implications of measuring expressive genericity for copyright law are profound.
Such scoring could assist courts in determining copyright scope during
litigation, inform the registration practices of Copyright Offices, allowing
registration of only highly original synthetic works, and help copyright owners
signal the value of their works and facilitate fairer licensing deals. More
generally, this approach offers valuable insights to policymakers grappling
with adapting copyright law to the challenges posed by the era of GenAI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at ACM CSLAW 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Language Models for Text Classification: Is In-Context Learning Enough? <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17661v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17661v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aleksandra Edwards, Jose Camacho-Collados
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent foundational language models have shown state-of-the-art performance
in many NLP tasks in zero- and few-shot settings. An advantage of these models
over more standard approaches based on fine-tuning is the ability to understand
instructions written in natural language (prompts), which helps them generalise
better to different tasks and domains without the need for specific training
data. This makes them suitable for addressing text classification problems for
domains with limited amounts of annotated instances. However, existing research
is limited in scale and lacks understanding of how text generation models
combined with prompting techniques compare to more established methods for text
classification such as fine-tuning masked language models. In this paper, we
address this research gap by performing a large-scale evaluation study for 16
text classification datasets covering binary, multiclass, and multilabel
problems. In particular, we compare zero- and few-shot approaches of large
language models to fine-tuning smaller language models. We also analyse the
results by prompt, classification type, domain, and number of labels. In
general, the results show how fine-tuning smaller and more efficient language
models can still outperform few-shot approaches of larger language models,
which have room for improvement when it comes to text classification.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Intrinsic Subgraph Generation for Interpretable Graph based Visual
  Question Answering <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17647v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17647v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pascal Tilli, Ngoc Thang Vu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The large success of deep learning based methods in Visual Question Answering
(VQA) has concurrently increased the demand for explainable methods. Most
methods in Explainable Artificial Intelligence (XAI) focus on generating
post-hoc explanations rather than taking an intrinsic approach, the latter
characterizing an interpretable model. In this work, we introduce an
interpretable approach for graph-based VQA and demonstrate competitive
performance on the GQA dataset. This approach bridges the gap between
interpretability and performance. Our model is designed to intrinsically
produce a subgraph during the question-answering process as its explanation,
providing insight into the decision making. To evaluate the quality of these
generated subgraphs, we compare them against established post-hoc
explainability methods for graph neural networks, and perform a human
evaluation. Moreover, we present quantitative metrics that correlate with the
evaluations of human assessors, acting as automatic metrics for the generated
explanatory subgraphs. Our implementation is available at
https://github.com/DigitalPhonetics/Intrinsic-Subgraph-Generation-for-VQA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DANCER: Entity Description Augmented Named Entity Corrector for
  Automatic Speech Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17645v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17645v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi-Cheng Wang, Hsin-Wei Wang, Bi-Cheng Yan, Chi-Han Lin, Berlin Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  End-to-end automatic speech recognition (E2E ASR) systems often suffer from
mistranscription of domain-specific phrases, such as named entities, sometimes
leading to catastrophic failures in downstream tasks. A family of fast and
lightweight named entity correction (NEC) models for ASR have recently been
proposed, which normally build on phonetic-level edit distance algorithms and
have shown impressive NEC performance. However, as the named entity (NE) list
grows, the problems of phonetic confusion in the NE list are exacerbated; for
example, homophone ambiguities increase substantially. In view of this, we
proposed a novel Description Augmented Named entity CorrEctoR (dubbed DANCER),
which leverages entity descriptions to provide additional information to
facilitate mitigation of phonetic confusion for NEC on ASR transcription. To
this end, an efficient entity description augmented masked language model
(EDA-MLM) comprised of a dense retrieval model is introduced, enabling MLM to
adapt swiftly to domain-specific entities for the NEC task. A series of
experiments conducted on the AISHELL-1 and Homophone datasets confirm the
effectiveness of our modeling approach. DANCER outperforms a strong baseline,
the phonetic edit-distance-based NEC model (PED-NEC), by a character error rate
(CER) reduction of about 7% relatively on AISHELL-1 for named entities. More
notably, when tested on Homophone that contain named entities of high phonetic
confusion, DANCER offers a more pronounced CER reduction of 46% relatively over
PED-NEC for named entities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ REFeREE: A REference-FREE Model-Based Metric for Text Simplification <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17640v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17640v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yichen Huang, Ekaterina Kochmar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text simplification lacks a universal standard of quality, and annotated
reference simplifications are scarce and costly. We propose to alleviate such
limitations by introducing REFeREE, a reference-free model-based metric with a
3-stage curriculum. REFeREE leverages an arbitrarily scalable pretraining stage
and can be applied to any quality standard as long as a small number of human
annotations are available. Our experiments show that our metric outperforms
existing reference-based metrics in predicting overall ratings and reaches
competitive and consistent performance in predicting specific ratings while
requiring no reference simplifications at inference time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mix-Initiative Response Generation with Dynamic Prefix Tuning <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17636v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17636v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxiang Nie, Heyan Huang, Xian-Ling Mao, Lizi Liao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mixed initiative serves as one of the key factors in controlling conversation
directions. For a speaker, responding passively or leading proactively would
result in rather different responses. However, most dialogue systems focus on
training a holistic response generation model without any distinction among
different initiatives. It leads to the cross-contamination problem, where the
model confuses different initiatives and generates inappropriate responses.
Moreover, obtaining plenty of human annotations for initiative labels can be
expensive. To address this issue, we propose a general mix-Initiative Dynamic
Prefix Tuning framework (IDPT) to decouple different initiatives from the
generation model, which learns initiative-aware prefixes in both supervised and
unsupervised settings. Specifically, IDPT decouples initiative factors into
different prefix parameters and uses the attention mechanism to adjust the
selection of initiatives in guiding generation dynamically. The prefix
parameters can be tuned towards accurate initiative prediction as well as
mix-initiative response generation. Extensive experiments on two public
dialogue datasets show that the proposed IDPT outperforms previous baselines on
both automatic metrics and human evaluations. It also manages to generate
appropriate responses with manipulated initiatives.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the main conference of NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ "You are an expert annotator": Automatic Best-Worst-Scaling Annotations
  for Emotion Intensity Modeling <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17612v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17612v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christopher Bagdon, Prathamesh Karmalker, Harsha Gurulingappa, Roman Klinger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Labeling corpora constitutes a bottleneck to create models for new tasks or
domains. Large language models mitigate the issue with automatic corpus
labeling methods, particularly for categorical annotations. Some NLP tasks such
as emotion intensity prediction, however, require text regression, but there is
no work on automating annotations for continuous label assignments. Regression
is considered more challenging than classification: The fact that humans
perform worse when tasked to choose values from a rating scale lead to
comparative annotation methods, including best-worst scaling. This raises the
question if large language model-based annotation methods show similar
patterns, namely that they perform worse on rating scale annotation tasks than
on comparative annotation tasks. To study this, we automate emotion intensity
predictions and compare direct rating scale predictions, pairwise comparisons
and best-worst scaling. We find that the latter shows the highest reliability.
A transformer regressor fine-tuned on these data performs nearly on par with a
model trained on the original manual annotations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted for publication in NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Denoising Table-Text Retrieval for Open-Domain Question Answering <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17611v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17611v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Deokhyung Kang, Baikjin Jung, Yunsu Kim, Gary Geunbae Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In table-text open-domain question answering, a retriever system retrieves
relevant evidence from tables and text to answer questions. Previous studies in
table-text open-domain question answering have two common challenges: firstly,
their retrievers can be affected by false-positive labels in training datasets;
secondly, they may struggle to provide appropriate evidence for questions that
require reasoning across the table. To address these issues, we propose
Denoised Table-Text Retriever (DoTTeR). Our approach involves utilizing a
denoised training dataset with fewer false positive labels by discarding
instances with lower question-relevance scores measured through a false
positive detection model. Subsequently, we integrate table-level ranking
information into the retriever to assist in finding evidence for questions that
demand reasoning across the table. To encode this ranking information, we
fine-tune a rank-aware column encoder to identify minimum and maximum values
within a column. Experimental results demonstrate that DoTTeR significantly
outperforms strong baselines on both retrieval recall and downstream QA tasks.
Our code is available at https://github.com/deokhk/DoTTeR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Coimagining the Future of Voice Assistants with Cultural Sensitivity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17599v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17599v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Katie Seaborn, Yuto Sawa, Mizuki Watanabe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Voice assistants (VAs) are becoming a feature of our everyday life. Yet, the
user experience (UX) is often limited, leading to underuse, disengagement, and
abandonment. Co-designing interactions for VAs with potential end-users can be
useful. Crowdsourcing this process online and anonymously may add value.
However, most work has been done in the English-speaking West on dialogue data
sets. We must be sensitive to cultural differences in language, social
interactions, and attitudes towards technology. Our aims were to explore the
value of co-designing VAs in the non-Western context of Japan and demonstrate
the necessity of cultural sensitivity. We conducted an online elicitation study
(N = 135) where Americans (n = 64) and Japanese people (n = 71) imagined
dialogues (N = 282) and activities (N = 73) with future VAs. We discuss the
implications for coimagining interactions with future VAs, offer design
guidelines for the Japanese and English-speaking US contexts, and suggest
opportunities for cultural plurality in VA design and scholarship.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards a Zero-Data, Controllable, Adaptive Dialog System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17582v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17582v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dirk Väth, Lindsey Vanderlyn, Ngoc Thang Vu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conversational Tree Search (V\"ath et al., 2023) is a recent approach to
controllable dialog systems, where domain experts shape the behavior of a
Reinforcement Learning agent through a dialog tree. The agent learns to
efficiently navigate this tree, while adapting to information needs, e.g.,
domain familiarity, of different users. However, the need for additional
training data hinders deployment in new domains. To address this, we explore
approaches to generate this data directly from dialog trees. We improve the
original approach, and show that agents trained on synthetic data can achieve
comparable dialog success to models trained on human data, both when using a
commercial Large Language Model for generation, or when using a smaller
open-source model, running on a single GPU. We further demonstrate the
scalability of our approach by collecting and testing on two new datasets:
ONBOARD, a new domain helping foreign residents moving to a new city, and the
medical domain DIAGNOSE, a subset of Wikipedia articles related to scalp and
head symptoms. Finally, we perform human testing, where no statistically
significant differences were found in either objective or subjective measures
between models trained on human and generated data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Task-Oriented Paraphrase Analytics <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17564v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17564v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marcel Gohsen, Matthias Hagen, Martin Potthast, Benno Stein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since paraphrasing is an ill-defined task, the term "paraphrasing" covers
text transformation tasks with different characteristics. Consequently,
existing paraphrasing studies have applied quite different (explicit and
implicit) criteria as to when a pair of texts is to be considered a paraphrase,
all of which amount to postulating a certain level of semantic or lexical
similarity. In this paper, we conduct a literature review and propose a
taxonomy to organize the 25~identified paraphrasing (sub-)tasks. Using
classifiers trained to identify the tasks that a given paraphrasing instance
fits, we find that the distributions of task-specific instances in the known
paraphrase corpora vary substantially. This means that the use of these
corpora, without the respective paraphrase conditions being clearly defined
(which is the normal case), must lead to incomparable and misleading results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ m3P: Towards Multimodal Multilingual Translation with Multimodal <span class="highlight-title">Prompt</span> <span class="chip">COLING 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17556v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17556v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jian Yang, Hongcheng Guo, Yuwei Yin, Jiaqi Bai, Bing Wang, Jiaheng Liu, Xinnian Liang, Linzheng Cahi, Liqun Yang, Zhoujun Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multilingual translation supports multiple translation directions by
projecting all languages in a shared space, but the translation quality is
undermined by the difference between languages in the text-only modality,
especially when the number of languages is large. To bridge this gap, we
introduce visual context as the universal language-independent representation
to facilitate multilingual translation. In this paper, we propose a framework
to leverage the multimodal prompt to guide the Multimodal Multilingual neural
Machine Translation (m3P), which aligns the representations of different
languages with the same meaning and generates the conditional vision-language
memory for translation. We construct a multilingual multimodal instruction
dataset (InstrMulti102) to support 102 languages. Our method aims to minimize
the representation distance of different languages by regarding the image as a
central language. Experimental results show that m3P outperforms previous
text-only baselines and multilingual multimodal methods by a large margin.
Furthermore, the probing experiments validate the effectiveness of our method
in enhancing translation under the low-resource and massively multilingual
scenario.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RuBia: A Russian Language Bias Detection <span class="highlight-title">Dataset</span> <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17553v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17553v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Veronika Grigoreva, Anastasiia Ivanova, Ilseyar Alimova, Ekaterina Artemova
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Warning: this work contains upsetting or disturbing content.
  Large language models (LLMs) tend to learn the social and cultural biases
present in the raw pre-training data. To test if an LLM's behavior is fair,
functional datasets are employed, and due to their purpose, these datasets are
highly language and culture-specific. In this paper, we address a gap in the
scope of multilingual bias evaluation by presenting a bias detection dataset
specifically designed for the Russian language, dubbed as RuBia. The RuBia
dataset is divided into 4 domains: gender, nationality, socio-economic status,
and diverse, each of the domains is further divided into multiple fine-grained
subdomains. Every example in the dataset consists of two sentences with the
first reinforcing a potentially harmful stereotype or trope and the second
contradicting it. These sentence pairs were first written by volunteers and
then validated by native-speaking crowdsourcing workers. Overall, there are
nearly 2,000 unique sentence pairs spread over 19 subdomains in RuBia. To
illustrate the dataset's purpose, we conduct a diagnostic evaluation of
state-of-the-art or near-state-of-the-art LLMs and discuss the LLMs'
predisposition to social biases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted to LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Naive Bayes-based Context Extension for Large Language Models <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17552v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17552v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianlin Su, Murtadha Ahmed,  Wenbo, Luo Ao, Mingren Zhu, Yunfeng Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have shown promising in-context learning
abilities. However, conventional In-Context Learning (ICL) approaches are often
impeded by length limitations of transformer architecture, which pose
challenges when attempting to effectively integrate supervision from a
substantial number of demonstration examples. In this paper, we introduce a
novel framework, called Naive Bayes-based Context Extension (NBCE), to enable
existing LLMs to perform ICL with an increased number of demonstrations by
significantly expanding their context size. Importantly, this expansion does
not require fine-tuning or dependence on particular model architectures, all
the while preserving linear efficiency. NBCE initially splits the context into
equal-sized windows fitting the target LLM's maximum length. Then, it
introduces a voting mechanism to select the most relevant window, regarded as
the posterior context. Finally, it employs Bayes' theorem to generate the test
task. Our experimental results demonstrate that NBCE substantially enhances
performance, particularly as the number of demonstration examples increases,
consistently outperforming alternative methods. The NBCE code will be made
publicly accessible. The code NBCE is available at:
https://github.com/amurtadha/NBCE-master
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to main NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Decoding excellence: Mapping the demand for psychological traits of
  operations and supply chain professionals through text mining 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17546v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17546v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        S. Di Luozzo, A. Fronzetti Colladon, M. M. Schiraldi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The current study proposes an innovative methodology for the profiling of
psychological traits of Operations Management (OM) and Supply Chain Management
(SCM) professionals. We use innovative methods and tools of text mining and
social network analysis to map the demand for relevant skills from a set of job
descriptions, with a focus on psychological characteristics. The proposed
approach aims to evaluate the market demand for specific traits by combining
relevant psychological constructs, text mining techniques, and an innovative
measure, namely, the Semantic Brand Score. We apply the proposed methodology to
a dataset of job descriptions for OM and SCM professionals, with the objective
of providing a mapping of their relevant required skills, including
psychological characteristics. In addition, the analysis is then detailed by
considering the region of the organization that issues the job description, its
organizational size, and the seniority level of the open position in order to
understand their nuances. Finally, topic modeling is used to examine key
components and their relative significance in job descriptions. By employing a
novel methodology and considering contextual factors, we provide an innovative
understanding of the attitudinal traits that differentiate professionals. This
research contributes to talent management, recruitment practices, and
professional development initiatives, since it provides new figures and
perspectives to improve the effectiveness and success of Operations Management
and Supply Chain Management professionals.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Gaze-grounded Visual Question Answering <span class="highlight-title">Dataset</span> for Clarifying
  Ambiguous Japanese Questions <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17545v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17545v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shun Inadumi, Seiya Kawano, Akishige Yuguchi, Yasutomo Kawanishi, Koichiro Yoshino
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Situated conversations, which refer to visual information as visual question
answering (VQA), often contain ambiguities caused by reliance on directive
information. This problem is exacerbated because some languages, such as
Japanese, often omit subjective or objective terms. Such ambiguities in
questions are often clarified by the contexts in conversational situations,
such as joint attention with a user or user gaze information. In this study, we
propose the Gaze-grounded VQA dataset (GazeVQA) that clarifies ambiguous
questions using gaze information by focusing on a clarification process
complemented by gaze information. We also propose a method that utilizes gaze
target estimation results to improve the accuracy of GazeVQA tasks. Our
experimental results showed that the proposed method improved the performance
in some cases of a VQA system on GazeVQA and identified some typical problems
of GazeVQA tasks that need to be improved.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Language Models Are State-of-the-Art Evaluator for Grammatical
  Error Correction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17540v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17540v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Masamune Kobayashi, Masato Mita, Mamoru Komachi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have been reported to outperform existing
automatic evaluation metrics in some tasks, such as text summarization and
machine translation. However, there has been a lack of research on LLMs as
evaluators in grammatical error correction (GEC). In this study, we investigate
the performance of LLMs in GEC evaluation by employing prompts designed to
incorporate various evaluation criteria inspired by previous research. Our
extensive experimental results demonstrate that GPT-4 achieved Kendall's rank
correlation of 0.662 with human judgments, surpassing all existing methods.
Furthermore, in recent GEC evaluations, we have underscored the significance of
the LLMs scale and particularly emphasized the importance of fluency among
evaluation criteria.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ILLUMINER: Instruction-tuned Large Language Models as Few-shot Intent
  Classifier and Slot Filler <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17536v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17536v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paramita Mirza, Viju Sudhi, Soumya Ranjan Sahoo, Sinchana Ramakanth Bhat
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-art intent classification (IC) and slot filling (SF) methods
often rely on data-intensive deep learning models, limiting their practicality
for industry applications. Large language models on the other hand,
particularly instruction-tuned models (Instruct-LLMs), exhibit remarkable
zero-shot performance across various natural language tasks. This study
evaluates Instruct-LLMs on popular benchmark datasets for IC and SF,
emphasizing their capacity to learn from fewer examples. We introduce
ILLUMINER, an approach framing IC and SF as language generation tasks for
Instruct-LLMs, with a more efficient SF-prompting method compared to prior
work. A comprehensive comparison with multiple baselines shows that our
approach, using the FLAN-T5 11B model, outperforms the state-of-the-art joint
IC+SF method and in-context learning with GPT3.5 (175B), particularly in slot
filling by 11.1--32.2 percentage points. Additionally, our in-depth ablation
study demonstrates that parameter-efficient fine-tuning requires less than 6%
of training data to yield comparable performance with traditional full-weight
fine-tuning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sparse Logistic Regression with High-order Features for Automatic
  Grammar Rule Extraction from Treebanks <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17534v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17534v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Santiago Herrera, Caio Corro, Sylvain Kahane
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Descriptive grammars are highly valuable, but writing them is time-consuming
and difficult. Furthermore, while linguists typically use corpora to create
them, grammar descriptions often lack quantitative data. As for formal
grammars, they can be challenging to interpret. In this paper, we propose a new
method to extract and explore significant fine-grained grammar patterns and
potential syntactic grammar rules from treebanks, in order to create an
easy-to-understand corpus-based grammar. More specifically, we extract
descriptions and rules across different languages for two linguistic phenomena,
agreement and word order, using a large search space and paying special
attention to the ranking order of the extracted rules. For that, we use a
linear classifier to extract the most salient features that predict the
linguistic phenomena under study. We associate statistical information to each
rule, and we compare the ranking of the model's results to those of other
quantitative and statistical measures. Our method captures both well-known and
less well-known significant grammar rules in Spanish, French, and Wolof.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in LREC-Coling 2024 proceedings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multilingual Sentence-T5: Scalable Sentence Encoders for Multilingual
  Applications <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17528v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17528v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chihiro Yano, Akihiko Fukuchi, Shoko Fukasawa, Hideyuki Tachibana, Yotaro Watanabe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prior work on multilingual sentence embedding has demonstrated that the
efficient use of natural language inference (NLI) data to build
high-performance models can outperform conventional methods. However, the
potential benefits from the recent ``exponential'' growth of language models
with billions of parameters have not yet been fully explored. In this paper, we
introduce Multilingual Sentence T5 (m-ST5), as a larger model of NLI-based
multilingual sentence embedding, by extending Sentence T5, an existing
monolingual model. By employing the low-rank adaptation (LoRA) technique, we
have achieved a successful scaling of the model's size to 5.7 billion
parameters. We conducted experiments to evaluate the performance of sentence
embedding and verified that the method outperforms the NLI-based prior
approach. Furthermore, we also have confirmed a positive correlation between
the size of the model and its performance. It was particularly noteworthy that
languages with fewer resources or those with less linguistic similarity to
English benefited more from the parameter increase. Our model is available at
https://huggingface.co/pkshatech/m-ST5.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Provably Secure Disambiguating Neural Linguistic Steganography 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17524v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17524v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuang Qi, Kejiang Chen, Kai Zeng, Weiming Zhang, Nenghai Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent research in provably secure neural linguistic steganography has
overlooked a crucial aspect: the sender must detokenize stegotexts to avoid
raising suspicion from the eavesdropper. The segmentation ambiguity problem,
which arises when using language models based on subwords, leads to occasional
decoding failures in all neural language steganography implementations based on
these models. Current solutions to this issue involve altering the probability
distribution of candidate words, rendering them incompatible with provably
secure steganography. We propose a novel secure disambiguation method named
SyncPool, which effectively addresses the segmentation ambiguity problem. We
group all tokens with prefix relationships in the candidate pool before the
steganographic embedding algorithm runs to eliminate uncertainty among
ambiguous tokens. To enable the receiver to synchronize the sampling process of
the sender, a shared cryptographically-secure pseudorandom number generator
(CSPRNG) is deployed to select a token from the ambiguity pool. SyncPool does
not change the size of the candidate pool or the distribution of tokens and
thus is applicable to provably secure language steganography methods. We
provide theoretical proofs and experimentally demonstrate the applicability of
our solution to various languages and models, showing its potential to
significantly improve the reliability and security of neural linguistic
steganography systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MapGuide: A Simple yet Effective Method to Reconstruct Continuous
  Language from Brain Activities <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17516v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17516v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinpei Zhao, Jingyuan Sun, Shaonan Wang, Jing Ye, Xiaohan Zhang, Chengqing Zong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Decoding continuous language from brain activity is a formidable yet
promising field of research. It is particularly significant for aiding people
with speech disabilities to communicate through brain signals. This field
addresses the complex task of mapping brain signals to text. The previous best
attempt reverse-engineered this process in an indirect way: it began by
learning to encode brain activity from text and then guided text generation by
aligning with predicted brain responses. In contrast, we propose a simple yet
effective method that guides text reconstruction by directly comparing them
with the predicted text embeddings mapped from brain activities. Comprehensive
experiments reveal that our method significantly outperforms the current
state-of-the-art model, showing average improvements of 77% and 54% on BLEU and
METEOR scores. We further validate the proposed modules through detailed
ablation studies and case analyses and highlight a critical correlation: the
more precisely we map brain activities to text embeddings, the better the text
reconstruction results. Such insight can simplify the task of reconstructing
language from brain activities for future work, emphasizing the importance of
improving brain-to-text-embedding mapping techniques.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NAACL 2024 main conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sharing the Cost of Success: A Game for Evaluating and Learning
  Collaborative Multi-Agent Instruction Giving and Following Policies <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17497v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17497v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philipp Sadler, Sherzod Hakimov, David Schlangen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In collaborative goal-oriented settings, the participants are not only
interested in achieving a successful outcome, but do also implicitly negotiate
the effort they put into the interaction (by adapting to each other). In this
work, we propose a challenging interactive reference game that requires two
players to coordinate on vision and language observations. The learning signal
in this game is a score (given after playing) that takes into account the
achieved goal and the players' assumed efforts during the interaction. We show
that a standard Proximal Policy Optimization (PPO) setup achieves a high
success rate when bootstrapped with heuristic partner behaviors that implement
insights from the analysis of human-human interactions. And we find that a
pairing of neural partners indeed reduces the measured joint effort when
playing together repeatedly. However, we observe that in comparison to a
reasonable heuristic pairing there is still room for improvement -- which
invites further research in the direction of cost-sharing in collaborative
interactions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, Accepted at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DGoT: Dynamic Graph of Thoughts for Scientific Abstract Generation <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17491v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17491v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyu Ning, Yutong Zhao, Yitong Liu, Hongwen Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The method of training language models based on domain datasets has obtained
significant achievements in the task of generating scientific paper abstracts.
However, such models face problems of generalization and expensive training
costs. The use of large language models (LLMs) to solve the task of generating
paper abstracts saves the cost of model training. However, due to the
hallucination problem of LLM, it is often necessary to improve the reliability
of the results through multi-round query prompt approach such as Graph of
Thoughts (GoT), which also brings additional reasoning costs. In this paper, we
propose a Dynamic Graph of Thought (DGoT). It not only inherits the advantages
of the existing GoT prompt approach, but also dynamically adjust the graph
structure according to data characteristics while reducing model reasoning
cost. Experimental results show that our method's cost-effectiveness in
abstract generation tasks is only 43.7% to 56.4% of other multi-round query
prompt approaches. Our code is available at https://github.com/JayceNing/DGoT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ KDMCSE: Knowledge Distillation Multimodal Sentence Embeddings with
  Adaptive Angular margin Contrastive Learning <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17486v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17486v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cong-Duy Nguyen, Thong Nguyen, Xiaobao Wu, Anh Tuan Luu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Previous work on multimodal sentence embedding has proposed multimodal
contrastive learning and achieved promising results. However, by taking the
rest of the batch as negative samples without reviewing when forming
contrastive pairs, those studies encountered many suspicious and noisy negative
examples, significantly affecting the methods' overall performance. In this
work, we propose KDMCSE (Knowledge Distillation Multimodal contrastive learning
of Sentence Embeddings), a novel approach that enhances the discrimination and
generalizability of multimodal representation and inherits the knowledge from
the teacher model to learn the difference between positive and negative
instances and via that, can detect noisy and wrong negative samples effectively
before they are calculated in the contrastive objective. Furthermore, to
overcome the limitation of modeling the variation within negative pairs, we
introduce a new contrastive objective, AdapACSE (Adaptive Angular Margin
Supervised Contrastive Learning for Multimodal sentence embeddings), that
enhances the discriminative representation by strengthening the margin within
the angular space while capturing varying semantics within the negative.
Experimental results on widely used Semantic Textual Similarity (STS)
benchmarks demonstrate the effectiveness of our approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Incorporating Exponential Smoothing into MLP: A Simple but Effective
  Sequence Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17445v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17445v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiqun Chu, Zuoquan Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modeling long-range dependencies in sequential data is a crucial step in
sequence learning. A recently developed model, the Structured State Space (S4),
demonstrated significant effectiveness in modeling long-range sequences.
However, It is unclear whether the success of S4 can be attributed to its
intricate parameterization and HiPPO initialization or simply due to State
Space Models (SSMs). To further investigate the potential of the deep SSMs, we
start with exponential smoothing (ETS), a simple SSM, and propose a stacked
architecture by directly incorporating it into an element-wise MLP. We augment
simple ETS with additional parameters and complex field to reduce the inductive
bias. Despite increasing less than 1\% of parameters of element-wise MLP, our
models achieve comparable results to S4 on the LRA benchmark.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 5 tables, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust and Scalable Model Editing for Large Language Models <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17431v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17431v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingfa Chen, Zhengyan Zhang, Xu Han, Chaojun Xiao, Zhiyuan Liu, Chen Chen, Kuai Li, Tao Yang, Maosong Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) can make predictions using parametric
knowledge--knowledge encoded in the model weights--or contextual
knowledge--knowledge presented in the context. In many scenarios, a desirable
behavior is that LLMs give precedence to contextual knowledge when it conflicts
with the parametric knowledge, and fall back to using their parametric
knowledge when the context is irrelevant. This enables updating and correcting
the model's knowledge by in-context editing instead of retraining. Previous
works have shown that LLMs are inclined to ignore contextual knowledge and fail
to reliably fall back to parametric knowledge when presented with irrelevant
context. In this work, we discover that, with proper prompting methods,
instruction-finetuned LLMs can be highly controllable by contextual knowledge
and robust to irrelevant context. Utilizing this feature, we propose EREN (Edit
models by REading Notes) to improve the scalability and robustness of LLM
editing. To better evaluate the robustness of model editors, we collect a new
dataset, that contains irrelevant questions that are more challenging than the
ones in existing datasets. Empirical results show that our method outperforms
current state-of-the-art methods by a large margin. Unlike existing techniques,
it can integrate knowledge from multiple edits, and correctly respond to
syntactically similar but semantically unrelated inputs (and vice versa). The
source code can be found at https://github.com/thunlp/EREN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LREC-COLING 2024 paper, 16 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Aligning Large Language Models for Enhancing Psychiatric Interviews
  through Symptom Delineation and Summarization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17428v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17428v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jae-hee So, Joonhwan Chang, Eunji Kim, Junho Na, JiYeon Choi, Jy-yong Sohn, Byung-Hoon Kim, Sang Hui Chu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in Large Language Models (LLMs) have accelerated their
usage in various domains. Given the fact that psychiatric interviews are
goal-oriented and structured dialogues between the professional interviewer and
the interviewee, it is one of the most underexplored areas where LLMs can
contribute substantial value. Here, we explore the use of LLMs for enhancing
psychiatric interviews, by analyzing counseling data from North Korean
defectors with traumatic events and mental health issues. Specifically, we
investigate whether LLMs can (1) delineate the part of the conversation that
suggests psychiatric symptoms and name the symptoms, and (2) summarize
stressors and symptoms, based on the interview dialogue transcript. Here, the
transcript data was labeled by mental health experts for training and
evaluation of LLMs. Our experimental results show that appropriately prompted
LLMs can achieve high performance on both the symptom delineation task and the
summarization task. This research contributes to the nascent field of applying
LLMs to psychiatric interview and demonstrates their potential effectiveness in
aiding mental health practitioners.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LM-Combiner: A Contextual Rewriting Model for Chinese Grammatical Error
  Correction <span class="chip">COLING 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17413v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17413v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixuan Wang, Baoxin Wang, Yijun Liu, Dayong Wu, Wanxiang Che
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Over-correction is a critical problem in Chinese grammatical error correction
(CGEC) task. Recent work using model ensemble methods based on voting can
effectively mitigate over-correction and improve the precision of the GEC
system. However, these methods still require the output of several GEC systems
and inevitably lead to reduced error recall. In this light, we propose the
LM-Combiner, a rewriting model that can directly modify the over-correction of
GEC system outputs without a model ensemble. Specifically, we train the model
on an over-correction dataset constructed through the proposed K-fold cross
inference method, which allows it to directly generate filtered sentences by
combining the original and the over-corrected text. In the inference stage, we
directly take the original sentences and the output results of other systems as
input and then obtain the filtered sentences through LM-Combiner. Experiments
on the FCGEC dataset show that our proposed method effectively alleviates the
over-correction of the original system (+18.2 Precision) while ensuring the
error recall remains unchanged. Besides, we find that LM-Combiner still has a
good rewriting performance even with small parameters and few training data,
and thus can cost-effectively mitigate the over-correction of black-box GEC
systems (e.g., ChatGPT).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PCToolkit: A Unified Plug-and-Play <span class="highlight-title">Prompt</span> Compression Toolkit of Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17411v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17411v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinyi Li, Yihuai Lan, Lei Wang, Hao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prompt compression is an innovative method for efficiently condensing input
prompts while preserving essential information. To facilitate quick-start
services, user-friendly interfaces, and compatibility with common datasets and
metrics, we present the Prompt Compression Toolkit (PCToolkit). This toolkit is
a unified plug-and-play solution for compressing prompts in Large Language
Models (LLMs), featuring cutting-edge prompt compressors, diverse datasets, and
metrics for comprehensive performance evaluation. PCToolkit boasts a modular
design, allowing for easy integration of new datasets and metrics through
portable and user-friendly interfaces. In this paper, we outline the key
components and functionalities of PCToolkit. We conducted evaluations of the
compressors within PCToolkit across various natural language tasks, including
reconstruction, summarization, mathematical problem-solving, question
answering, few-shot learning, synthetic tasks, code completion, boolean
expressions, multiple choice questions, and lies recognition.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>For open-source repository, see
  https://github.com/3DAgentWorld/Toolkit-for-Prompt-Compression</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Transcribing Bengali Text with Regional Dialects to IPA using District
  Guided Tokens 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17407v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17407v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        S M Jishanul Islam, Sadia Ahmmed, Sahid Hossain Mustakim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate transcription of Bengali text to the International Phonetic Alphabet
(IPA) is a challenging task due to the complex phonology of the language and
context-dependent sound changes. This challenge is even more for regional
Bengali dialects due to unavailability of standardized spelling conventions for
these dialects, presence of local and foreign words popular in those regions
and phonological diversity across different regions. This paper presents an
approach to this sequence-to-sequence problem by introducing the District
Guided Tokens (DGT) technique on a new dataset spanning six districts of
Bangladesh. The key idea is to provide the model with explicit information
about the regional dialect or "district" of the input text before generating
the IPA transcription. This is achieved by prepending a district token to the
input sequence, effectively guiding the model to understand the unique phonetic
patterns associated with each district. The DGT technique is applied to
fine-tune several transformer-based models, on this new dataset. Experimental
results demonstrate the effectiveness of DGT, with the ByT5 model achieving
superior performance over word-based models like mT5, BanglaT5, and umT5. This
is attributed to ByT5's ability to handle a high percentage of
out-of-vocabulary words in the test set. The proposed approach highlights the
importance of incorporating regional dialect information into ubiquitous
natural language processing systems for languages with diverse phonological
variations. The following work was a result of the "Bhashamul" challenge, which
is dedicated to solving the problem of Bengali text with regional dialects to
IPA transcription https://www.kaggle.com/competitions/regipa/. The training and
inference notebooks are available through the competition link.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work became the champion of the Bhashamul challenge</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ELLEN: Extremely Lightly Supervised Learning For Efficient Named Entity
  Recognition <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17385v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17385v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haris Riaz, Razvan-Gabriel Dumitru, Mihai Surdeanu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we revisit the problem of semi-supervised named entity
recognition (NER) focusing on extremely light supervision, consisting of a
lexicon containing only 10 examples per class. We introduce ELLEN, a simple,
fully modular, neuro-symbolic method that blends fine-tuned language models
with linguistic rules. These rules include insights such as ''One Sense Per
Discourse'', using a Masked Language Model as an unsupervised NER, leveraging
part-of-speech tags to identify and eliminate unlabeled entities as false
negatives, and other intuitions about classifier confidence scores in local and
global context. ELLEN achieves very strong performance on the CoNLL-2003
dataset when using the minimal supervision from the lexicon above. It also
outperforms most existing (and considerably more complex) semi-supervised NER
methods under the same supervision settings commonly used in the literature
(i.e., 5% of the training data). Further, we evaluate our CoNLL-2003 model in a
zero-shot scenario on WNUT-17 where we find that it outperforms GPT-3.5 and
achieves comparable performance to GPT-4. In a zero-shot setting, ELLEN also
achieves over 75% of the performance of a strong, fully supervised model
trained on gold data. Our code is available at:
https://github.com/hriaz17/ELLEN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Chat<span class="highlight-title">GPT</span> Rates Natural Language Explanation Quality Like Humans: But on
  Which Scales? <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17368v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17368v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fan Huang, Haewoon Kwak, Kunwoo Park, Jisun An
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As AI becomes more integral in our lives, the need for transparency and
responsibility grows. While natural language explanations (NLEs) are vital for
clarifying the reasoning behind AI decisions, evaluating them through human
judgments is complex and resource-intensive due to subjectivity and the need
for fine-grained ratings. This study explores the alignment between ChatGPT and
human assessments across multiple scales (i.e., binary, ternary, and 7-Likert
scale). We sample 300 data instances from three NLE datasets and collect 900
human annotations for both informativeness and clarity scores as the text
quality measurement. We further conduct paired comparison experiments under
different ranges of subjectivity scores, where the baseline comes from 8,346
human annotations. Our results show that ChatGPT aligns better with humans in
more coarse-grained scales. Also, paired comparisons and dynamic prompting
(i.e., providing semantically similar examples in the prompt) improve the
alignment. This research advances our understanding of large language models'
capabilities to assess the text explanation quality in different configurations
for responsible AI development.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accpeted by LREC-COLING 2024 main conference, long paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Extracting Biomedical Entities from Noisy Audio Transcripts <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17363v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17363v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nima Ebadi, Kellen Morgan, Adrian Tan, Billy Linares, Sheri Osborn, Emma Majors, Jeremy Davis, Anthony Rios
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic Speech Recognition (ASR) technology is fundamental in transcribing
spoken language into text, with considerable applications in the clinical
realm, including streamlining medical transcription and integrating with
Electronic Health Record (EHR) systems. Nevertheless, challenges persist,
especially when transcriptions contain noise, leading to significant drops in
performance when Natural Language Processing (NLP) models are applied. Named
Entity Recognition (NER), an essential clinical task, is particularly affected
by such noise, often termed the ASR-NLP gap. Prior works have primarily studied
ASR's efficiency in clean recordings, leaving a research gap concerning the
performance in noisy environments. This paper introduces a novel dataset,
BioASR-NER, designed to bridge the ASR-NLP gap in the biomedical domain,
focusing on extracting adverse drug reactions and mentions of entities from the
Brief Test of Adult Cognition by Telephone (BTACT) exam. Our dataset offers a
comprehensive collection of almost 2,000 clean and noisy recordings. In
addressing the noise challenge, we present an innovative transcript-cleaning
method using GPT4, investigating both zero-shot and few-shot methodologies. Our
study further delves into an error analysis, shedding light on the types of
errors in transcription software, corrections by GPT4, and the challenges GPT4
faces. This paper aims to foster improved understanding and potential solutions
for the ASR-NLP gap, ultimately supporting enhanced healthcare documentation
practices.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bridging Textual and Tabular Worlds for Fact Verification: A
  Lightweight, Attention-Based Model <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17361v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17361v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shirin Dabbaghi Varnosfaderani, Canasai Kruengkrai, Ramin Yahyapour, Junichi Yamagishi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  FEVEROUS is a benchmark and research initiative focused on fact extraction
and verification tasks involving unstructured text and structured tabular data.
In FEVEROUS, existing works often rely on extensive preprocessing and utilize
rule-based transformations of data, leading to potential context loss or
misleading encodings. This paper introduces a simple yet powerful model that
nullifies the need for modality conversion, thereby preserving the original
evidence's context. By leveraging pre-trained models on diverse text and
tabular datasets and by incorporating a lightweight attention-based mechanism,
our approach efficiently exploits latent connections between different data
types, thereby yielding comprehensive and reliable verdict predictions. The
model's modular structure adeptly manages multi-modal information, ensuring the
integrity and authenticity of the original evidence are uncompromised.
Comparative analyses reveal that our approach exhibits competitive performance,
aligning itself closely with top-tier models on the FEVEROUS benchmark.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for a presentation at LREC-COLING 2024 - The 2024 Joint
  International Conference on Computational Linguistics, Language Resources and
  Evaluation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Chain-of-Action: Faithful and Multimodal Question Answering through
  Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17359v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17359v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenyu Pan, Haozheng Luo, Manling Li, Han Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a Chain-of-Action (CoA) framework for multimodal and
retrieval-augmented Question-Answering (QA). Compared to the literature, CoA
overcomes two major challenges of current QA applications: (i) unfaithful
hallucination that is inconsistent with real-time or domain facts and (ii) weak
reasoning performance over compositional information. Our key contribution is a
novel reasoning-retrieval mechanism that decomposes a complex question into a
reasoning chain via systematic prompting and pre-designed actions.
Methodologically, we propose three types of domain-adaptable `Plug-and-Play'
actions for retrieving real-time information from heterogeneous sources. We
also propose a multi-reference faith score (MRFS) to verify and resolve
conflicts in the answers. Empirically, we exploit both public benchmarks and a
Web3 case study to demonstrate the capability of CoA over other methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Disambiguate Entity Matching through Relation Discovery with Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17344v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17344v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zezhou Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Entity matching is a critical challenge in data integration and cleaning,
central to tasks like fuzzy joins and deduplication. Traditional approaches
have focused on overcoming fuzzy term representations through methods such as
edit distance, Jaccard similarity, and more recently, embeddings and deep
neural networks, including advancements from large language models (LLMs) like
GPT. However, the core challenge in entity matching extends beyond term
fuzziness to the ambiguity in defining what constitutes a "match," especially
when integrating with external databases. This ambiguity arises due to varying
levels of detail and granularity among entities, complicating exact matches. We
propose a novel approach that shifts focus from purely identifying semantic
similarities to understanding and defining the "relations" between entities as
crucial for resolving ambiguities in matching. By predefining a set of
relations relevant to the task at hand, our method allows analysts to navigate
the spectrum of similarity more effectively, from exact matches to conceptually
related entities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Language Models are Free Boosters for Biomedical Imaging Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17343v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17343v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhixin Lai, Jing Wu, Suiyao Chen, Yucheng Zhou, Anna Hovakimyan, Naira Hovakimyan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we uncover the unexpected efficacy of residual-based large
language models (LLMs) as part of encoders for biomedical imaging tasks, a
domain traditionally devoid of language or textual data. The approach diverges
from established methodologies by utilizing a frozen transformer block,
extracted from pre-trained LLMs, as an innovative encoder layer for the direct
processing of visual tokens. This strategy represents a significant departure
from the standard multi-modal vision-language frameworks, which typically hinge
on language-driven prompts and inputs. We found that these LLMs could boost
performance across a spectrum of biomedical imaging applications, including
both 2D and 3D visual classification tasks, serving as plug-and-play boosters.
More interestingly, as a byproduct, we found that the proposed framework
achieved superior performance, setting new state-of-the-art results on
extensive, standardized datasets in MedMNIST-2D and 3D. Through this work, we
aim to open new avenues for employing LLMs in biomedical imaging and enriching
the understanding of their potential in this specialized domain.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Don't Listen To Me: Understanding and Exploring Jailbreak <span class="highlight-title">Prompt</span>s of
  Large Language Models <span class="chip">USENIX Security 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17336v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17336v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiyuan Yu, Xiaogeng Liu, Shunning Liang, Zach Cameron, Chaowei Xiao, Ning Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in generative AI have enabled ubiquitous access to large
language models (LLMs). Empowered by their exceptional capabilities to
understand and generate human-like text, these models are being increasingly
integrated into our society. At the same time, there are also concerns on the
potential misuse of this powerful technology, prompting defensive measures from
service providers. To overcome such protection, jailbreaking prompts have
recently emerged as one of the most effective mechanisms to circumvent security
restrictions and elicit harmful content originally designed to be prohibited.
  Due to the rapid development of LLMs and their ease of access via natural
languages, the frontline of jailbreak prompts is largely seen in online forums
and among hobbyists. To gain a better understanding of the threat landscape of
semantically meaningful jailbreak prompts, we systemized existing prompts and
measured their jailbreak effectiveness empirically. Further, we conducted a
user study involving 92 participants with diverse backgrounds to unveil the
process of manually creating jailbreak prompts. We observed that users often
succeeded in jailbreak prompts generation regardless of their expertise in
LLMs. Building on the insights from the user study, we also developed a system
using AI as the assistant to automate the process of jailbreak prompt
generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by USENIX Security 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ JMultiWOZ: A Large-Scale Japanese Multi-Domain Task-Oriented Dialogue
  <span class="highlight-title">Dataset</span> <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17319v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17319v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Atsumoto Ohashi, Ryu Hirai, Shinya Iizuka, Ryuichiro Higashinaka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dialogue datasets are crucial for deep learning-based task-oriented dialogue
system research. While numerous English language multi-domain task-oriented
dialogue datasets have been developed and contributed to significant
advancements in task-oriented dialogue systems, such a dataset does not exist
in Japanese, and research in this area is limited compared to that in English.
In this study, towards the advancement of research and development of
task-oriented dialogue systems in Japanese, we constructed JMultiWOZ, the first
Japanese language large-scale multi-domain task-oriented dialogue dataset.
Using JMultiWOZ, we evaluated the dialogue state tracking and response
generation capabilities of the state-of-the-art methods on the existing major
English benchmark dataset MultiWOZ2.2 and the latest large language model
(LLM)-based methods. Our evaluation results demonstrated that JMultiWOZ
provides a benchmark that is on par with MultiWOZ2.2. In addition, through
evaluation experiments of interactive dialogues with the models and human
participants, we identified limitations in the task completion capabilities of
LLMs in Japanese.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Project MOSLA: Recording Every Moment of Second Language Acquisition <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17314v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17314v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Masato Hagiwara, Joshua Tanner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Second language acquisition (SLA) is a complex and dynamic process. Many SLA
studies that have attempted to record and analyze this process have typically
focused on a single modality (e.g., textual output of learners), covered only a
short period of time, and/or lacked control (e.g., failed to capture every
aspect of the learning process). In Project MOSLA (Moments of Second Language
Acquisition), we have created a longitudinal, multimodal, multilingual, and
controlled dataset by inviting participants to learn one of three target
languages (Arabic, Spanish, and Chinese) from scratch over a span of two years,
exclusively through online instruction, and recording every lesson using Zoom.
The dataset is semi-automatically annotated with speaker/language IDs and
transcripts by both human annotators and fine-tuned state-of-the-art speech
models. Our experiments reveal linguistic insights into learners' proficiency
development over time, as well as the potential for automatically detecting the
areas of focus on the screen purely from the unannotated multimodal data. Our
dataset is freely available for research purposes and can serve as a valuable
resource for a wide range of applications, including but not limited to SLA,
proficiency assessment, language and speech processing, pedagogy, and
multimodal learning analytics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural Multimodal Topic Modeling: A Comprehensive Evaluation <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17308v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17308v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Felipe González-Pizarro, Giuseppe Carenini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural topic models can successfully find coherent and diverse topics in
textual data. However, they are limited in dealing with multimodal datasets
(e.g., images and text). This paper presents the first systematic and
comprehensive evaluation of multimodal topic modeling of documents containing
both text and images. In the process, we propose two novel topic modeling
solutions and two novel evaluation metrics. Overall, our evaluation on an
unprecedented rich and diverse collection of datasets indicates that both of
our models generate coherent and diverse topics. Nevertheless, the extent to
which one method outperforms the other depends on the metrics and dataset
combinations, which suggests further exploration of hybrid solutions in the
future. Notably, our succinct human evaluation aligns with the outcomes
determined by our proposed metrics. This alignment not only reinforces the
credibility of our metrics but also highlights the potential for their
application in guiding future multimodal topic modeling endeavors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Camera-Ready for LREC-COLING 2024 (Long Paper)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HILL: Hierarchy-aware Information Lossless Contrastive Learning for
  Hierarchical Text Classification <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17307v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17307v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        He Zhu, Junran Wu, Ruomei Liu, Yue Hou, Ze Yuan, Shangzhe Li, Yicheng Pan, Ke Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing self-supervised methods in natural language processing (NLP),
especially hierarchical text classification (HTC), mainly focus on
self-supervised contrastive learning, extremely relying on human-designed
augmentation rules to generate contrastive samples, which can potentially
corrupt or distort the original information. In this paper, we tend to
investigate the feasibility of a contrastive learning scheme in which the
semantic and syntactic information inherent in the input sample is adequately
reserved in the contrastive samples and fused during the learning process.
Specifically, we propose an information lossless contrastive learning strategy
for HTC, namely \textbf{H}ierarchy-aware \textbf{I}nformation \textbf{L}ossless
contrastive \textbf{L}earning (HILL), which consists of a text encoder
representing the input document, and a structure encoder directly generating
the positive sample. The structure encoder takes the document embedding as
input, extracts the essential syntactic information inherent in the label
hierarchy with the principle of structural entropy minimization, and injects
the syntactic information into the text representation via hierarchical
representation learning. Experiments on three common datasets are conducted to
verify the superiority of HILL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Decoding Probing: Revealing Internal Linguistic Structures in Neural
  Language Models using Minimal Pairs <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17299v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17299v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linyang He, Peili Chen, Ercong Nie, Yuanning Li, Jonathan R. Brennan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inspired by cognitive neuroscience studies, we introduce a novel `decoding
probing' method that uses minimal pairs benchmark (BLiMP) to probe internal
linguistic characteristics in neural language models layer by layer. By
treating the language model as the `brain' and its representations as `neural
activations', we decode grammaticality labels of minimal pairs from the
intermediate layers' representations. This approach reveals: 1) Self-supervised
language models capture abstract linguistic structures in intermediate layers
that GloVe and RNN language models cannot learn. 2) Information about syntactic
grammaticality is robustly captured through the first third layers of GPT-2 and
also distributed in later layers. As sentence complexity increases, more layers
are required for learning grammatical capabilities. 3) Morphological and
semantics/syntax interface-related features are harder to capture than syntax.
4) For Transformer-based models, both embeddings and attentions capture
grammatical features but show distinct patterns. Different attention heads
exhibit similar tendencies toward various linguistic phenomena, but with varied
contributions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ InternLM2 Technical Report 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17297v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17297v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei Chu, Xiaoyi Dong, Haodong Duan, Qi Fan, Zhaoye Fei, Yang Gao, Jiaye Ge, Chenya Gu, Yuzhe Gu, Tao Gui, Aijia Guo, Qipeng Guo, Conghui He, Yingfan Hu, Ting Huang, Tao Jiang, Penglong Jiao, Zhenjiang Jin, Zhikai Lei, Jiaxing Li, Jingwen Li, Linyang Li, Shuaibin Li, Wei Li, Yining Li, Hongwei Liu, Jiangning Liu, Jiawei Hong, Kaiwen Liu, Kuikun Liu, Xiaoran Liu, Chengqi Lv, Haijun Lv, Kai Lv, Li Ma, Runyuan Ma, Zerun Ma, Wenchang Ning, Linke Ouyang, Jiantao Qiu, Yuan Qu, Fukai Shang, Yunfan Shao, Demin Song, Zifan Song, Zhihao Sui, Peng Sun, Yu Sun, Huanze Tang, Bin Wang, Guoteng Wang, Jiaqi Wang, Jiayu Wang, Rui Wang, Yudong Wang, Ziyi Wang, Xingjian Wei, Qizhen Weng, Fan Wu, Yingtong Xiong, Chao Xu, Ruiliang Xu, Hang Yan, Yirong Yan, Xiaogui Yang, Haochen Ye, Huaiyuan Ying, Jia Yu, Jing Yu, Yuhang Zang, Chuyu Zhang, Li Zhang, Pan Zhang, Peng Zhang, Ruijie Zhang, Shuo Zhang, Songyang Zhang, Wenjian Zhang, Wenwei Zhang, Xingcheng Zhang, Xinyue Zhang, Hui Zhao, Qian Zhao, Xiaomeng Zhao, Fengzhe Zhou, Zaida Zhou, Jingming Zhuo, Yicheng Zou, Xipeng Qiu, Yu Qiao, Dahua Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The evolution of Large Language Models (LLMs) like ChatGPT and GPT-4 has
sparked discussions on the advent of Artificial General Intelligence (AGI).
However, replicating such advancements in open-source models has been
challenging. This paper introduces InternLM2, an open-source LLM that
outperforms its predecessors in comprehensive evaluations across 6 dimensions
and 30 benchmarks, long-context modeling, and open-ended subjective evaluations
through innovative pre-training and optimization techniques. The pre-training
process of InternLM2 is meticulously detailed, highlighting the preparation of
diverse data types including text, code, and long-context data. InternLM2
efficiently captures long-term dependencies, initially trained on 4k tokens
before advancing to 32k tokens in pre-training and fine-tuning stages,
exhibiting remarkable performance on the 200k ``Needle-in-a-Haystack" test.
InternLM2 is further aligned using Supervised Fine-Tuning (SFT) and a novel
Conditional Online Reinforcement Learning from Human Feedback (COOL RLHF)
strategy that addresses conflicting human preferences and reward hacking. By
releasing InternLM2 models in different training stages and model sizes, we
provide the community with insights into the model's evolution.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Common Ground Tracking in Multimodal Dialogue 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17284v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17284v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ibrahim Khebour, Kenneth Lai, Mariah Bradford, Yifan Zhu, Richard Brutti, Christopher Tam, Jingxuan Tu, Benjamin Ibarra, Nathaniel Blanchard, Nikhil Krishnaswamy, James Pustejovsky
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Within Dialogue Modeling research in AI and NLP, considerable attention has
been spent on ``dialogue state tracking'' (DST), which is the ability to update
the representations of the speaker's needs at each turn in the dialogue by
taking into account the past dialogue moves and history. Less studied but just
as important to dialogue modeling, however, is ``common ground tracking''
(CGT), which identifies the shared belief space held by all of the participants
in a task-oriented dialogue: the task-relevant propositions all participants
accept as true. In this paper we present a method for automatically identifying
the current set of shared beliefs and ``questions under discussion'' (QUDs) of
a group with a shared goal. We annotate a dataset of multimodal interactions in
a shared physical space with speech transcriptions, prosodic features,
gestures, actions, and facets of collaboration, and operationalize these
features for use in a deep neural model to predict moves toward construction of
common ground. Model outputs cascade into a set of formal closure rules derived
from situated evidence and belief axioms and update operations. We empirically
assess the contribution of each feature type toward successful construction of
common ground relative to ground truth, establishing a benchmark in this novel,
challenging task.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automate Knowledge Concept Tagging on Math Questions with LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17281v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17281v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hang Li, Tianlong Xu, Jiliang Tang, Qingsong Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge concept tagging for questions plays a crucial role in contemporary
intelligent educational applications, including learning progress diagnosis,
practice question recommendations, and course content organization.
Traditionally, these annotations have been conducted manually with help from
pedagogical experts, as the task requires not only a strong semantic
understanding of both question stems and knowledge definitions but also deep
insights into connecting question-solving logic with corresponding knowledge
concepts. In this paper, we explore automating the tagging task using Large
Language Models (LLMs), in response to the inability of prior manual methods to
meet the rapidly growing demand for concept tagging in questions posed by
advanced educational applications. Moreover, the zero/few-shot learning
capability of LLMs makes them well-suited for application in educational
scenarios, which often face challenges in collecting large-scale,
expertise-annotated datasets. By conducting extensive experiments with a
variety of representative LLMs, we demonstrate that LLMs are a promising tool
for concept tagging in math questions. Furthermore, through case studies
examining the results from different LLMs, we draw some empirical conclusions
about the key factors for success in applying LLMs to the automatic concept
tagging task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LocalTweets to LocalHealth: A Mental Health Surveillance Framework Based
  on Twitter Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.13452v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.13452v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vijeta Deshpande, Minhwa Lee, Zonghai Yao, Zihao Zhang, Jason Brian Gibbons, Hong Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prior research on Twitter (now X) data has provided positive evidence of its
utility in developing supplementary health surveillance systems. In this study,
we present a new framework to surveil public health, focusing on mental health
(MH) outcomes. We hypothesize that locally posted tweets are indicative of
local MH outcomes and collect tweets posted from 765 neighborhoods (census
block groups) in the USA. We pair these tweets from each neighborhood with the
corresponding MH outcome reported by the Center for Disease Control (CDC) to
create a benchmark dataset, LocalTweets. With LocalTweets, we present the first
population-level evaluation task for Twitter-based MH surveillance systems. We
then develop an efficient and effective method, LocalHealth, for predicting MH
outcomes based on LocalTweets. When used with GPT3.5, LocalHealth achieves the
highest F1-score and accuracy of 0.7429 and 79.78\%, respectively, a 59\%
improvement in F1-score over the GPT3.5 in zero-shot setting. We also utilize
LocalHealth to extrapolate CDC's estimates to proxy unreported neighborhoods,
achieving an F1-score of 0.7291. Our work suggests that Twitter data can be
effectively leveraged to simulate neighborhood-level MH outcomes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Simple and Scalable Strategies to Continually <span class="highlight-title">Pre-train</span> Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08763v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08763v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adam Ibrahim, Benjamin Thérien, Kshitij Gupta, Mats L. Richter, Quentin Anthony, Timothée Lesort, Eugene Belilovsky, Irina Rish
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are routinely pre-trained on billions of tokens,
only to start the process over again once new data becomes available. A much
more efficient solution is to continually pre-train these models, saving
significant compute compared to re-training. However, the distribution shift
induced by new data typically results in degraded performance on previous data
or poor adaptation to the new data. In this work, we show that a simple and
scalable combination of learning rate (LR) re-warming, LR re-decaying, and
replay of previous data is sufficient to match the performance of fully
re-training from scratch on all available data, as measured by the final loss
and the average score on several language model (LM) evaluation benchmarks.
Specifically, we show this for a weak but realistic distribution shift between
two commonly used LLM pre-training datasets (English$\rightarrow$English) and a
stronger distribution shift (English$\rightarrow$German) at the $405$M
parameter model scale with large dataset sizes (hundreds of billions of
tokens). Selecting the weak but realistic shift for larger-scale experiments,
we also find that our continual learning strategies match the re-training
baseline for a 10B parameter LLM. Our results demonstrate that LLMs can be
successfully updated via simple and scalable continual learning strategies,
matching the re-training baseline using only a fraction of the compute.
Finally, inspired by previous work, we propose alternatives to the cosine
learning rate schedule that help circumvent forgetting induced by LR re-warming
and that are not bound to a fixed token budget.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large Language Models Offer an Alternative to the Traditional Approach
  of Topic Modelling <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16248v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16248v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yida Mu, Chun Dong, Kalina Bontcheva, Xingyi Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Topic modelling, as a well-established unsupervised technique, has found
extensive use in automatically detecting significant topics within a corpus of
documents. However, classic topic modelling approaches (e.g., LDA) have certain
drawbacks, such as the lack of semantic understanding and the presence of
overlapping topics. In this work, we investigate the untapped potential of
large language models (LLMs) as an alternative for uncovering the underlying
topics within extensive text corpora. To this end, we introduce a framework
that prompts LLMs to generate topics from a given set of documents and
establish evaluation protocols to assess the clustering efficacy of LLMs. Our
findings indicate that LLMs with appropriate prompts can stand out as a viable
alternative, capable of generating relevant topic titles and adhering to human
guidelines to refine and merge topics. Through in-depth experiments and
evaluation, we summarise the advantages and constraints of employing LLMs in
topic extraction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AI and Generative AI for Research Discovery and Summarization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.06795v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.06795v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mark Glickman, Yi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  AI and generative AI tools, including chatbots like ChatGPT that rely on
large language models (LLMs), have burst onto the scene this year, creating
incredible opportunities to increase work productivity and improve our lives.
Statisticians and data scientists have begun experiencing the benefits from the
availability of these tools in numerous ways, such as the generation of
programming code from text prompts to analyze data or fit statistical models.
One area that these tools can make a substantial impact is in research
discovery and summarization. Standalone tools and plugins to chatbots are being
developed that allow researchers to more quickly find relevant literature than
pre-2023 search tools. Furthermore, generative AI tools have improved to the
point where they can summarize and extract the key points from research
articles in succinct language. Finally, chatbots based on highly parameterized
LLMs can be used to simulate abductive reasoning, which provides researchers
the ability to make connections among related technical topics, which can also
be used for research discovery. We review the developments in AI and generative
AI for research discovery and summarization, and propose directions where these
types of tools are likely to head in the future that may be of interest to
statistician and data scientists.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generator-Retriever-Generator Approach for Open-Domain Question
  Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.11278v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.11278v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdelrahman Abdallah, Adam Jatowt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Open-domain question answering (QA) tasks usually require the retrieval of
relevant information from a large corpus to generate accurate answers. We
propose a novel approach called Generator-Retriever-Generator (GRG) that
combines document retrieval techniques with a large language model (LLM), by
first prompting the model to generate contextual documents based on a given
question. In parallel, a dual-encoder network retrieves documents that are
relevant to the question from an external corpus. The generated and retrieved
documents are then passed to the second LLM, which generates the final answer.
By combining document retrieval and LLM generation, our approach addresses the
challenges of open-domain QA, such as generating informative and contextually
relevant answers. GRG outperforms the state-of-the-art generate-then-read and
retrieve-then-read pipelines (GENREAD and RFiD) improving their performance by
at least by +5.2, +4.2, and +1.6 on TriviaQA, NQ, and WebQ datasets,
respectively. We provide code, datasets, and checkpoints at
https://github.com/abdoelsayed2016/GRG.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Chat<span class="highlight-title">GPT</span> Needs SPADE (Sustainability, PrivAcy, Digital divide, and
  Ethics) Evaluation: A <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.03123v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.03123v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sunder Ali Khowaja, Parus Khuwaja, Kapal Dev, Weizheng Wang, Lewis Nkenyereye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  ChatGPT is another large language model (LLM) vastly available for the
consumers on their devices but due to its performance and ability to converse
effectively, it has gained a huge popularity amongst research as well as
industrial community. Recently, many studies have been published to show the
effectiveness, efficiency, integration, and sentiments of chatGPT and other
LLMs. In contrast, this study focuses on the important aspects that are mostly
overlooked, i.e. sustainability, privacy, digital divide, and ethics and
suggests that not only chatGPT but every subsequent entry in the category of
conversational bots should undergo Sustainability, PrivAcy, Digital divide, and
Ethics (SPADE) evaluation. This paper discusses in detail the issues and
concerns raised over chatGPT in line with aforementioned characteristics. We
also discuss the recent EU AI Act briefly in accordance with the SPADE
evaluation. We support our hypothesis by some preliminary data collection and
visualizations along with hypothesized facts. We also suggest mitigations and
recommendations for each of the concerns. Furthermore, we also suggest some
policies and recommendations for AI policy act, if designed by the governments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 8 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AMuRD: Annotated Arabic-English Receipt <span class="highlight-title">Dataset</span> for Key Information
  Extraction and Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.09800v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.09800v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdelrahman Abdallah, Mahmoud Abdalla, Mohamed Elkasaby, Yasser Elbendary, Adam Jatowt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The extraction of key information from receipts is a complex task that
involves the recognition and extraction of text from scanned receipts. This
process is crucial as it enables the retrieval of essential content and
organizing it into structured documents for easy access and analysis. In this
paper, we present AMuRD, a novel multilingual human-annotated dataset
specifically designed for information extraction from receipts. This dataset
comprises $47,720$ samples and addresses the key challenges in information
extraction and item classification - the two critical aspects of data analysis
in the retail industry. Each sample includes annotations for item names and
attributes such as price, brand, and more. This detailed annotation facilitates
a comprehensive understanding of each item on the receipt. Furthermore, the
dataset provides classification into $44$ distinct product categories. This
classification feature allows for a more organized and efficient analysis of
the items, enhancing the usability of the dataset for various applications. In
our study, we evaluated various language model architectures, e.g., by
fine-tuning LLaMA models on the AMuRD dataset. Our approach yielded exceptional
results, with an F1 score of 97.43\% and accuracy of 94.99\% in information
extraction and classification, and an even higher F1 score of 98.51\% and
accuracy of 97.06\% observed in specific tasks. The dataset and code are
publicly accessible for further
researchhttps://github.com/Update-For-Integrated-Business-AI/AMuRD.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Training <span class="highlight-title">BERT</span> Models to Carry Over a Coding System Developed on One
  Corpus to Another <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.03742v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.03742v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dalma Galambos, Pál Zsámboki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper describes how we train BERT models to carry over a coding system
developed on the paragraphs of a Hungarian literary journal to another. The aim
of the coding system is to track trends in the perception of literary
translation around the political transformation in 1989 in Hungary. To evaluate
not only task performance but also the consistence of the annotation, moreover,
to get better predictions from an ensemble, we use 10-fold crossvalidation.
Extensive hyperparameter tuning is used to obtain the best possible results and
fair comparisons. To handle label imbalance, we use loss functions and metrics
robust to it. Evaluation of the effect of domain shift is carried out by
sampling a test set from the target domain. We establish the sample size by
estimating the bootstrapped confidence interval via simulations. This way, we
show that our models can carry over one annotation system to the target domain.
Comparisons are drawn to provide insights such as learning multilabel
correlations and confidence penalty improve resistance to domain shift, and
domain adaptation on OCR-ed text on another domain improves performance almost
to the same extent as that on the corpus under study. See our code at
https://codeberg.org/zsamboki/bert-annotator-ensemble.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Camera-ready version, to be presented at the 2024 Joint International
  Conference on Computational Linguistics, Language Resources and Evaluation
  (LREC-COLING 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient <span class="highlight-title">Pre-train</span>ing for Localized Instruction Generation of Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.15964v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.15964v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anil Batra, Davide Moltisanti, Laura Sevilla-Lara, Marcus Rohrbach, Frank Keller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Procedural videos show step-by-step demonstrations of tasks like recipe
preparation. Understanding such videos is challenging, involving the precise
localization of steps and the generation of textual instructions. Manually
annotating steps and writing instructions is costly, which limits the size of
current datasets and hinders effective learning. Leveraging large but noisy
video-transcript datasets for pre-training can boost performance, but demands
significant computational resources. Furthermore, transcripts contain
irrelevant content and exhibit style variation compared to instructions written
by human annotators. To mitigate both issues, we propose a technique,
Sieve-&-Swap, to automatically curate a smaller dataset: (i) Sieve filters
irrelevant transcripts and (ii) Swap enhances the quality of the text
instruction by automatically replacing the transcripts with human-written
instructions from a text-only recipe dataset. The curated dataset, three orders
of magnitude smaller than current web-scale datasets, enables efficient
training of large-scale models with competitive performance. We complement our
Sieve-\&-Swap approach with a Procedure Transformer (ProcX) for end-to-end step
localization and instruction generation for procedural videos. When this model
is pre-trained on our curated dataset, it achieves state-of-the-art performance
in zero-shot and finetuning settings on YouCook2 and Tasty, while using a
fraction of the computational resources.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This version has some missing experiments and elaborative technical
  details</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Blinded by Generated Contexts: How Language Models Merge Generated and
  Retrieved Contexts for Open-Domain QA? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.11911v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.11911v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hexiang Tan, Fei Sun, Wanli Yang, Yuanzhuo Wang, Qi Cao, Xueqi Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While auxiliary information has become a key to enhancing Large Language
Models (LLMs), relatively little is known about how LLMs merge these contexts,
specifically contexts generated by LLMs and those retrieved from external
sources. To investigate this, we formulate a systematic framework to identify
whether LLMs' responses, derived from the integration of generated and
retrieved contexts, are attributed to either generated or retrieved contexts.
To easily trace the origin of the response, we construct datasets with
conflicting contexts, i.e., each question is paired with both generated and
retrieved contexts, yet only one of them contains the correct answer. Our
experiments reveal a significant bias in several LLMs (GPT-4/3.5 and Llama2) to
favor generated contexts, even when they provide incorrect information. We
further identify two key factors contributing to this bias: i) contexts
generated by LLMs typically show greater similarity to the questions,
increasing their likelihood of being selected; ii) the segmentation process
used in retrieved contexts disrupts their completeness, thereby hindering their
full utilization in LLMs. Our analysis enhances the understanding of how LLMs
merge diverse contexts, offering valuable insights for advancing current
augmentation methods for LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Measuring Entrainment in Spontaneous Code-switched Speech <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.07703v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.07703v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Debasmita Bhattacharya, Siying Ding, Alayna Nguyen, Julia Hirschberg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It is well-known that speakers who entrain to one another have more
successful conversations than those who do not. Previous research has shown
that interlocutors entrain on linguistic features in both written and spoken
monolingual domains. More recent work on code-switched communication has also
shown preliminary evidence of entrainment on certain aspects of code-switching
(CSW). However, such studies of entrainment in code-switched domains have been
extremely few and restricted to human-machine textual interactions. Our work
studies code-switched spontaneous speech between humans, finding that (1)
patterns of written and spoken entrainment in monolingual settings largely
generalize to code-switched settings, and (2) some patterns of entrainment on
code-switching in dialogue agent-generated text generalize to spontaneous
code-switched speech. Our findings give rise to important implications for the
potentially "universal" nature of entrainment as a communication phenomenon,
and potential applications in inclusive and interactive speech technology.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Edits: camera-ready manuscript for NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Decode Neural signal as Speech 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.01748v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.01748v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiqian Yang, Yiqun Duan, Qiang Zhang, Renjing Xu, Hui Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Decoding language from brain dynamics is an important open direction in the
realm of brain-computer interface (BCI), especially considering the rapid
growth of large language models. Compared to invasive-based signals which
require electrode implantation surgery, non-invasive neural signals (e.g. EEG,
MEG) have attracted increasing attention considering their safety and
generality. However, the exploration is not adequate in three aspects: 1)
previous methods mainly focus on EEG but none of the previous works address
this problem on MEG with better signal quality; 2) prior works have
predominantly used ``teacher-forcing" during generative decoding, which is
impractical; 3) prior works are mostly ``BART-based" not fully auto-regressive,
which performs better in other sequence tasks. In this paper, we explore the
brain-to-text translation of MEG signals in a speech-decoding formation. Here
we are the first to investigate a cross-attention-based ``whisper" model for
generating text directly from MEG signals without teacher forcing. Our model
achieves impressive BLEU-1 scores of 60.30 and 52.89 without pretraining \&
teacher-forcing on two major datasets (\textit{GWilliams} and
\textit{Schoffelen}). This paper conducts a comprehensive review to understand
how speech decoding formation performs on the neural decoding tasks, including
pretraining initialization, training \& evaluation set splitting, augmentation,
and scaling law.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploiting Semantic Reconstruction to Mitigate Hallucinations in
  Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16167v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16167v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minchan Kim, Minyeong Kim, Junik Bae, Suhwan Choi, Sungkyung Kim, Buru Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hallucinations in vision-language models pose a significant challenge to
their reliability, particularly in the generation of long captions. Current
methods fall short of accurately identifying and mitigating these
hallucinations. To address this issue, we introduce ESREAL, a novel
unsupervised learning framework designed to suppress the generation of
hallucinations through accurate localization and penalization of hallucinated
tokens. Initially, ESREAL creates a reconstructed image based on the generated
caption and aligns its corresponding regions with those of the original image.
This semantic reconstruction aids in identifying both the presence and type of
token-level hallucinations within the generated caption. Subsequently, ESREAL
computes token-level hallucination scores by assessing the semantic similarity
of aligned regions based on the type of hallucination. Finally, ESREAL employs
a proximal policy optimization algorithm, where it selectively penalizes
hallucinated tokens according to their token-level hallucination scores. Our
framework notably reduces hallucinations in LLaVA, InstructBLIP, and mPLUG-Owl2
by 32.81%, 27.08%, and 7.46% on the CHAIR metric. This improvement is achieved
solely through signals derived from the image itself, without the need for any
image-text pairs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Accelerating Scientific Discovery with Generative Knowledge Extraction,
  Graph-Based Representation, and Multimodal Intelligent Graph Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11996v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11996v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Markus J. Buehler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Leveraging generative Artificial Intelligence (AI), we have transformed a
dataset comprising 1,000 scientific papers into an ontological knowledge graph.
Through an in-depth structural analysis, we have calculated node degrees,
identified communities and connectivities, and evaluated clustering
coefficients and betweenness centrality of pivotal nodes, uncovering
fascinating knowledge architectures. The graph has an inherently scale-free
nature, is highly connected, and can be used for graph reasoning by taking
advantage of transitive and isomorphic properties that reveal unprecedented
interdisciplinary relationships that can be used to answer queries, identify
gaps in knowledge, propose never-before-seen material designs, and predict
material behaviors. We compute deep node embeddings for combinatorial node
similarity ranking for use in a path sampling strategy links dissimilar
concepts that have previously not been related. One comparison revealed
structural parallels between biological materials and Beethoven's 9th Symphony,
highlighting shared patterns of complexity through isomorphic mapping. In
another example, the algorithm proposed a hierarchical mycelium-based composite
based on integrating path sampling with principles extracted from Kandinsky's
'Composition VII' painting. The resulting material integrates an innovative set
of concepts that include a balance of chaos/order, adjustable porosity,
mechanical strength, and complex patterned chemical functionalization. We
uncover other isomorphisms across science, technology and art, revealing a
nuanced ontology of immanence that reveal a context-dependent heterarchical
interplay of constituents. Graph-based generative AI achieves a far higher
degree of novelty, explorative capacity, and technical detail, than
conventional approaches and establishes a widely useful framework for
innovation by revealing hidden connections.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unveiling the Pitfalls of Knowledge Editing for Large Language Models <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02129v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02129v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhoubo Li, Ningyu Zhang, Yunzhi Yao, Mengru Wang, Xi Chen, Huajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the cost associated with fine-tuning Large Language Models (LLMs)
continues to rise, recent research efforts have pivoted towards developing
methodologies to edit implicit knowledge embedded within LLMs. Yet, there's
still a dark cloud lingering overhead -- will knowledge editing trigger
butterfly effect? since it is still unclear whether knowledge editing might
introduce side effects that pose potential risks or not. This paper pioneers
the investigation into the potential pitfalls associated with knowledge editing
for LLMs. To achieve this, we introduce new benchmark datasets and propose
innovative evaluation metrics. Our results underline two pivotal concerns: (1)
Knowledge Conflict: Editing groups of facts that logically clash can magnify
the inherent inconsistencies in LLMs-a facet neglected by previous methods. (2)
Knowledge Distortion: Altering parameters with the aim of editing factual
knowledge can irrevocably warp the innate knowledge structure of LLMs.
Experimental results vividly demonstrate that knowledge editing might
inadvertently cast a shadow of unintended consequences on LLMs, which warrant
attention and efforts for future works. Code and data are available at
https://github.com/zjunlp/PitfallsKnowledgeEditing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CLASSLA-web: Comparable Web Corpora of South Slavic Languages Enriched
  with Linguistic and Genre Annotation <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12721v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12721v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikola Ljubešić, Taja Kuzman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a collection of highly comparable web corpora of
Slovenian, Croatian, Bosnian, Montenegrin, Serbian, Macedonian, and Bulgarian,
covering thereby the whole spectrum of official languages in the South Slavic
language space. The collection of these corpora comprises a total of 13 billion
tokens of texts from 26 million documents. The comparability of the corpora is
ensured by a comparable crawling setup and the usage of identical crawling and
post-processing technology. All the corpora were linguistically annotated with
the state-of-the-art CLASSLA-Stanza linguistic processing pipeline, and
enriched with document-level genre information via the Transformer-based
multilingual X-GENRE classifier, which further enhances comparability at the
level of linguistic annotation and metadata enrichment. The genre-focused
analysis of the resulting corpora shows a rather consistent distribution of
genres throughout the seven corpora, with variations in the most prominent
genre categories being well-explained by the economic strength of each language
community. A comparison of the distribution of genre categories across the
corpora indicates that web corpora from less developed countries primarily
consist of news articles. Conversely, web corpora from economically more
developed countries exhibit a smaller proportion of news content, with a
greater presence of promotional and opinionated texts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the LREC-COLING 2024 conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unleashing the Emergent Cognitive Synergy in Large Language Models: A
  Task-Solving Agent through Multi-Persona Self-Collaboration <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05300v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05300v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenhailong Wang, Shaoguang Mao, Wenshan Wu, Tao Ge, Furu Wei, Heng Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human intelligence thrives on cognitive synergy, where collaboration among
different minds yield superior outcomes compared to isolated individuals. In
this work, we propose Solo Performance Prompting (SPP), which transforms a
single LLM into a cognitive synergist by engaging in multi-turn
self-collaboration with multiple personas. A cognitive synergist is an
intelligent agent that collaboratively combines multiple minds' strengths and
knowledge to enhance problem-solving in complex tasks. By dynamically
identifying and simulating different personas based on task inputs, SPP
unleashes the potential of cognitive synergy in LLMs. Our in-depth analysis
shows that assigning multiple fine-grained personas in LLMs improves
problem-solving abilities compared to using a single or fixed number of
personas. We evaluate SPP on three challenging tasks: Trivia Creative Writing,
Codenames Collaborative, and Logic Grid Puzzle, encompassing both
knowledge-intensive and reasoning-intensive types. Unlike previous works, such
as Chain-of-Thought, that solely enhance the reasoning abilities in LLMs,
experimental results demonstrate that SPP effectively reduces factual
hallucination, and maintains strong reasoning capabilities. Additionally,
comparative experiments show that cognitive synergy only emerges in GPT-4 and
does not appear in less capable models, such as GPT-3.5-turbo and
Llama2-13b-chat, which draws an interesting analogy to human development. Code,
data, and prompts can be found at:
https://github.com/MikeWangWZHL/Solo-Performance-Prompting.git.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as a main conference paper at NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring Representational Disparities Between Multilingual and
  Bilingual Translation Models <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.14230v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.14230v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Neha Verma, Kenton Murray, Kevin Duh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multilingual machine translation has proven immensely useful for both
parameter efficiency and overall performance across many language pairs via
complete multilingual parameter sharing. However, some language pairs in
multilingual models can see worse performance than in bilingual models,
especially in the one-to-many translation setting. Motivated by their empirical
differences, we examine the geometric differences in representations from
bilingual models versus those from one-to-many multilingual models.
Specifically, we compute the isotropy of these representations using intrinsic
dimensionality and IsoScore, in order to measure how the representations
utilize the dimensions in their underlying vector space. Using the same
evaluation data in both models, we find that for a given language pair, its
multilingual model decoder representations are consistently less isotropic and
occupy fewer dimensions than comparable bilingual model decoder
representations. Additionally, we show that much of the anisotropy in
multilingual decoder representations can be attributed to modeling
language-specific information, therefore limiting remaining representational
capacity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FENICE: Factuality Evaluation of summarization based on Natural language
  Inference and Claim Extraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.02270v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.02270v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alessandro Scirè, Karim Ghonim, Roberto Navigli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in text summarization, particularly with the advent of
Large Language Models (LLMs), have shown remarkable performance. However, a
notable challenge persists as a substantial number of automatically-generated
summaries exhibit factual inconsistencies, such as hallucinations. In response
to this issue, various approaches for the evaluation of consistency for
summarization have emerged. Yet, these newly-introduced metrics face several
limitations, including lack of interpretability, focus on short document
summaries (e.g., news articles), and computational impracticality, especially
for LLM-based metrics. To address these shortcomings, we propose Factuality
Evaluation of summarization based on Natural language Inference and Claim
Extraction (FENICE), a more interpretable and efficient factuality-oriented
metric. FENICE leverages an NLI-based alignment between information in the
source document and a set of atomic facts, referred to as claims, extracted
from the summary. Our metric sets a new state of the art on AGGREFACT, the
de-facto benchmark for factuality evaluation. Moreover, we extend our
evaluation to a more challenging setting by conducting a human annotation
process of long-form summarization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, long paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Coarse-Tuning for Ad-hoc Document Retrieval Using <span class="highlight-title">Pre-train</span>ed Language
  Models <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16915v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16915v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Atsushi Keyaki, Ribeka Keyaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning in information retrieval systems using pre-trained language
models (PLM-based IR) requires learning query representations and
query-document relations, in addition to downstream task-specific learning.
This study introduces coarse-tuning as an intermediate learning stage that
bridges pre-training and fine-tuning. By learning query representations and
query-document relations in coarse-tuning, we aim to reduce the load of
fine-tuning and improve the learning effect of downstream IR tasks. We propose
Query-Document Pair Prediction (QDPP) for coarse-tuning, which predicts the
appropriateness of query-document pairs. Evaluation experiments show that the
proposed method significantly improves MRR and/or nDCG@5 in four ad-hoc
document retrieval datasets. Furthermore, the results of the query prediction
task suggested that coarse-tuning facilitated learning of query representation
and query-document relations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EthioLLM: Multilingual Large Language Models for Ethiopian Languages
  with Task Evaluation <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13737v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13737v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Atnafu Lambebo Tonja, Israel Abebe Azime, Tadesse Destaw Belay, Mesay Gemeda Yigezu, Moges Ahmed Mehamed, Abinew Ali Ayele, Ebrahim Chekol Jibril, Michael Melese Woldeyohannis, Olga Kolesnikova, Philipp Slusallek, Dietrich Klakow, Shengwu Xiong, Seid Muhie Yimam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have gained popularity recently due to their
outstanding performance in various downstream Natural Language Processing (NLP)
tasks. However, low-resource languages are still lagging behind current
state-of-the-art (SOTA) developments in the field of NLP due to insufficient
resources to train LLMs. Ethiopian languages exhibit remarkable linguistic
diversity, encompassing a wide array of scripts, and are imbued with profound
religious and cultural significance. This paper introduces EthioLLM --
multilingual large language models for five Ethiopian languages (Amharic,
Ge'ez, Afan Oromo, Somali, and Tigrinya) and English, and Ethiobenchmark -- a
new benchmark dataset for various downstream NLP tasks. We evaluate the
performance of these models across five downstream NLP tasks. We open-source
our multilingual language models, new benchmark datasets for various downstream
tasks, and task-specific fine-tuned language models and discuss the performance
of the models. Our dataset and models are available at the
https://huggingface.co/EthioNLP repository.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-Coling 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Design Space for Intelligent and Interactive Writing Assistants 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14117v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14117v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mina Lee, Katy Ilonka Gero, John Joon Young Chung, Simon Buckingham Shum, Vipul Raheja, Hua Shen, Subhashini Venugopalan, Thiemo Wambsganss, David Zhou, Emad A. Alghamdi, Tal August, Avinash Bhat, Madiha Zahrah Choksi, Senjuti Dutta, Jin L. C. Guo, Md Naimul Hoque, Yewon Kim, Simon Knight, Seyed Parsa Neshaei, Agnia Sergeyuk, Antonette Shibani, Disha Shrivastava, Lila Shroff, Jessi Stark, Sarah Sterman, Sitong Wang, Antoine Bosselut, Daniel Buschek, Joseph Chee Chang, Sherol Chen, Max Kreminski, Joonsuk Park, Roy Pea, Eugenia H. Rho, Shannon Zejiang Shen, Pao Siangliulue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In our era of rapid technological advancement, the research landscape for
writing assistants has become increasingly fragmented across various research
communities. We seek to address this challenge by proposing a design space as a
structured way to examine and explore the multidimensional space of intelligent
and interactive writing assistants. Through a large community collaboration, we
explore five aspects of writing assistants: task, user, technology,
interaction, and ecosystem. Within each aspect, we define dimensions (i.e.,
fundamental components of an aspect) and codes (i.e., potential options for
each dimension) by systematically reviewing 115 papers. Our design space aims
to offer researchers and designers a practical tool to navigate, comprehend,
and compare the various possibilities of writing assistants, and aid in the
envisioning and design of new writing assistants.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a conference paper at CHI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BAN-PL: a Novel Polish <span class="highlight-title">Dataset</span> of Banned Harmful and Offensive Content
  from Wykop.pl web service <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.10592v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.10592v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anna Kołos, Inez Okulska, Kinga Głąbińska, Agnieszka Karlińska, Emilia Wiśnios, Paweł Ellerik, Andrzej Prałat
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since the Internet is flooded with hate, it is one of the main tasks for NLP
experts to master automated online content moderation. However, advancements in
this field require improved access to publicly available accurate and
non-synthetic datasets of social media content. For the Polish language, such
resources are very limited. In this paper, we address this gap by presenting a
new open dataset of offensive social media content for the Polish language. The
dataset comprises content from Wykop.pl, a popular online service often
referred to as the "Polish Reddit", reported by users and banned in the
internal moderation process. It contains a total of 691,662 posts and comments,
evenly divided into two categories: "harmful" and "neutral" ("non-harmful").
The anonymized subset of the BAN-PL dataset consisting on 24,000 pieces (12,000
for each class), along with preprocessing scripts have been made publicly
available. Furthermore the paper offers valuable insights into real-life
content moderation processes and delves into an analysis of linguistic features
and content characteristics of the dataset. Moreover, a comprehensive
anonymization procedure has been meticulously described and applied. The
prevalent biases encountered in similar datasets, including post-moderation and
pre-selection biases, are also discussed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for LREC-COLING 2024 Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hyacinth6B: A large language model for Traditional Chinese 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13334v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13334v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chih-Wei Song, Yin-Te Tsai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This research's primary motivation of this study is to address the high
hardware and computational demands typically associated with LLMs.Therefore,our
goal is to find a balance between model lightness and performance,striving to
maximize performance while using a comparatively lightweight model. Hyacinth6B
was developed with this objective in mind,aiming to fully leverage the core
capabilities of LLMs without incurring substantial resource costs, effectively
pushing the boundaries of smaller model's performance. The training approach
involves parameter efficient finetuning using the LoRA method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large Language Model for Multi-objective Evolutionary Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.12541v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.12541v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fei Liu, Xi Lin, Zhenkun Wang, Shunyu Yao, Xialiang Tong, Mingxuan Yuan, Qingfu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multiobjective evolutionary algorithms (MOEAs) are major methods for solving
multiobjective optimization problems (MOPs). Many MOEAs have been proposed in
the past decades, of which the search operators need a carefully handcrafted
design with domain knowledge. Recently, some attempts have been made to replace
the manually designed operators in MOEAs with learning-based operators (e.g.,
neural network models). However, much effort is still required for designing
and training such models, and the learned operators might not generalize well
on new problems. To tackle the above challenges, this work investigates a novel
approach that leverages the powerful large language model (LLM) to design MOEA
operators. With proper prompt engineering, we successfully let a general LLM
serve as a black-box search operator for decomposition-based MOEA (MOEA/D) in a
zero-shot manner. In addition, by learning from the LLM behavior, we further
design an explicit white-box operator with randomness and propose a new version
of decomposition-based MOEA, termed MOEA/D-LO. Experimental studies on
different test benchmarks show that our proposed method can achieve competitive
performance with widely used MOEAs. It is also promising to see the operator
only learned from a few instances can have robust generalization performance on
unseen problems with quite different patterns and settings. The results reveal
the potential benefits of using pre-trained LLMs in the design of MOEAs.To
foster reproducibility and accessibility, the source code is
https://github.com/FeiLiu36/LLM4MOEA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ COPR: Continual Learning Human Preference through Optimal Policy
  Regularization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.15694v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.15694v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Zhang, Lin Gui, Yuanzhao Zhai, Hui Wang, Yu Lei, Ruifeng Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The technique of Reinforcement Learning from Human Feedback (RLHF) is a
commonly employed method to improve pre-trained Language Models (LM), enhancing
their ability to conform to human preferences. Nevertheless, the current
RLHF-based LMs necessitate full retraining each time novel queries or feedback
are introduced, which becomes a challenging task because human preferences can
vary between different domains or tasks. Retraining LMs poses practical
difficulties in many real-world situations due to the significant time and
computational resources required, along with concerns related to data privacy.
To address this limitation, we propose a new method called Continual Optimal
Policy Regularization (COPR), in which we compute the distribution of optimal
policy bypassing the partition function and then regularize the current policy
based on the historically optimal distribution to mitigate Catastrophic
Forgetting (CF). COPR involves a single learning phase and doesn't necessitate
complex reinforcement learning. Importantly, it shares the capability with RLHF
to learn from unlabeled data by maintaining a scoring module, similar to reward
model, making it flexible for continually learning without human feedback. Our
experimental results show that COPR outperforms strong Continuous Learning (CL)
baselines when it comes to consistently aligning with human preferences on
incremental tasks and domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Advancing Topic Segmentation and Outline Generation in Chinese Texts:
  The Paragraph-level Topic Representation, Corpus, and Benchmark <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.14790v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.14790v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Feng Jiang, Weihao Liu, Xiaomin Chu, Peifeng Li, Qiaoming Zhu, Haizhou Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Topic segmentation and outline generation strive to divide a document into
coherent topic sections and generate corresponding subheadings, unveiling the
discourse topic structure of a document. Compared with sentence-level topic
structure, the paragraph-level topic structure can quickly grasp and understand
the overall context of the document from a higher level, benefitting many
downstream tasks such as summarization, discourse parsing, and information
retrieval. However, the lack of large-scale, high-quality Chinese
paragraph-level topic structure corpora restrained relative research and
applications. To fill this gap, we build the Chinese paragraph-level topic
representation, corpus, and benchmark in this paper. Firstly, we propose a
hierarchical paragraph-level topic structure representation with three layers
to guide the corpus construction. Then, we employ a two-stage man-machine
collaborative annotation method to construct the largest Chinese
Paragraph-level Topic Structure corpus (CPTS), achieving high quality. We also
build several strong baselines, including ChatGPT, to validate the
computability of CPTS on two fundamental tasks (topic segmentation and outline
generation) and preliminarily verified its usefulness for the downstream task
(discourse parsing).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Spanish Resource Grammar version 2023 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.13318v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.13318v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Olga Zamaraeva, Lorena S. Allegue, Carlos Gómez-Rodríguez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present the latest version of the Spanish Resource Grammar (SRG), a
grammar of Spanish implemented in the HPSG formalism. Such grammars encode a
complex set of hypotheses about syntax making them a resource for empirical
testing of linguistic theory. They also encode a strict notion of
grammaticality which makes them a resource for natural language processing
applications in computer-assisted language learning. This version of the SRG
uses the recent version of the Freeling morphological analyzer and is released
along with an automatically created, manually verified treebank of 2,291
sentences. We explain the treebanking process, emphasizing how it is different
from treebanking with manual annotation and how it contributes to
empirically-driven development of syntactic theory. The treebanks' high level
of consistency and detail makes them a resource for training high-quality
semantic parsers and generally systems that benefit from precise and detailed
semantics. Finally, we present the grammar's coverage and overgeneration on 100
sentences from a learner corpus, a new research line related to developing
methodologies for robust empirical evaluation of hypotheses in second language
acquisition.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Motion Generation from Fine-grained Textual Descriptions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13518v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13518v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kunhang Li, Yansong Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The task of text2motion is to generate human motion sequences from given
textual descriptions, where the model explores diverse mappings from natural
language instructions to human body movements. While most existing works are
confined to coarse-grained motion descriptions, e.g., "A man squats.",
fine-grained descriptions specifying movements of relevant body parts are
barely explored. Models trained with coarse-grained texts may not be able to
learn mappings from fine-grained motion-related words to motion primitives,
resulting in the failure to generate motions from unseen descriptions. In this
paper, we build a large-scale language-motion dataset specializing in
fine-grained textual descriptions, FineHumanML3D, by feeding GPT-3.5-turbo with
step-by-step instructions with pseudo-code compulsory checks. Accordingly, we
design a new text2motion model, FineMotionDiffuse, making full use of
fine-grained textual information. Our quantitative evaluation shows that
FineMotionDiffuse trained on FineHumanML3D improves FID by a large margin of
0.38, compared with competitive baselines. According to the qualitative
evaluation and case study, our model outperforms MotionDiffuse in generating
spatially or chronologically composite motions, by learning the implicit
mappings from fine-grained descriptions to the corresponding basic motions. We
release our data at https://github.com/KunhangL/finemotiondiffuse.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tandem <span class="highlight-title">Transformer</span>s for Inference Efficient LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.08644v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.08644v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aishwarya P S, Pranav Ajit Nair, Yashas Samaga, Toby Boyd, Sanjiv Kumar, Prateek Jain, Praneeth Netrapalli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The autoregressive nature of conventional large language models (LLMs)
inherently limits inference speed, as tokens are generated sequentially. While
speculative and parallel decoding techniques attempt to mitigate this, they
face limitations: either relying on less accurate smaller models for generation
or failing to fully leverage the base LLM's representations.
  We introduce a novel architecture, Tandem transformers, to address these
issues. This architecture uniquely combines (1) a small autoregressive model
and (2) a large model operating in block mode (processing multiple tokens
simultaneously). The small model's predictive accuracy is substantially
enhanced by granting it attention to the large model's richer representations.
On the PaLM2 pretraining dataset, a tandem of PaLM2-Bison and PaLM2-Gecko
demonstrates a 3.3% improvement in next-token prediction accuracy over a
standalone PaLM2-Gecko, offering a 1.16x speedup compared to a PaLM2-Otter
model with comparable downstream performance. We further incorporate the tandem
model within the speculative decoding (SPEED) framework where the large model
validates tokens from the small model. This ensures that the Tandem of
PaLM2-Bison and PaLM2-Gecko achieves substantial speedup (around 1.14x faster
than using vanilla PaLM2-Gecko in SPEED) while maintaining identical downstream
task accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Multimodal Approach to Device-Directed Speech Detection with Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14438v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14438v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dominik Wagner, Alexander Churchill, Siddharth Sigtia, Panayiotis Georgiou, Matt Mirsamadi, Aarshee Mishra, Erik Marchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Interactions with virtual assistants typically start with a predefined
trigger phrase followed by the user command. To make interactions with the
assistant more intuitive, we explore whether it is feasible to drop the
requirement that users must begin each command with a trigger phrase. We
explore this task in three ways: First, we train classifiers using only
acoustic information obtained from the audio waveform. Second, we take the
decoder outputs of an automatic speech recognition (ASR) system, such as 1-best
hypotheses, as input features to a large language model (LLM). Finally, we
explore a multimodal system that combines acoustic and lexical features, as
well as ASR decoder signals in an LLM. Using multimodal information yields
relative equal-error-rate improvements over text-only and audio-only models of
up to 39% and 61%. Increasing the size of the LLM and training with low-rank
adaption leads to further relative EER reductions of up to 18% on our dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2312.03632</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deciphering the Impact of <span class="highlight-title">Pretrain</span>ing Data on Large Language Models
  through Machine Unlearning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.11537v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.11537v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Zhao, Li Du, Xiao Ding, Kai Xiong, Zhouhao Sun, Jun Shi, Ting Liu, Bing Qin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Through pretraining on a corpus with various sources, Large Language Models
(LLMs) have gained impressive performance. However, the impact of each
component of the pretraining corpus remains opaque. As a result, the
organization of the pretraining corpus is still empirical and may deviate from
the optimal. To address this issue, we systematically analyze the impact of 48
datasets from 5 major categories of pretraining data of LLMs and measure their
impacts on LLMs using benchmarks about nine major categories of model
capabilities. Our analyses provide empirical results about the contribution of
multiple corpora on the performances of LLMs, along with their joint impact
patterns, including complementary, orthogonal, and correlational relationships.
We also identify a set of ``high-impact data'' such as Books that is
significantly related to a set of model capabilities. These findings provide
insights into the organization of data to support more efficient pretraining of
LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ High-throughput Biomedical Relation Extraction for Semi-Structured Web
  Articles Empowered by Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.08274v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.08274v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Songchi Zhou, Sheng Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Objective: To develop a high-throughput biomedical relation extraction system
that takes advantage of the large language models'(LLMs) reading comprehension
ability and biomedical world knowledge in a scalable and evidential manner.
Methods: We formulate the relation extraction task as binary classifications
for large language models. Specifically, LLMs make the decision based on the
external corpus and its world knowledge, giving the reason for the judgment for
factual verification. This method is tailored for semi-structured web articles,
wherein we designate the main title as the tail entity and explicitly
incorporate it into the context, and the potential head entities are matched
based on a biomedical thesaurus. Moreover, lengthy contents are sliced into
text chunks, embedded, and retrieved with additional embedding models. Results:
Using an open-source LLM, we extracted 248659 relation triplets of three
distinct relation types from three reputable biomedical websites. To assess the
efficacy of the basic pipeline employed for biomedical relation extraction, we
curated a benchmark dataset annotated by a medical expert. Evaluation results
indicate that the pipeline exhibits performance comparable to that of GPT-4.
Case studies further illuminate challenges faced by contemporary LLMs in the
context of biomedical relation extraction for semi-structured web articles.
Conclusion: The proposed method has demonstrated its effectiveness in
leveraging the strengths of LLMs for high-throughput biomedical relation
extraction. Its adaptability is evident, as it can be seamlessly extended to
diverse semi-structured biomedical websites, facilitating the extraction of
various types of biomedical relations with ease.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ STEntConv: Predicting Disagreement with Stance Detection and a Signed
  Graph Convolutional Network <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15885v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15885v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Isabelle Lorge, Li Zhang, Xiaowen Dong, Janet B. Pierrehumbert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rise of social media platforms has led to an increase in polarised online
discussions, especially on political and socio-cultural topics such as
elections and climate change. We propose a simple and novel unsupervised method
to predict whether the authors of two posts agree or disagree, leveraging user
stances about named entities obtained from their posts. We present STEntConv, a
model which builds a graph of users and named entities weighted by stance and
trains a Signed Graph Convolutional Network (SGCN) to detect disagreement
between comment and reply posts. We run experiments and ablation studies and
show that including this information improves disagreement detection
performance on a dataset of Reddit posts for a range of controversial subreddit
topics, without the need for platform-specific features or user history.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for the 2024 Joint International Conference on Computational
  Linguistics, Language Resources and Evaluation (LREC-COLING 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PWESuite: Phonetic Word Embeddings and Tasks They Facilitate <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.02541v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.02541v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vilém Zouhar, Kalvin Chang, Chenxuan Cui, Nathaniel Carlson, Nathaniel Robinson, Mrinmaya Sachan, David Mortensen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mapping words into a fixed-dimensional vector space is the backbone of modern
NLP. While most word embedding methods successfully encode semantic
information, they overlook phonetic information that is crucial for many tasks.
We develop three methods that use articulatory features to build phonetically
informed word embeddings. To address the inconsistent evaluation of existing
phonetic word embedding methods, we also contribute a task suite to fairly
evaluate past, current, and future methods. We evaluate both (1) intrinsic
aspects of phonetic word embeddings, such as word retrieval and correlation
with sound similarity, and (2) extrinsic performance on tasks such as rhyme and
cognate detection and sound analogies. We hope our task suite will promote
reproducibility and inspire future phonetic embedding research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mastering Text, Code and Math Simultaneously via Fusing Highly
  Specialized Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08281v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08281v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ning Ding, Yulin Chen, Ganqu Cui, Xingtai Lv, Weilin Zhao, Ruobing Xie, Bowen Zhou, Zhiyuan Liu, Maosong Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Underlying data distributions of natural language, programming code, and
mathematical symbols vary vastly, presenting a complex challenge for large
language models (LLMs) that strive to achieve high performance across all three
domains simultaneously. Achieving a very high level of proficiency for an LLM
within a specific domain often requires extensive training with relevant
corpora, which is typically accompanied by a sacrifice in performance in other
domains. In this paper, we propose to fuse models that are already
highly-specialized directly. The proposed fusing framework, UltraFuser,
consists of three distinct specialists that are already sufficiently trained on
language, coding, and mathematics. A token-level gating mechanism is introduced
to blend the specialists' outputs. A two-stage training strategy accompanied by
balanced sampling is designed to ensure stability. To effectively train the
fused model, we further construct a high-quality supervised instruction tuning
dataset, UltraChat 2, which includes text, code, and mathematical content. This
dataset comprises approximately 300,000 instructions and covers a wide range of
topics in each domain. Experiments show that our model could simultaneously
achieve mastery of the three crucial domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Document Embeddings via Self-Contrastive Bregman Divergence
  Learning <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.16031v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.16031v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Saggau, Mina Rezaei, Bernd Bischl, Ilias Chalkidis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning quality document embeddings is a fundamental problem in natural
language processing (NLP), information retrieval (IR), recommendation systems,
and search engines. Despite recent advances in the development of
transformer-based models that produce sentence embeddings with self-contrastive
learning, the encoding of long documents (Ks of words) is still challenging
with respect to both efficiency and quality considerations. Therefore, we train
Longfomer-based document encoders using a state-of-the-art unsupervised
contrastive learning method (SimCSE). Further on, we complement the baseline
method -- siamese neural network -- with additional convex neural networks
based on functional Bregman divergence aiming to enhance the quality of the
output document representations. We show that overall the combination of a
self-contrastive siamese network and our proposed neural Bregman network
outperforms the baselines in two linear classification settings on three long
document topic classification tasks from the legal and biomedical domains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, short paper at Findings of ACL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Detecting Sexual Content at the Sentence Level in First Millennium Latin
  Texts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.14974v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.14974v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thibault Clérice
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we propose to evaluate the use of deep learning methods for
semantic classification at the sentence level to accelerate the process of
corpus building in the field of humanities and linguistics, a traditional and
time-consuming task. We introduce a novel corpus comprising around 2500
sentences spanning from 300 BCE to 900 CE including sexual semantics (medical,
erotica, etc.). We evaluate various sentence classification approaches and
different input embedding layers, and show that all consistently outperform
simple token-based searches. We explore the integration of idiolectal and
sociolectal metadata embeddings (centuries, author, type of writing), but find
that it leads to overfitting. Our results demonstrate the effectiveness of this
approach, achieving high precision and true positive rates (TPR) of
respectively 70.60% and 86.33% using HAN. We evaluate the impact of the dataset
size on the model performances (420 instead of 2013), and show that, while our
models perform worse, they still offer a high enough precision and TPR, even
without MLM, respectively 69% and 51%. Given the result, we provide an analysis
of the attention mechanism as a supporting added value for humanists in order
to produce more data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ High-order Joint Constituency and Dependency Parsing <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.11888v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.11888v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanggan Gu, Yang Hou, Zhefeng Wang, Xinyu Duan, Zhenghua Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work revisits the topic of jointly parsing constituency and dependency
trees, i.e., to produce compatible constituency and dependency trees
simultaneously for input sentences, which is attractive considering that the
two types of trees are complementary in representing syntax. The original work
of Zhou and Zhao (2019) performs joint parsing only at the inference phase.
They train two separate parsers under the multi-task learning framework (i.e.,
one shared encoder and two independent decoders). They design an ad-hoc dynamic
programming-based decoding algorithm of $O(n^5)$ time complexity for finding
optimal compatible tree pairs. Compared to their work, we make progress in
three aspects: (1) adopting a much more efficient decoding algorithm of
$O(n^4)$ time complexity, (2) exploring joint modeling at the training phase,
instead of only at the inference phase, (3) proposing high-order scoring
components to promote constituent-dependency interaction. We conduct
experiments and analysis on seven languages, covering both rich-resource and
low-resource scenarios. Results and analysis show that joint modeling leads to
a modest overall performance boost over separate modeling, but substantially
improves the complete matching ratio of whole trees, thanks to the explicit
modeling of tree compatibility.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RU22Fact: Optimizing Evidence for Multilingual Explainable Fact-Checking
  on Russia-Ukraine Conflict 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16662v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16662v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yirong Zeng, Xiao Ding, Yi Zhao, Xiangyu Li, Jie Zhang, Chao Yao, Ting Liu, Bing Qin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fact-checking is the task of verifying the factuality of a given claim by
examining the available evidence. High-quality evidence plays a vital role in
enhancing fact-checking systems and facilitating the generation of explanations
that are understandable to humans. However, the provision of both sufficient
and relevant evidence for explainable fact-checking systems poses a challenge.
To tackle this challenge, we propose a method based on a Large Language Model
to automatically retrieve and summarize evidence from the Web. Furthermore, we
construct RU22Fact, a novel multilingual explainable fact-checking dataset on
the Russia-Ukraine conflict in 2022 of 16K samples, each containing real-world
claims, optimized evidence, and referenced explanation. To establish a baseline
for our dataset, we also develop an end-to-end explainable fact-checking system
to verify claims and generate explanations. Experimental results demonstrate
the prospect of optimized evidence in increasing fact-checking performance and
also indicate the possibility of further progress in the end-to-end claim
verification and explanation generation tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 3 figures, accepted by lrec-coling2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Born With a Silver Spoon? Investigating Socioeconomic Bias in Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14633v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14633v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Smriti Singh, Shuvam Keshari, Vinija Jain, Aman Chadha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Socioeconomic bias in society exacerbates disparities, influencing access to
opportunities and resources based on individuals' economic and social
backgrounds. This pervasive issue perpetuates systemic inequalities, hindering
the pursuit of inclusive progress as a society. In this paper, we investigate
the presence of socioeconomic bias, if any, in large language models. To this
end, we introduce a novel dataset SilverSpoon, consisting of 3000 samples that
illustrate hypothetical scenarios that involve underprivileged people
performing ethically ambiguous actions due to their circumstances, and ask
whether the action is ethically justified. Further, this dataset has a
dual-labeling scheme and has been annotated by people belonging to both ends of
the socioeconomic spectrum. Using SilverSpoon, we evaluate the degree of
socioeconomic bias expressed in large language models and the variation of this
degree as a function of model size. We also perform qualitative analysis to
analyze the nature of this bias. Our analysis reveals that while humans
disagree on which situations require empathy toward the underprivileged, most
large language models are unable to empathize with the socioeconomically
underprivileged regardless of the situation. To foster further research in this
domain, we make SilverSpoon and our evaluation harness publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Understanding Performance of Long-Document Ranking Models through
  Comprehensive Evaluation and Leaderboarding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.01262v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.01262v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leonid Boytsov, David Akinpelu, Tianyi Lin, Fangwei Gao, Yutian Zhao, Jeffrey Huang, Eric Nyberg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We evaluated 20+ Transformer models for ranking of long documents (including
recent LongP models trained with FlashAttention) and compared them with simple
FirstP baselines (applying the same model to input truncated to the first 512
tokens). We used MS MARCO Documents v1 as a primary training set and evaluated
models in the zero-shot scenario as well as after fine-tuning on other
collections.
  In our initial experiments with standard collections we found that
long-document models underperformed FirstP or outperformed it by at most 5% on
average in terms of MRR or NDCG. We then conjectured that this was not due to
models inability to process long context but rather due to a positional bias of
relevant passages, which tended to be among the first 512 document tokens. We
found evidence that this bias was, indeed, present in at least two test sets,
which motivated us to create a new collection MS MARCO FarRelevant where the
relevant passages were not present among the first 512 tokens.
  Unlike standard collections where we observed both little benefit from
incorporating longer contexts and limited variability in model performance
(within a few %), experiments on MS MARCO FarRelevant uncovered dramatic
differences among models. FirstP models performed roughly at the
random-baseline level in both zero-shot and fine-tuning scenarios. Simple
aggregation models (e.g., MaxP) had good zero-shot accuracy but benefited
little from fine-tuning. Most other models had poor zero-shot performance
(sometimes at a random baseline level) but outstripped MaxP by as much 13-28\%
after finetuning. Thus, positional bias not only diminishes benefits of
processing longer document contexts but also leads to model overfitting to this
bias and performing poorly in a zero-shot setting when a distribution of
relevant passages changes substantially.
  We make our software and MS MARCO FarRelevant available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Topic Detection and Tracking with Time-Aware Document Embeddings <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.06166v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.06166v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hang Jiang, Doug Beeferman, Weiquan Mao, Deb Roy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The time at which a message is communicated is a vital piece of metadata in
many real-world natural language processing tasks such as Topic Detection and
Tracking (TDT). TDT systems aim to cluster a corpus of news articles by event,
and in that context, stories that describe the same event are likely to have
been written at around the same time. Prior work on time modeling for TDT takes
this into account, but does not well capture how time interacts with the
semantic nature of the event. For example, stories about a tropical storm are
likely to be written within a short time interval, while stories about a movie
release may appear over weeks or months. In our work, we design a neural method
that fuses temporal and textual information into a single representation of
news documents for event detection. We fine-tune these time-aware document
embeddings with a triplet loss architecture, integrate the model into
downstream TDT systems, and evaluate the systems on two benchmark TDT data sets
in English. In the retrospective setting, we apply clustering algorithms to the
time-aware embeddings and show substantial improvements over baselines on the
News2013 data set. In the online streaming setting, we add our document encoder
to an existing state-of-the-art TDT pipeline and demonstrate that it can
benefit the overall performance. We conduct ablation studies on the time
representation and fusion algorithm strategies, showing that our proposed model
outperforms alternative strategies. Finally, we probe the model to examine how
it handles recurring events more effectively than previous TDT systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">GPT</span>-4 Is Too Smart To Be Safe: Stealthy Chat with LLMs via Cipher <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.06463v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.06463v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Youliang Yuan, Wenxiang Jiao, Wenxuan Wang, Jen-tse Huang, Pinjia He, Shuming Shi, Zhaopeng Tu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Safety lies at the core of the development of Large Language Models (LLMs).
There is ample work on aligning LLMs with human ethics and preferences,
including data filtering in pretraining, supervised fine-tuning, reinforcement
learning from human feedback, and red teaming, etc. In this study, we discover
that chat in cipher can bypass the safety alignment techniques of LLMs, which
are mainly conducted in natural languages. We propose a novel framework
CipherChat to systematically examine the generalizability of safety alignment
to non-natural languages -- ciphers. CipherChat enables humans to chat with
LLMs through cipher prompts topped with system role descriptions and few-shot
enciphered demonstrations. We use CipherChat to assess state-of-the-art LLMs,
including ChatGPT and GPT-4 for different representative human ciphers across
11 safety domains in both English and Chinese. Experimental results show that
certain ciphers succeed almost 100% of the time to bypass the safety alignment
of GPT-4 in several safety domains, demonstrating the necessity of developing
safety alignment for non-natural languages. Notably, we identify that LLMs seem
to have a ''secret cipher'', and propose a novel SelfCipher that uses only role
play and several demonstrations in natural language to evoke this capability.
SelfCipher surprisingly outperforms existing human ciphers in almost all cases.
Our code and data will be released at https://github.com/RobustNLP/CipherChat.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR 2024. 21 pages, 3 figures, 13 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Take Care of Your <span class="highlight-title">Prompt</span> Bias! Investigating and Mitigating <span class="highlight-title">Prompt</span> Bias
  in Factual Knowledge Extraction <span class="chip">COLING 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09963v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09963v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyang Xu, Keqin Peng, Liang Ding, Dacheng Tao, Xiliang Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent research shows that pre-trained language models (PLMs) suffer from
"prompt bias" in factual knowledge extraction, i.e., prompts tend to introduce
biases toward specific labels. Prompt bias presents a significant challenge in
assessing the factual knowledge within PLMs. Therefore, this paper aims to
improve the reliability of existing benchmarks by thoroughly investigating and
mitigating prompt bias. We show that: 1) all prompts in the experiments exhibit
non-negligible bias, with gradient-based prompts like AutoPrompt and OptiPrompt
displaying significantly higher levels of bias; 2) prompt bias can amplify
benchmark accuracy unreasonably by overfitting the test datasets, especially on
imbalanced datasets like LAMA. Based on these findings, we propose a
representation-based approach to mitigate the prompt bias during inference
time. Specifically, we first estimate the biased representation using
prompt-only querying, and then remove it from the model's internal
representations to generate the debiased representations, which are used to
produce the final debiased outputs. Experiments across various prompts, PLMs,
and benchmarks show that our approach can not only correct the overfitted
performance caused by prompt bias, but also significantly improve the prompt
retrieval capability (up to 10% absolute performance gain). These results
indicate that our approach effectively alleviates prompt bias in knowledge
evaluation, thereby enhancing the reliability of benchmark assessments.
Hopefully, our plug-and-play approach can be a golden standard to strengthen
PLMs toward reliable knowledge bases. Code and data are released in
https://github.com/FelliYang/PromptBias.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can Large Language Models Discern Evidence for Scientific Hypotheses?
  Case Studies in the Social Sciences 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.06578v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.06578v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sai Koneru, Jian Wu, Sarah Rajtmajer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hypothesis formulation and testing are central to empirical research. A
strong hypothesis is a best guess based on existing evidence and informed by a
comprehensive view of relevant literature. However, with exponential increase
in the number of scientific articles published annually, manual aggregation and
synthesis of evidence related to a given hypothesis is a challenge. Our work
explores the ability of current large language models (LLMs) to discern
evidence in support or refute of specific hypotheses based on the text of
scientific abstracts. We share a novel dataset for the task of scientific
hypothesis evidencing using community-driven annotations of studies in the
social sciences. We compare the performance of LLMs to several state-of-the-art
benchmarks and highlight opportunities for future research in this area. The
dataset is available at
https://github.com/Sai90000/ScientificHypothesisEvidencing.git
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Troika: Multi-Path Cross-Modal Traction for Compositional Zero-Shot
  Learning <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.15230v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.15230v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siteng Huang, Biao Gong, Yutong Feng, Min Zhang, Yiliang Lv, Donglin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent compositional zero-shot learning (CZSL) methods adapt pre-trained
vision-language models (VLMs) by constructing trainable prompts only for
composed state-object pairs. Relying on learning the joint representation of
seen compositions, these methods ignore the explicit modeling of the state and
object, thus limiting the exploitation of pre-trained knowledge and
generalization to unseen compositions. With a particular focus on the
universality of the solution, in this work, we propose a novel paradigm for
CZSL models that establishes three identification branches (i.e., Multi-Path)
to jointly model the state, object, and composition. The presented Troika is
our implementation that aligns the branch-specific prompt representations with
decomposed visual features. To calibrate the bias between semantically similar
multi-modal representations, we further devise a Cross-Modal Traction module
into Troika that shifts the prompt representation towards the current visual
content. We conduct extensive experiments on three popular benchmarks, where
our method significantly outperforms existing methods in both closed-world and
open-world settings. The code will be available at
https://github.com/bighuang624/Troika.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards a RAG-based Summarization Agent for the Electron-Ion Collider 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15729v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15729v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Karthik Suresh, Neeltje Kackar, Luke Schleck, Cristiano Fanelli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The complexity and sheer volume of information encompassing documents,
papers, data, and other resources from large-scale experiments demand
significant time and effort to navigate, making the task of accessing and
utilizing these varied forms of information daunting, particularly for new
collaborators and early-career scientists. To tackle this issue, a Retrieval
Augmented Generation (RAG)--based Summarization AI for EIC (RAGS4EIC) is under
development. This AI-Agent not only condenses information but also effectively
references relevant responses, offering substantial advantages for
collaborators. Our project involves a two-step approach: first, querying a
comprehensive vector database containing all pertinent experiment information;
second, utilizing a Large Language Model (LLM) to generate concise summaries
enriched with citations based on user queries and retrieved data. We describe
the evaluation methods that use RAG assessments (RAGAs) scoring mechanisms to
assess the effectiveness of responses. Furthermore, we describe the concept of
prompt template-based instruction-tuning which provides flexibility and
accuracy in summarization. Importantly, the implementation relies on LangChain,
which serves as the foundation of our entire workflow. This integration ensures
efficiency and scalability, facilitating smooth deployment and accessibility
for various user groups within the Electron Ion Collider (EIC) community. This
innovative AI-driven framework not only simplifies the understanding of vast
datasets but also encourages collaborative participation, thereby empowering
researchers. As a demonstration, a web application has been developed to
explain each stage of the RAG Agent development in detail.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>updated title to have no latex formatting</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AIOS: LLM Agent Operating System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16971v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16971v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Mei, Zelong Li, Shuyuan Xu, Ruosong Ye, Yingqiang Ge, Yongfeng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integration and deployment of large language model (LLM)-based
intelligent agents have been fraught with challenges that compromise their
efficiency and efficacy. Among these issues are sub-optimal scheduling and
resource allocation of agent requests over the LLM, the difficulties in
maintaining context during interactions between agent and LLM, and the
complexities inherent in integrating heterogeneous agents with different
capabilities and specializations. The rapid increase of agent quantity and
complexity further exacerbates these issues, often leading to bottlenecks and
sub-optimal utilization of resources. Inspired by these challenges, this paper
presents AIOS, an LLM agent operating system, which embeds large language model
into operating systems (OS) as the brain of the OS, enabling an operating
system "with soul" -- an important step towards AGI. Specifically, AIOS is
designed to optimize resource allocation, facilitate context switch across
agents, enable concurrent execution of agents, provide tool service for agents,
and maintain access control for agents. We present the architecture of such an
operating system, outline the core challenges it aims to resolve, and provide
the basic design and implementation of the AIOS. Our experiments on concurrent
execution of multiple agents demonstrate the reliability and efficiency of our
AIOS modules. Through this, we aim to not only improve the performance and
efficiency of LLM agents but also to pioneer for better development and
deployment of the AIOS ecosystem in the future. The project is open-source at
https://github.com/agiresearch/AIOS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 5 figures, 5 tables; comments and suggestions are
  appreciated</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Aligning with Human Judgement: The Role of Pairwise Preference in Large
  Language Model Evaluators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16950v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16950v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinhong Liu, Han Zhou, Zhijiang Guo, Ehsan Shareghi, Ivan Vulić, Anna Korhonen, Nigel Collier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated promising capabilities as
automatic evaluators in assessing the quality of generated natural language.
However, LLMs still exhibit biases in evaluation and often struggle to generate
coherent evaluations that align with human assessments. In this work, we first
conduct a systematic study of the misalignment between LLM evaluators and human
judgement, revealing that existing calibration methods aimed at mitigating
biases are insufficient for effectively aligning LLM evaluators. Inspired by
the use of preference data in RLHF, we formulate the evaluation as a ranking
problem and introduce Pairwise-preference Search (PairS), an uncertainty-guided
search method that employs LLMs to conduct pairwise comparisons and efficiently
ranks candidate texts. PairS achieves state-of-the-art performance on
representative evaluation tasks and demonstrates significant improvements over
direct scoring. Furthermore, we provide insights into the role of pairwise
preference in quantifying the transitivity of LLMs and demonstrate how PairS
benefits from calibration.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large Language Models in Biomedical and Health Informatics: A
  Bibliometric <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16303v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16303v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huizi Yu, Lizhou Fan, Lingyao Li, Jiayan Zhou, Zihui Ma, Lu Xian, Wenyue Hua, Sijia He, Mingyu Jin, Yongfeng Zhang, Ashvin Gandhi, Xin Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have rapidly become important tools in
Biomedical and Health Informatics (BHI), enabling new ways to analyze data,
treat patients, and conduct research. This bibliometric review aims to provide
a panoramic view of how LLMs have been used in BHI by examining research
articles and collaboration networks from 2022 to 2023. It further explores how
LLMs can improve Natural Language Processing (NLP) applications in various BHI
areas like medical diagnosis, patient engagement, electronic health record
management, and personalized medicine. To do this, our bibliometric review
identifies key trends, maps out research networks, and highlights major
developments in this fast-moving field. Lastly, it discusses the ethical
concerns and practical challenges of using LLMs in BHI, such as data privacy
and reliable medical recommendations. Looking ahead, we consider how LLMs could
further transform biomedical research as well as healthcare delivery and
patient outcomes. This bibliometric review serves as a resource for
stakeholders in healthcare, including researchers, clinicians, and
policymakers, to understand the current state and future potential of LLMs in
BHI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>50 pages, 7 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ First Tragedy, then Parse: History Repeats Itself in the New Era of
  Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.05020v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.05020v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Naomi Saphra, Eve Fleisig, Kyunghyun Cho, Adam Lopez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many NLP researchers are experiencing an existential crisis triggered by the
astonishing success of ChatGPT and other systems based on large language models
(LLMs). After such a disruptive change to our understanding of the field, what
is left to do? Taking a historical lens, we look for guidance from the first
era of LLMs, which began in 2005 with large $n$-gram models for machine
translation (MT). We identify durable lessons from the first era, and more
importantly, we identify evergreen problems where NLP researchers can continue
to make meaningful contributions in areas where LLMs are ascendant. We argue
that disparities in scale are transient and researchers can work to reduce
them; that data, rather than hardware, is still a bottleneck for many
applications; that meaningful realistic evaluation is still an open problem;
and that there is still room for speculative approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Zero-Shot Chain-of-Thought Reasoning in Large Language Models
  through Logic <span class="chip">COLING 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.13339v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.13339v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xufeng Zhao, Mengdi Li, Wenhao Lu, Cornelius Weber, Jae Hee Lee, Kun Chu, Stefan Wermter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in large language models have showcased their remarkable
generalizability across various domains. However, their reasoning abilities
still have significant room for improvement, especially when confronted with
scenarios requiring multi-step reasoning. Although large language models
possess extensive knowledge, their reasoning often fails to effectively utilize
this knowledge to establish a coherent thinking paradigm. These models
sometimes show hallucinations as their reasoning procedures are unconstrained
by logical principles. Aiming at improving the zero-shot chain-of-thought
reasoning ability of large language models, we propose LoT (Logical Thoughts),
a self-improvement prompting framework that leverages principles rooted in
symbolic logic, particularly Reductio ad Absurdum, to systematically verify and
rectify the reasoning processes step by step. Experimental evaluations
conducted on language tasks in diverse domains, including arithmetic,
commonsense, symbolic, causal inference, and social problems, demonstrate the
efficacy of enhanced reasoning by logic. The implementation code for LoT can be
accessed at: https://github.com/xf-zhao/LoT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in COLING 2024. Code see https://github.com/xf-zhao/LoT</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Do large language models resemble humans in language use? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.08014v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.08014v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenguang G. Cai, Xufeng Duan, David A. Haslett, Shuqi Wang, Martin J. Pickering
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) such as ChatGPT and Vicuna have shown remarkable
capacities in comprehending and producing language. However, their internal
workings remain a black box, and it is unclear whether LLMs and chatbots can
develop humanlike characteristics in language use. Cognitive scientists have
devised many experiments that probe, and have made great progress in
explaining, how people comprehend and produce language. We subjected ChatGPT
and Vicuna to 12 of these experiments ranging from sounds to dialogue,
preregistered and with 1000 runs (i.e., iterations) per experiment. ChatGPT and
Vicuna replicated the human pattern of language use in 10 and 7 out of the 12
experiments, respectively. The models associated unfamiliar words with
different meanings depending on their forms, continued to access recently
encountered meanings of ambiguous words, reused recent sentence structures,
attributed causality as a function of verb semantics, and accessed different
meanings and retrieved different words depending on an interlocutor's identity.
In addition, ChatGPT, but not Vicuna, nonliterally interpreted implausible
sentences that were likely to have been corrupted by noise, drew reasonable
inferences, and overlooked semantic fallacies in a sentence. Finally, unlike
humans, neither model preferred using shorter words to convey less informative
content, nor did they use context to resolve syntactic ambiguities. We discuss
how these convergences and divergences may result from the transformer
architecture. Overall, these experiments demonstrate that LLMs such as ChatGPT
(and Vicuna to a lesser extent) are humanlike in many aspects of human language
processing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SelfIE: Self-Interpretation of Large Language Model Embeddings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10949v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10949v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haozhe Chen, Carl Vondrick, Chengzhi Mao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  How do large language models (LLMs) obtain their answers? The ability to
explain and control an LLM's reasoning process is key for reliability,
transparency, and future model developments. We propose SelfIE
(Self-Interpretation of Embeddings), a framework that enables LLMs to interpret
their own embeddings in natural language by leveraging their ability to respond
to inquiries about a given passage. Capable of interpreting open-world concepts
in the hidden embeddings, SelfIE reveals LLM internal reasoning in cases such
as making ethical decisions, internalizing prompt injection, and recalling
harmful knowledge. SelfIE's text descriptions on hidden embeddings also open up
new avenues to control LLM reasoning. We propose Supervised Control, which
allows editing open-ended concepts while only requiring gradient computation of
individual layer. We extend RLHF to hidden embeddings and propose Reinforcement
Control that erases harmful knowledge in LLM without supervision targets.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Video Object Segmentation via Modulated Cross-Attention Memory 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17937v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17937v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdelrahman Shaker, Syed Talal Wasim, Martin Danelljan, Salman Khan, Ming-Hsuan Yang, Fahad Shahbaz Khan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, transformer-based approaches have shown promising results for
semi-supervised video object segmentation. However, these approaches typically
struggle on long videos due to increased GPU memory demands, as they frequently
expand the memory bank every few frames. We propose a transformer-based
approach, named MAVOS, that introduces an optimized and dynamic long-term
modulated cross-attention (MCA) memory to model temporal smoothness without
requiring frequent memory expansion. The proposed MCA effectively encodes both
local and global features at various levels of granularity while efficiently
maintaining consistent speed regardless of the video length. Extensive
experiments on multiple benchmarks, LVOS, Long-Time Video, and DAVIS 2017,
demonstrate the effectiveness of our proposed contributions leading to
real-time inference and markedly reduced memory demands without any degradation
in segmentation accuracy on long videos. Compared to the best existing
transformer-based approach, our MAVOS increases the speed by 7.6x, while
significantly reducing the GPU memory by 87% with comparable segmentation
performance on short and long video datasets. Notably on the LVOS dataset, our
MAVOS achieves a J&F score of 63.3% while operating at 37 frames per second
(FPS) on a single V100 GPU. Our code and models will be publicly available at:
https://github.com/Amshaker/MAVOS.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ConvoFusion: Multi-Modal Conversational Diffusion for Co-Speech Gesture
  Synthesis <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17936v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17936v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Hamza Mughal, Rishabh Dabral, Ikhsanul Habibie, Lucia Donatelli, Marc Habermann, Christian Theobalt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gestures play a key role in human communication. Recent methods for co-speech
gesture generation, while managing to generate beat-aligned motions, struggle
generating gestures that are semantically aligned with the utterance. Compared
to beat gestures that align naturally to the audio signal, semantically
coherent gestures require modeling the complex interactions between the
language and human motion, and can be controlled by focusing on certain words.
Therefore, we present ConvoFusion, a diffusion-based approach for multi-modal
gesture synthesis, which can not only generate gestures based on multi-modal
speech inputs, but can also facilitate controllability in gesture synthesis.
Our method proposes two guidance objectives that allow the users to modulate
the impact of different conditioning modalities (e.g. audio vs text) as well as
to choose certain words to be emphasized during gesturing. Our method is
versatile in that it can be trained either for generating monologue gestures or
even the conversational gestures. To further advance the research on
multi-party interactive gestures, the DnD Group Gesture dataset is released,
which contains 6 hours of gesture data showing 5 people interacting with one
another. We compare our method with several recent works and demonstrate
effectiveness of our method on a variety of tasks. We urge the reader to watch
our supplementary video at our website.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024. Project Page:
  https://vcai.mpi-inf.mpg.de/projects/ConvoFusion/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OmniVid: A Generative Framework for Universal Video Understanding <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17935v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17935v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junke Wang, Dongdong Chen, Chong Luo, Bo He, Lu Yuan, Zuxuan Wu, Yu-Gang Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The core of video understanding tasks, such as recognition, captioning, and
tracking, is to automatically detect objects or actions in a video and analyze
their temporal evolution. Despite sharing a common goal, different tasks often
rely on distinct model architectures and annotation formats. In contrast,
natural language processing benefits from a unified output space, i.e., text
sequences, which simplifies the training of powerful foundational language
models, such as GPT-3, with extensive training corpora. Inspired by this, we
seek to unify the output space of video understanding tasks by using languages
as labels and additionally introducing time and box tokens. In this way, a
variety of video tasks could be formulated as video-grounded token generation.
This enables us to address various types of video tasks, including
classification (such as action recognition), captioning (covering clip
captioning, video question answering, and dense video captioning), and
localization tasks (such as visual object tracking) within a fully shared
encoder-decoder architecture, following a generative framework. Through
comprehensive experiments, we demonstrate such a simple and straightforward
idea is quite effective and can achieve state-of-the-art or competitive results
on seven video benchmarks, providing a novel perspective for more universal
video understanding. Code is available at https://github.com/wangjk666/OmniVid.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AiOS: All-in-One-Stage Expressive Human Pose and Shape Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17934v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17934v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingping Sun, Yanjun Wang, Ailing Zeng, Wanqi Yin, Chen Wei, Wenjia Wang, Haiyi Mei, Chi Sing Leung, Ziwei Liu, Lei Yang, Zhongang Cai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Expressive human pose and shape estimation (a.k.a. 3D whole-body mesh
recovery) involves the human body, hand, and expression estimation. Most
existing methods have tackled this task in a two-stage manner, first detecting
the human body part with an off-the-shelf detection model and inferring the
different human body parts individually. Despite the impressive results
achieved, these methods suffer from 1) loss of valuable contextual information
via cropping, 2) introducing distractions, and 3) lacking inter-association
among different persons and body parts, inevitably causing performance
degradation, especially for crowded scenes. To address these issues, we
introduce a novel all-in-one-stage framework, AiOS, for multiple expressive
human pose and shape recovery without an additional human detection step.
Specifically, our method is built upon DETR, which treats multi-person
whole-body mesh recovery task as a progressive set prediction problem with
various sequential detection. We devise the decoder tokens and extend them to
our task. Specifically, we first employ a human token to probe a human location
in the image and encode global features for each instance, which provides a
coarse location for the later transformer block. Then, we introduce a
joint-related token to probe the human joint in the image and encoder a
fine-grained local feature, which collaborates with the global feature to
regress the whole-body mesh. This straightforward but effective model
outperforms previous state-of-the-art methods by a 9% reduction in NMVE on
AGORA, a 30% reduction in PVE on EHF, a 10% reduction in PVE on ARCTIC, and a
3% reduction in PVE on EgoBody.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Homepage: https://ttxskk.github.io/AiOS/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SLEDGE: Synthesizing Simulation Environments for Driving Agents with
  Generative Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17933v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17933v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kashyap Chitta, Daniel Dauner, Andreas Geiger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  SLEDGE is the first generative simulator for vehicle motion planning trained
on real-world driving logs. Its core component is a learned model that is able
to generate agent bounding boxes and lane graphs. The model's outputs serve as
an initial state for traffic simulation. The unique properties of the entities
to be generated for SLEDGE, such as their connectivity and variable count per
scene, render the naive application of most modern generative models to this
task non-trivial. Therefore, together with a systematic study of existing lane
graph representations, we introduce a novel raster-to-vector autoencoder
(RVAE). It encodes agents and the lane graph into distinct channels in a
rasterized latent map. This facilitates both lane-conditioned agent generation
and combined generation of lanes and agents with a Diffusion Transformer. Using
generated entities in SLEDGE enables greater control over the simulation, e.g.
upsampling turns or increasing traffic density. Further, SLEDGE can support
500m long routes, a capability not found in existing data-driven simulators
like nuPlan. It presents new challenges for planning algorithms, evidenced by
failure rates of over 40% for PDM, the winner of the 2023 nuPlan challenge,
when tested on hard routes and dense traffic generated by our model. Compared
to nuPlan, SLEDGE requires 500$\times$ less storage to set up (<4GB), making it
a more accessible option and helping with democratizing future research in this
field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Track Everything Everywhere Fast and Robustly 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17931v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17931v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunzhou Song, Jiahui Lei, Ziyun Wang, Lingjie Liu, Kostas Daniilidis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel test-time optimization approach for efficiently and
robustly tracking any pixel at any time in a video. The latest state-of-the-art
optimization-based tracking technique, OmniMotion, requires a prohibitively
long optimization time, rendering it impractical for downstream applications.
OmniMotion is sensitive to the choice of random seeds, leading to unstable
convergence. To improve efficiency and robustness, we introduce a novel
invertible deformation network, CaDeX++, which factorizes the function
representation into a local spatial-temporal feature grid and enhances the
expressivity of the coupling blocks with non-linear functions. While CaDeX++
incorporates a stronger geometric bias within its architectural design, it also
takes advantage of the inductive bias provided by the vision foundation models.
Our system utilizes monocular depth estimation to represent scene geometry and
enhances the objective by incorporating DINOv2 long-term semantics to regulate
the optimization process. Our experiments demonstrate a substantial improvement
in training speed (more than \textbf{10 times} faster), robustness, and
accuracy in tracking over the SoTA optimization-based method OmniMotion.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>project page: https://timsong412.github.io/FastOmniTrack/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Explaining Hypercomplex Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17929v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17929v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eleonora Lopez, Eleonora Grassucci, Debora Capriotti, Danilo Comminiello
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hypercomplex neural networks are gaining increasing interest in the deep
learning community. The attention directed towards hypercomplex models
originates from several aspects, spanning from purely theoretical and
mathematical characteristics to the practical advantage of lightweight models
over conventional networks, and their unique properties to capture both global
and local relations. In particular, a branch of these architectures,
parameterized hypercomplex neural networks (PHNNs), has also gained popularity
due to their versatility across a multitude of application domains.
Nonetheless, only few attempts have been made to explain or interpret their
intricacies. In this paper, we propose inherently interpretable PHNNs and
quaternion-like networks, thus without the need for any post-hoc method. To
achieve this, we define a type of cosine-similarity transform within the
parameterized hypercomplex domain. This PHB-cos transform induces weight
alignment with relevant input features and allows to reduce the model into a
single linear transform, rendering it directly interpretable. In this work, we
start to draw insights into how this unique branch of neural models operates.
We observe that hypercomplex networks exhibit a tendency to concentrate on the
shape around the main object of interest, in addition to the shape of the
object itself. We provide a thorough analysis, studying single neurons of
different layers and comparing them against how real-valued networks learn. The
code of the paper is available at https://github.com/ispamm/HxAI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The paper has been accepted at IEEE WCCI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FastCAR: Fast Classification And Regression Multi-Task Learning via Task
  Consolidation for Modelling a Continuous Property Variable of Object Classes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17926v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17926v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anoop Kini, Andreas Jansche, Timo Bernthaler, Gerhard Schneider
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  FastCAR is a novel task consolidation approach in Multi-Task Learning (MTL)
for a classification and a regression task, despite task heterogeneity with
only subtle correlation. It addresses object classification and continuous
property variable regression, a crucial use case in science and engineering.
FastCAR involves a labeling transformation approach that can be used with a
single-task regression network architecture. FastCAR outperforms traditional
MTL model families, parametrized in the landscape of architecture and loss
weighting schemes, when learning of both tasks are collectively considered
(classification accuracy of 99.54%, regression mean absolute percentage error
of 2.3%). The experiments performed used an Advanced Steel Property dataset
contributed by us. The dataset comprises 4536 images of 224x224 pixels,
annotated with object classes and hardness properties that take continuous
values. With the labeling transformation and single-task regression network
architecture, FastCAR achieves reduced latency and time efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AID: Attention Interpolation of Text-to-Image Diffusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17924v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17924v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiyuan He, Jinghao Wang, Ziwei Liu, Angela Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conditional diffusion models can create unseen images in various settings,
aiding image interpolation. Interpolation in latent spaces is well-studied, but
interpolation with specific conditions like text or poses is less understood.
Simple approaches, such as linear interpolation in the space of conditions,
often result in images that lack consistency, smoothness, and fidelity. To that
end, we introduce a novel training-free technique named Attention Interpolation
via Diffusion (AID). Our key contributions include 1) proposing an inner/outer
interpolated attention layer; 2) fusing the interpolated attention with
self-attention to boost fidelity; and 3) applying beta distribution to
selection to increase smoothness. We also present a variant, Prompt-guided
Attention Interpolation via Diffusion (PAID), that considers interpolation as a
condition-dependent generative process. This method enables the creation of new
images with greater consistency, smoothness, and efficiency, and offers control
over the exact path of interpolation. Our approach demonstrates effectiveness
for conceptual and spatial interpolation. Code and demo are available at
https://github.com/QY-H00/attention-interpolation-diffusion.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TC4D: Trajectory-Conditioned Text-to-4D Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17920v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17920v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sherwin Bahmani, Xian Liu, Yifan Wang, Ivan Skorokhodov, Victor Rong, Ziwei Liu, Xihui Liu, Jeong Joon Park, Sergey Tulyakov, Gordon Wetzstein, Andrea Tagliasacchi, David B. Lindell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent techniques for text-to-4D generation synthesize dynamic 3D scenes
using supervision from pre-trained text-to-video models. However, existing
representations for motion, such as deformation models or time-dependent neural
representations, are limited in the amount of motion they can generate-they
cannot synthesize motion extending far beyond the bounding box used for volume
rendering. The lack of a more flexible motion model contributes to the gap in
realism between 4D generation methods and recent, near-photorealistic video
generation models. Here, we propose TC4D: trajectory-conditioned text-to-4D
generation, which factors motion into global and local components. We represent
the global motion of a scene's bounding box using rigid transformation along a
trajectory parameterized by a spline. We learn local deformations that conform
to the global trajectory using supervision from a text-to-video model. Our
approach enables the synthesis of scenes animated along arbitrary trajectories,
compositional scene generation, and significant improvements to the realism and
amount of generated motion, which we evaluate qualitatively and through a user
study. Video results can be viewed on our website:
https://sherwinbahmani.github.io/tc4d.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://sherwinbahmani.github.io/tc4d</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CMP: Cooperative Motion Prediction with Multi-Agent Communication 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17916v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17916v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuoyuan Wu, Yuping Wang, Hengbo Ma, Zhaowei Li, Hang Qiu, Jiachen Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The confluence of the advancement of Autonomous Vehicles (AVs) and the
maturity of Vehicle-to-Everything (V2X) communication has enabled the
capability of cooperative connected and automated vehicles (CAVs). Building on
top of cooperative perception, this paper explores the feasibility and
effectiveness of cooperative motion prediction. Our method, CMP, takes LiDAR
signals as input to enhance tracking and prediction capabilities. Unlike
previous work that focuses separately on either cooperative perception or
motion prediction, our framework, to the best of our knowledge, is the first to
address the unified problem where CAVs share information in both perception and
prediction modules. Incorporated into our design is the unique capability to
tolerate realistic V2X bandwidth limitations and transmission delays, while
dealing with bulky perception representations. We also propose a prediction
aggregation module, which unifies the predictions obtained by different CAVs
and generates the final prediction. Through extensive experiments and ablation
studies, we demonstrate the effectiveness of our method in cooperative
perception, tracking, and motion prediction tasks. In particular, CMP reduces
the average prediction error by 17.2\% with fewer missing detections compared
with the no cooperation setting. Our work marks a significant step forward in
the cooperative capabilities of CAVs, showcasing enhanced performance in
complex scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Near-Field Lighting for Monocular Depth Estimation from
  Endoscopy Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17915v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17915v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akshay Paruchuri, Samuel Ehrenstein, Shuxian Wang, Inbar Fried, Stephen M. Pizer, Marc Niethammer, Roni Sengupta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Monocular depth estimation in endoscopy videos can enable assistive and
robotic surgery to obtain better coverage of the organ and detection of various
health issues. Despite promising progress on mainstream, natural image depth
estimation, techniques perform poorly on endoscopy images due to a lack of
strong geometric features and challenging illumination effects. In this paper,
we utilize the photometric cues, i.e., the light emitted from an endoscope and
reflected by the surface, to improve monocular depth estimation. We first
create two novel loss functions with supervised and self-supervised variants
that utilize a per-pixel shading representation. We then propose a novel depth
refinement network (PPSNet) that leverages the same per-pixel shading
representation. Finally, we introduce teacher-student transfer learning to
produce better depth maps from both synthetic data with supervision and
clinical data with self-supervision. We achieve state-of-the-art results on the
C3VD dataset while estimating high-quality depth maps from clinical data. Our
code, pre-trained models, and supplementary materials can be found on our
project page: https://ppsnet.github.io/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 7 tables, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ELGC-Net: Efficient Local-Global Context Aggregation for Remote Sensing
  Change Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17909v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17909v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mubashir Noman, Mustansar Fiaz, Hisham Cholakkal, Salman Khan, Fahad Shahbaz Khan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning has shown remarkable success in remote sensing change detection
(CD), aiming to identify semantic change regions between co-registered
satellite image pairs acquired at distinct time stamps. However, existing
convolutional neural network and transformer-based frameworks often struggle to
accurately segment semantic change regions. Moreover, transformers-based
methods with standard self-attention suffer from quadratic computational
complexity with respect to the image resolution, making them less practical for
CD tasks with limited training data. To address these issues, we propose an
efficient change detection framework, ELGC-Net, which leverages rich contextual
information to precisely estimate change regions while reducing the model size.
Our ELGC-Net comprises a Siamese encoder, fusion modules, and a decoder. The
focus of our design is the introduction of an Efficient Local-Global Context
Aggregator module within the encoder, capturing enhanced global context and
local spatial information through a novel pooled-transpose (PT) attention and
depthwise convolution, respectively. The PT attention employs pooling
operations for robust feature extraction and minimizes computational cost with
transposed attention. Extensive experiments on three challenging CD datasets
demonstrate that ELGC-Net outperforms existing methods. Compared to the recent
transformer-based CD approach (ChangeFormer), ELGC-Net achieves a 1.4% gain in
intersection over union metric on the LEVIR-CD dataset, while significantly
reducing trainable parameters. Our proposed ELGC-Net sets a new
state-of-the-art performance in remote sensing change detection benchmarks.
Finally, we also introduce ELGC-Net-LW, a lighter variant with significantly
reduced computational complexity, suitable for resource-constrained settings,
while achieving comparable performance. Project url
https://github.com/techmn/elgcnet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted at IEEE TGRS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scalable Non-Cartesian Magnetic Resonance Imaging with R2D2 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17905v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17905v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Yiwei, Tang Chao, Aghabiglou Amir, Chu Chung San, Wiaux Yves
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a new approach for non-Cartesian magnetic resonance image
reconstruction. While unrolled architectures provide robustness via
data-consistency layers, embedding measurement operators in Deep Neural Network
(DNN) can become impractical at large scale. Alternative Plug-and-Play (PnP)
approaches, where the denoising DNNs are blind to the measurement setting, are
not affected by this limitation and have also proven effective, but their
highly iterative nature also affects scalability. To address this scalability
challenge, we leverage the "Residual-to-Residual DNN series for high-Dynamic
range imaging (R2D2)" approach recently introduced in astronomical imaging.
R2D2's reconstruction is formed as a series of residual images, iteratively
estimated as outputs of DNNs taking the previous iteration's image estimate and
associated data residual as inputs. The method can be interpreted as a learned
version of the Matching Pursuit algorithm. We demonstrate R2D2 in simulation,
considering radial k-space sampling acquisition sequences. Our preliminary
results suggest that R2D2 achieves: (i) suboptimal performance compared to its
unrolled incarnation R2D2-Net, which is however non-scalable due to the
necessary embedding of NUFFT-based data-consistency layers; (ii) superior
reconstruction quality to a scalable version of R2D2-Net embedding an FFT-based
approximation for data consistency; (iii) superior reconstruction quality to
PnP, while only requiring few iterations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to IEEE EUSIPCO 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Serpent: Scalable and Efficient Image Restoration via Multi-scale
  Structured State Space Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17902v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17902v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Shahab Sepehri, Zalan Fabian, Mahdi Soltanolkotabi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The landscape of computational building blocks of efficient image restoration
architectures is dominated by a combination of convolutional processing and
various attention mechanisms. However, convolutional filters are inherently
local and therefore struggle at modeling long-range dependencies in images. On
the other hand, attention excels at capturing global interactions between
arbitrary image regions, however at a quadratic cost in image dimension. In
this work, we propose Serpent, an architecture that leverages recent advances
in state space models (SSMs) in its core computational block. SSMs, originally
introduced for sequence modeling, can maintain a global receptive field with a
favorable linear scaling in input size. Our preliminary results demonstrate
that Serpent can achieve reconstruction quality on par with state-of-the-art
techniques, while requiring orders of magnitude less compute (up to $150$ fold
reduction in FLOPS) and a factor of up to $5\times$ less GPU memory while
maintaining a compact model size.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 5 figures, preliminary workshop submission of a
  comprehensive work to be released soon</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Octree-GS: Towards Consistent Real-time Rendering with LOD-Structured 3D
  Gaussians 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17898v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17898v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kerui Ren, Lihan Jiang, Tao Lu, Mulin Yu, Linning Xu, Zhangkai Ni, Bo Dai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent 3D Gaussian splatting (3D-GS) has shown remarkable rendering
fidelity and efficiency compared to NeRF-based neural scene representations.
While demonstrating the potential for real-time rendering, 3D-GS encounters
rendering bottlenecks in large scenes with complex details due to an excessive
number of Gaussian primitives located within the viewing frustum. This
limitation is particularly noticeable in zoom-out views and can lead to
inconsistent rendering speeds in scenes with varying details. Moreover, it
often struggles to capture the corresponding level of details at different
scales with its heuristic density control operation. Inspired by the
Level-of-Detail (LOD) techniques, we introduce Octree-GS, featuring an
LOD-structured 3D Gaussian approach supporting level-of-detail decomposition
for scene representation that contributes to the final rendering results. Our
model dynamically selects the appropriate level from the set of
multi-resolution anchor points, ensuring consistent rendering performance with
adaptive LOD adjustments while maintaining high-fidelity rendering results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://city-super.github.io/octree-gs/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Survey</span> on 3D Egocentric Human Pose Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17893v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17893v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Mushfiqur Azam, Kevin Desai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Egocentric human pose estimation aims to estimate human body poses and
develop body representations from a first-person camera perspective. It has
gained vast popularity in recent years because of its wide range of
applications in sectors like XR-technologies, human-computer interaction, and
fitness tracking. However, to the best of our knowledge, there is no systematic
literature review based on the proposed solutions regarding egocentric 3D human
pose estimation. To that end, the aim of this survey paper is to provide an
extensive overview of the current state of egocentric pose estimation research.
In this paper, we categorize and discuss the popular datasets and the different
pose estimation models, highlighting the strengths and weaknesses of different
methods by comparative analysis. This survey can be a valuable resource for
both researchers and practitioners in the field, offering insights into key
concepts and cutting-edge solutions in egocentric pose estimation, its
wide-ranging applications, as well as the open problems with future scope.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 2D Gaussian Splatting for Geometrically Accurate Radiance Fields 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17888v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17888v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Binbin Huang, Zehao Yu, Anpei Chen, Andreas Geiger, Shenghua Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D Gaussian Splatting (3DGS) has recently revolutionized radiance field
reconstruction, achieving high quality novel view synthesis and fast rendering
speed without baking. However, 3DGS fails to accurately represent surfaces due
to the multi-view inconsistent nature of 3D Gaussians. We present 2D Gaussian
Splatting (2DGS), a novel approach to model and reconstruct geometrically
accurate radiance fields from multi-view images. Our key idea is to collapse
the 3D volume into a set of 2D oriented planar Gaussian disks. Unlike 3D
Gaussians, 2D Gaussians provide view-consistent geometry while modeling
surfaces intrinsically. To accurately recover thin surfaces and achieve stable
optimization, we introduce a perspective-accurate 2D splatting process
utilizing ray-splat intersection and rasterization. Additionally, we
incorporate depth distortion and normal consistency terms to further enhance
the quality of the reconstructions. We demonstrate that our differentiable
renderer allows for noise-free and detailed geometry reconstruction while
maintaining competitive appearance quality, fast training speed, and real-time
rendering. Our code will be made publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sen2Fire: A Challenging Benchmark <span class="highlight-title">Dataset</span> for Wildfire Detection using
  Sentinel Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17884v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17884v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yonghao Xu, Amanda Berg, Leif Haglund
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Utilizing satellite imagery for wildfire detection presents substantial
potential for practical applications. To advance the development of machine
learning algorithms in this domain, our study introduces the \textit{Sen2Fire}
dataset--a challenging satellite remote sensing dataset tailored for wildfire
detection. This dataset is curated from Sentinel-2 multi-spectral data and
Sentinel-5P aerosol product, comprising a total of 2466 image patches. Each
patch has a size of 512$\times$512 pixels with 13 bands. Given the distinctive
sensitivities of various wavebands to wildfire responses, our research focuses
on optimizing wildfire detection by evaluating different wavebands and
employing a combination of spectral indices, such as normalized burn ratio
(NBR) and normalized difference vegetation index (NDVI). The results suggest
that, in contrast to using all bands for wildfire detection, selecting specific
band combinations yields superior performance. Additionally, our study
underscores the positive impact of integrating Sentinel-5 aerosol data for
wildfire detection. The code and dataset are available online
(https://zenodo.org/records/10881058).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Superior and Pragmatic Talking Face Generation with Teacher-Student
  Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17883v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17883v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chao Liang, Jianwen Jiang, Tianyun Zhong, Gaojie Lin, Zhengkun Rong, Jiaqi Yang, Yongming Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Talking face generation technology creates talking videos from arbitrary
appearance and motion signal, with the "arbitrary" offering ease of use but
also introducing challenges in practical applications. Existing methods work
well with standard inputs but suffer serious performance degradation with
intricate real-world ones. Moreover, efficiency is also an important concern in
deployment. To comprehensively address these issues, we introduce SuperFace, a
teacher-student framework that balances quality, robustness, cost and
editability. We first propose a simple but effective teacher model capable of
handling inputs of varying qualities to generate high-quality results. Building
on this, we devise an efficient distillation strategy to acquire an
identity-specific student model that maintains quality with significantly
reduced computational load. Our experiments validate that SuperFace offers a
more comprehensive solution than existing methods for the four mentioned
objectives, especially in reducing FLOPs by 99\% with the student model.
SuperFace can be driven by both video and audio and allows for localized facial
attributes editing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deepfake Generation and Detection: A Benchmark and <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17881v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17881v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gan Pei, Jiangning Zhang, Menghan Hu, Guangtao Zhai, Chengjie Wang, Zhenyu Zhang, Jian Yang, Chunhua Shen, Dacheng Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In addition to the advancements in deepfake generation, corresponding
detection technologies need to continuously evolve to regulate the potential
misuse of deepfakes, such as for privacy invasion and phishing attacks. This
survey comprehensively reviews the latest developments in deepfake generation
and detection, summarizing and analyzing the current state of the art in this
rapidly evolving field. We first unify task definitions, comprehensively
introduce datasets and metrics, and discuss the development of generation and
detection technology frameworks. Then, we discuss the development of several
related sub-fields and focus on researching four mainstream deepfake fields:
popular face swap, face reenactment, talking face generation, and facial
attribute editing, as well as foreign detection. Subsequently, we
comprehensively benchmark representative methods on popular datasets for each
field, fully evaluating the latest and influential works published in top
conferences/journals. Finally, we analyze the challenges and future research
directions of the discussed fields. We closely follow the latest developments
in https://github.com/flyingby/Awesome-Deepfake-Generation-and-Detection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Low-Latency Neural Stereo Streaming <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17879v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17879v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiqi Hou, Farzad Farhadzadeh, Amir Said, Guillaume Sautiere, Hoang Le
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rise of new video modalities like virtual reality or autonomous driving
has increased the demand for efficient multi-view video compression methods,
both in terms of rate-distortion (R-D) performance and in terms of delay and
runtime. While most recent stereo video compression approaches have shown
promising performance, they compress left and right views sequentially, leading
to poor parallelization and runtime performance. This work presents Low-Latency
neural codec for Stereo video Streaming (LLSS), a novel parallel stereo video
coding method designed for fast and efficient low-latency stereo video
streaming. Instead of using a sequential cross-view motion compensation like
existing methods, LLSS introduces a bidirectional feature shifting module to
directly exploit mutual information among views and encode them effectively
with a joint cross-view prior model for entropy coding. Thanks to this design,
LLSS processes left and right views in parallel, minimizing latency; all while
substantially improving R-D performance compared to both existing neural and
conventional codecs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Boosting Diffusion Models with Moving Average Sampling in Frequency
  Domain <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17870v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17870v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yurui Qian, Qi Cai, Yingwei Pan, Yehao Li, Ting Yao, Qibin Sun, Tao Mei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have recently brought a powerful revolution in image
generation. Despite showing impressive generative capabilities, most of these
models rely on the current sample to denoise the next one, possibly resulting
in denoising instability. In this paper, we reinterpret the iterative denoising
process as model optimization and leverage a moving average mechanism to
ensemble all the prior samples. Instead of simply applying moving average to
the denoised samples at different timesteps, we first map the denoised samples
to data space and then perform moving average to avoid distribution shift
across timesteps. In view that diffusion models evolve the recovery from
low-frequency components to high-frequency details, we further decompose the
samples into different frequency components and execute moving average
separately on each component. We name the complete approach "Moving Average
Sampling in Frequency domain (MASF)". MASF could be seamlessly integrated into
mainstream pre-trained diffusion models and sampling schedules. Extensive
experiments on both unconditional and conditional diffusion models demonstrate
that our MASF leads to superior performances compared to the baselines, with
almost negligible additional complexity cost.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ To Supervise or Not to Supervise: Understanding and Addressing the Key
  Challenges of 3D Transfer Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17869v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17869v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Souhail Hadgi, Lei Li, Maks Ovsjanikov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transfer learning has long been a key factor in the advancement of many
fields including 2D image analysis. Unfortunately, its applicability in 3D data
processing has been relatively limited. While several approaches for 3D
transfer learning have been proposed in recent literature, with contrastive
learning gaining particular prominence, most existing methods in this domain
have only been studied and evaluated in limited scenarios. Most importantly,
there is currently a lack of principled understanding of both when and why 3D
transfer learning methods are applicable. Remarkably, even the applicability of
standard supervised pre-training is poorly understood. In this work, we conduct
the first in-depth quantitative and qualitative investigation of supervised and
contrastive pre-training strategies and their utility in downstream 3D tasks.
We demonstrate that layer-wise analysis of learned features provides
significant insight into the downstream utility of trained networks. Informed
by this analysis, we propose a simple geometric regularization strategy, which
improves the transferability of supervised pre-training. Our work thus sheds
light onto both the specific challenges of 3D transfer learning, as well as
strategies to overcome them.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hierarchical Open-Vocabulary 3D Scene Graphs for Language-Grounded Robot
  Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17846v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17846v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdelrhman Werby, Chenguang Huang, Martin Büchner, Abhinav Valada, Wolfram Burgard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent open-vocabulary robot mapping methods enrich dense geometric maps with
pre-trained visual-language features. While these maps allow for the prediction
of point-wise saliency maps when queried for a certain language concept,
large-scale environments and abstract queries beyond the object level still
pose a considerable hurdle, ultimately limiting language-grounded robotic
navigation. In this work, we present HOV-SG, a hierarchical open-vocabulary 3D
scene graph mapping approach for language-grounded robot navigation. Leveraging
open-vocabulary vision foundation models, we first obtain state-of-the-art
open-vocabulary segment-level maps in 3D and subsequently construct a 3D scene
graph hierarchy consisting of floor, room, and object concepts, each enriched
with open-vocabulary features. Our approach is able to represent multi-story
buildings and allows robotic traversal of those using a cross-floor Voronoi
graph. HOV-SG is evaluated on three distinct datasets and surpasses previous
baselines in open-vocabulary semantic accuracy on the object, room, and floor
level while producing a 75% reduction in representation size compared to dense
open-vocabulary maps. In order to prove the efficacy and generalization
capabilities of HOV-SG, we showcase successful long-horizon
language-conditioned robot navigation within real-world multi-storage
environments. We provide code and trial video data at http://hovsg.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code and video are available at http://hovsg.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ReMamber: Referring Image Segmentation with Mamba Twister 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17839v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17839v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhuan Yang, Chaofan Ma, Jiangchao Yao, Zhun Zhong, Ya Zhang, Yanfeng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Referring Image Segmentation (RIS) leveraging transformers has achieved great
success on the interpretation of complex visual-language tasks. However, the
quadratic computation cost makes it resource-consuming in capturing long-range
visual-language dependencies. Fortunately, Mamba addresses this with efficient
linear complexity in processing. However, directly applying Mamba to
multi-modal interactions presents challenges, primarily due to inadequate
channel interactions for the effective fusion of multi-modal data. In this
paper, we propose ReMamber, a novel RIS architecture that integrates the power
of Mamba with a multi-modal Mamba Twister block. The Mamba Twister explicitly
models image-text interaction, and fuses textual and visual features through
its unique channel and spatial twisting mechanism. We achieve the
state-of-the-art on three challenging benchmarks. Moreover, we conduct thorough
analyses of ReMamber and discuss other fusion designs using Mamba. These
provide valuable perspectives for future research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GTA-HDR: A Large-Scale Synthetic <span class="highlight-title">Dataset</span> for HDR Image Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17837v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17837v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hrishav Bakul Barua, Kalin Stefanov, KokSheik Wong, Abhinav Dhall, Ganesh Krishnasamy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High Dynamic Range (HDR) content (i.e., images and videos) has a broad range
of applications. However, capturing HDR content from real-world scenes is
expensive and time- consuming. Therefore, the challenging task of
reconstructing visually accurate HDR images from their Low Dynamic Range (LDR)
counterparts is gaining attention in the vision research community. A major
challenge in this research problem is the lack of datasets, which capture
diverse scene conditions (e.g., lighting, shadows, weather, locations,
landscapes, objects, humans, buildings) and various image features (e.g.,
color, contrast, saturation, hue, luminance, brightness, radiance). To address
this gap, in this paper, we introduce GTA-HDR, a large-scale synthetic dataset
of photo-realistic HDR images sampled from the GTA-V video game. We perform
thorough evaluation of the proposed dataset, which demonstrates significant
qualitative and quantitative improvements of the state-of-the-art HDR image
reconstruction methods. Furthermore, we demonstrate the effectiveness of the
proposed dataset and its impact on additional computer vision tasks including
3D human pose estimation, human body part segmentation, and holistic scene
segmentation. The dataset, data collection pipeline, and evaluation code are
available at: https://github.com/HrishavBakulBarua/GTA-HDR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A foundation model utilizing chest CT volumes and radiology reports for
  supervised-level zero-shot detection of abnormalities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17834v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17834v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ibrahim Ethem Hamamci, Sezgin Er, Furkan Almas, Ayse Gulnihan Simsek, Sevval Nil Esirgun, Irem Dogan, Muhammed Furkan Dasdelen, Bastian Wittmann, Enis Simsar, Mehmet Simsar, Emine Bensu Erdemir, Abdullah Alanbay, Anjany Sekuboyina, Berkan Lafci, Mehmet K. Ozdemir, Bjoern Menze
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A major challenge in computational research in 3D medical imaging is the lack
of comprehensive datasets. Addressing this issue, our study introduces CT-RATE,
the first 3D medical imaging dataset that pairs images with textual reports.
CT-RATE consists of 25,692 non-contrast chest CT volumes, expanded to 50,188
through various reconstructions, from 21,304 unique patients, along with
corresponding radiology text reports. Leveraging CT-RATE, we developed CT-CLIP,
a CT-focused contrastive language-image pre-training framework. As a versatile,
self-supervised model, CT-CLIP is designed for broad application and does not
require task-specific training. Remarkably, CT-CLIP outperforms
state-of-the-art, fully supervised methods in multi-abnormality detection
across all key metrics, thus eliminating the need for manual annotation. We
also demonstrate its utility in case retrieval, whether using imagery or
textual queries, thereby advancing knowledge dissemination. The open-source
release of CT-RATE and CT-CLIP marks a significant advancement in medical AI,
enhancing 3D imaging analysis and fostering innovation in healthcare.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Assessment of Multimodal Large Language Models in Alignment with Human
  Values 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17830v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17830v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhelun Shi, Zhipin Wang, Hongxing Fan, Zaibin Zhang, Lijun Li, Yongting Zhang, Zhenfei Yin, Lu Sheng, Yu Qiao, Jing Shao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) aim to serve as versatile assistants aligned
with human values, as defined by the principles of being helpful, honest, and
harmless (hhh). However, in terms of Multimodal Large Language Models (MLLMs),
despite their commendable performance in perception and reasoning tasks, their
alignment with human values remains largely unexplored, given the complexity of
defining hhh dimensions in the visual world and the difficulty in collecting
relevant data that accurately mirrors real-world situations. To address this
gap, we introduce Ch3Ef, a Compreh3ensive Evaluation dataset and strategy for
assessing alignment with human expectations. Ch3Ef dataset contains 1002
human-annotated data samples, covering 12 domains and 46 tasks based on the hhh
principle. We also present a unified evaluation strategy supporting assessment
across various scenarios and different perspectives. Based on the evaluation
results, we summarize over 10 key findings that deepen the understanding of
MLLM capabilities, limitations, and the dynamic relationships between
evaluation levels, guiding future advancements in the field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2311.02692</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DiffH2O: Diffusion-Based Synthesis of Hand-Object Interactions from
  Textual Descriptions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17827v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17827v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sammy Christen, Shreyas Hampali, Fadime Sener, Edoardo Remelli, Tomas Hodan, Eric Sauser, Shugao Ma, Bugra Tekin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating natural hand-object interactions in 3D is challenging as the
resulting hand and object motions are expected to be physically plausible and
semantically meaningful. Furthermore, generalization to unseen objects is
hindered by the limited scale of available hand-object interaction datasets. We
propose DiffH2O, a novel method to synthesize realistic, one or two-handed
object interactions from provided text prompts and geometry of the object. The
method introduces three techniques that enable effective learning from limited
data. First, we decompose the task into a grasping stage and a text-based
interaction stage and use separate diffusion models for each. In the grasping
stage, the model only generates hand motions, whereas in the interaction phase
both hand and object poses are synthesized. Second, we propose a compact
representation that tightly couples hand and object poses. Third, we propose
two different guidance schemes to allow more control of the generated motions:
grasp guidance and detailed textual guidance. Grasp guidance takes a single
target grasping pose and guides the diffusion model to reach this grasp at the
end of the grasping stage, which provides control over the grasping pose. Given
a grasping motion from this stage, multiple different actions can be prompted
in the interaction phase. For textual guidance, we contribute comprehensive
text descriptions to the GRAB dataset and show that they enable our method to
have more fine-grained control over hand-object interactions. Our quantitative
and qualitative evaluation demonstrates that the proposed method outperforms
baseline methods and leads to natural hand-object motions. Moreover, we
demonstrate the practicality of our framework by utilizing a hand pose estimate
from an off-the-shelf pose estimator for guidance, and then sampling multiple
different actions in the interaction stage.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://diffh2o.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Image <span class="highlight-title">Pre-Train</span>ing with Siamese Cropped Masked Autoencoders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17823v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17823v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexandre Eymaël, Renaud Vandeghen, Anthony Cioppa, Silvio Giancola, Bernard Ghanem, Marc Van Droogenbroeck
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised pre-training of image encoders is omnipresent in the
literature, particularly following the introduction of Masked autoencoders
(MAE). Current efforts attempt to learn object-centric representations from
motion in videos. In particular, SiamMAE recently introduced a Siamese network,
training a shared-weight encoder from two frames of a video with a high
asymmetric masking ratio (95%). In this work, we propose CropMAE, an
alternative approach to the Siamese pre-training introduced by SiamMAE. Our
method specifically differs by exclusively considering pairs of cropped images
sourced from the same image but cropped differently, deviating from the
conventional pairs of frames extracted from a video. CropMAE therefore
alleviates the need for video datasets, while maintaining competitive
performances and drastically reducing pre-training time. Furthermore, we
demonstrate that CropMAE learns similar object-centric representations without
explicit motion, showing that current self-supervised learning methods do not
learn objects from motion, but rather thanks to the Siamese architecture.
Finally, CropMAE achieves the highest masking ratio to date (98.5%), enabling
the reconstruction of images using only two visible patches. Our code is
available at https://github.com/alexandre-eymael/CropMAE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 6 figures, 3 tables, 1 page of supplementary material</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DN-Splatter: Depth and Normal Priors for Gaussian Splatting and Meshing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17822v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17822v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matias Turkulainen, Xuqian Ren, Iaroslav Melekhov, Otto Seiskari, Esa Rahtu, Juho Kannala
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D Gaussian splatting, a novel differentiable rendering technique, has
achieved state-of-the-art novel view synthesis results with high rendering
speeds and relatively low training times. However, its performance on scenes
commonly seen in indoor datasets is poor due to the lack of geometric
constraints during optimization. We extend 3D Gaussian splatting with depth and
normal cues to tackle challenging indoor datasets and showcase techniques for
efficient mesh extraction, an important downstream application. Specifically,
we regularize the optimization procedure with depth information, enforce local
smoothness of nearby Gaussians, and use the geometry of the 3D Gaussians
supervised by normal cues to achieve better alignment with the true scene
geometry. We improve depth estimation and novel view synthesis results over
baselines and show how this simple yet effective regularization technique can
be used to directly extract meshes from the Gaussian representation yielding
more physically accurate reconstructions on indoor scenes. Our code will be
released in https://github.com/maturk/dn-splatter.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Annotated Biomedical Video Generation using Denoising Diffusion
  Probabilistic Models and Flow Fields 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17808v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17808v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rüveyda Yilmaz, Dennis Eschweiler, Johannes Stegmaier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The segmentation and tracking of living cells play a vital role within the
biomedical domain, particularly in cancer research, drug development, and
developmental biology. These are usually tedious and time-consuming tasks that
are traditionally done by biomedical experts. Recently, to automatize these
processes, deep learning based segmentation and tracking methods have been
proposed. These methods require large-scale datasets and their full potential
is constrained by the scarcity of annotated data in the biomedical imaging
domain. To address this limitation, we propose Biomedical Video Diffusion Model
(BVDM), capable of generating realistic-looking synthetic microscopy videos.
Trained only on a single real video, BVDM can generate videos of arbitrary
length with pixel-level annotations that can be used for training data-hungry
models. It is composed of a denoising diffusion probabilistic model (DDPM)
generating high-fidelity synthetic cell microscopy images and a flow prediction
model (FPM) predicting the non-rigid transformation between consecutive video
frames. During inference, initially, the DDPM imposes realistic cell textures
on synthetic cell masks which are generated based on real data statistics. The
flow prediction model predicts the flow field between consecutive masks and
applies that to the DDPM output from the previous time frame to create the next
one while keeping temporal consistency. BVDM outperforms state-of-the-art
synthetic live cell microscopy video generation models. Furthermore, we
demonstrate that a sufficiently large synthetic dataset enhances the
performance of cell segmentation and tracking models compared to using a
limited amount of available real data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Text-to-Image Consistency via Automatic <span class="highlight-title">Prompt</span> Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17804v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17804v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oscar Mañas, Pietro Astolfi, Melissa Hall, Candace Ross, Jack Urbanek, Adina Williams, Aishwarya Agrawal, Adriana Romero-Soriano, Michal Drozdzal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Impressive advances in text-to-image (T2I) generative models have yielded a
plethora of high performing models which are able to generate aesthetically
appealing, photorealistic images. Despite the progress, these models still
struggle to produce images that are consistent with the input prompt,
oftentimes failing to capture object quantities, relations and attributes
properly. Existing solutions to improve prompt-image consistency suffer from
the following challenges: (1) they oftentimes require model fine-tuning, (2)
they only focus on nearby prompt samples, and (3) they are affected by
unfavorable trade-offs among image quality, representation diversity, and
prompt-image consistency. In this paper, we address these challenges and
introduce a T2I optimization-by-prompting framework, OPT2I, which leverages a
large language model (LLM) to improve prompt-image consistency in T2I models.
Our framework starts from a user prompt and iteratively generates revised
prompts with the goal of maximizing a consistency score. Our extensive
validation on two datasets, MSCOCO and PartiPrompts, shows that OPT2I can boost
the initial consistency score by up to 24.9% in terms of DSG score while
preserving the FID and increasing the recall between generated and real data.
Our work paves the way toward building more reliable and robust T2I systems by
harnessing the power of LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards 3D Vision with Low-Cost Single-Photon Cameras 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17801v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17801v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fangzhou Mu, Carter Sifferman, Sacha Jungerman, Yiquan Li, Mark Han, Michael Gleicher, Mohit Gupta, Yin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a method for reconstructing 3D shape of arbitrary Lambertian
objects based on measurements by miniature, energy-efficient, low-cost
single-photon cameras. These cameras, operating as time resolved image sensors,
illuminate the scene with a very fast pulse of diffuse light and record the
shape of that pulse as it returns back from the scene at a high temporal
resolution. We propose to model this image formation process, account for its
non-idealities, and adapt neural rendering to reconstruct 3D geometry from a
set of spatially distributed sensors with known poses. We show that our
approach can successfully recover complex 3D shapes from simulated data. We
further demonstrate 3D object reconstruction from real-world captures,
utilizing measurements from a commodity proximity sensor. Our work draws a
connection between image-based modeling and active range scanning and is a step
towards 3D vision with single-photon cameras.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating the Efficacy of <span class="highlight-title">Prompt</span>-Engineered Large Multimodal Models
  Versus Fine-Tuned Vision <span class="highlight-title">Transformer</span>s in Image-Based Security Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17787v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17787v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fouad Trad, Ali Chehab
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The success of Large Language Models (LLMs) has led to a parallel rise in the
development of Large Multimodal Models (LMMs), such as Gemini-pro, which have
begun to transform a variety of applications. These sophisticated multimodal
models are designed to interpret and analyze complex data, integrating both
textual and visual information on a scale previously unattainable, opening new
avenues for a range of applications. This paper investigates the applicability
and effectiveness of prompt-engineered Gemini-pro LMMs versus fine-tuned Vision
Transformer (ViT) models in addressing critical security challenges. We focus
on two distinct tasks: a visually evident task of detecting simple triggers,
such as small squares in images, indicative of potential backdoors, and a
non-visually evident task of malware classification through visual
representations. Our results highlight a significant divergence in performance,
with Gemini-pro falling short in accuracy and reliability when compared to
fine-tuned ViT models. The ViT models, on the other hand, demonstrate
exceptional accuracy, achieving near-perfect performance on both tasks. This
study not only showcases the strengths and limitations of prompt-engineered
LMMs in cybersecurity applications but also emphasizes the unmatched efficacy
of fine-tuned ViT models for precise and dependable tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GenesisTex: Adapting Image Denoising Diffusion to Texture Space 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17782v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17782v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenjian Gao, Boyan Jiang, Xinghui Li, Yingpeng Zhang, Qian Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present GenesisTex, a novel method for synthesizing textures for 3D
geometries from text descriptions. GenesisTex adapts the pretrained image
diffusion model to texture space by texture space sampling. Specifically, we
maintain a latent texture map for each viewpoint, which is updated with
predicted noise on the rendering of the corresponding viewpoint. The sampled
latent texture maps are then decoded into a final texture map. During the
sampling process, we focus on both global and local consistency across multiple
viewpoints: global consistency is achieved through the integration of style
consistency mechanisms within the noise prediction network, and low-level
consistency is achieved by dynamically aligning latent textures. Finally, we
apply reference-based inpainting and img2img on denser views for texture
refinement. Our approach overcomes the limitations of slow optimization in
distillation-based methods and instability in inpainting-based methods.
Experiments on meshes from various sources demonstrate that our method
surpasses the baseline methods quantitatively and qualitatively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CT Synthesis with Conditional Diffusion Models for Abdominal Lymph Node
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17770v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17770v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongrui Yu, Hanyu Chen, Zitian Zhang, Qiong Xiao, Wenhui Lei, Linrui Dai, Yu Fu, Hui Tan, Guan Wang, Peng Gao, Xiaofan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the significant success achieved by deep learning methods in medical
image segmentation, researchers still struggle in the computer-aided diagnosis
of abdominal lymph nodes due to the complex abdominal environment, small and
indistinguishable lesions, and limited annotated data. To address these
problems, we present a pipeline that integrates the conditional diffusion model
for lymph node generation and the nnU-Net model for lymph node segmentation to
improve the segmentation performance of abdominal lymph nodes through
synthesizing a diversity of realistic abdominal lymph node data. We propose
LN-DDPM, a conditional denoising diffusion probabilistic model (DDPM) for lymph
node (LN) generation. LN-DDPM utilizes lymph node masks and anatomical
structure masks as model conditions. These conditions work in two conditioning
mechanisms: global structure conditioning and local detail conditioning, to
distinguish between lymph nodes and their surroundings and better capture lymph
node characteristics. The obtained paired abdominal lymph node images and masks
are used for the downstream segmentation task. Experimental results on the
abdominal lymph node datasets demonstrate that LN-DDPM outperforms other
generative methods in the abdominal lymph node image synthesis and better
assists the downstream abdominal lymph node segmentation task.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MUTE-SLAM: Real-Time Neural SLAM with Multiple Tri-Plane Hash
  Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17765v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17765v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Yan, Ruomin He, Zhenghua Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce MUTE-SLAM, a real-time neural RGB-D SLAM system employing
multiple tri-plane hash-encodings for efficient scene representation. MUTE-SLAM
effectively tracks camera positions and incrementally builds a scalable
multi-map representation for both small and large indoor environments. It
dynamically allocates sub-maps for newly observed local regions, enabling
constraint-free mapping without prior scene information. Unlike traditional
grid-based methods, we use three orthogonal axis-aligned planes for
hash-encoding scene properties, significantly reducing hash collisions and the
number of trainable parameters. This hybrid approach not only speeds up
convergence but also enhances the fidelity of surface reconstruction.
Furthermore, our optimization strategy concurrently optimizes all sub-maps
intersecting with the current camera frustum, ensuring global consistency.
Extensive testing on both real-world and synthetic datasets has shown that
MUTE-SLAM delivers state-of-the-art surface reconstruction quality and
competitive tracking performance across diverse indoor settings. The code will
be made public upon acceptance of the paper.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Makeup Prior Models for 3D Facial Makeup Estimation and Applications <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17761v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17761v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingchao Yang, Takafumi Taketomi, Yuki Endo, Yoshihiro Kanamori
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we introduce two types of makeup prior models to extend
existing 3D face prior models: PCA-based and StyleGAN2-based priors. The
PCA-based prior model is a linear model that is easy to construct and is
computationally efficient. However, it retains only low-frequency information.
Conversely, the StyleGAN2-based model can represent high-frequency information
with relatively higher computational cost than the PCA-based model. Although
there is a trade-off between the two models, both are applicable to 3D facial
makeup estimation and related applications. By leveraging makeup prior models
and designing a makeup consistency module, we effectively address the
challenges that previous methods faced in robustly estimating makeup,
particularly in the context of handling self-occluded faces. In experiments, we
demonstrate that our approach reduces computational costs by several orders of
magnitude, achieving speeds up to 180 times faster. In addition, by improving
the accuracy of the estimated makeup, we confirm that our methods are highly
advantageous for various 3D facial makeup applications such as 3D makeup face
reconstruction, user-friendly makeup editing, makeup transfer, and
interpolation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR2024. Project: https://yangxingchao.github.io/makeup-priors-page</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Noise2Noise Denoising of CRISM Hyperspectral Data <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17757v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17757v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Robert Platt, Rossella Arcucci, Cédric John
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hyperspectral data acquired by the Compact Reconnaissance Imaging
Spectrometer for Mars (CRISM) have allowed for unparalleled mapping of the
surface mineralogy of Mars. Due to sensor degradation over time, a significant
portion of the recently acquired data is considered unusable. Here a new
data-driven model architecture, Noise2Noise4Mars (N2N4M), is introduced to
remove noise from CRISM images. Our model is self-supervised and does not
require zero-noise target data, making it well suited for use in Planetary
Science applications where high quality labelled data is scarce. We demonstrate
its strong performance on synthetic-noise data and CRISM images, and its impact
on downstream classification performance, outperforming benchmark methods on
most metrics. This allows for detailed analysis for critical sites of interest
on the Martian surface, including proposed lander sites.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 3 figures. Accepted as a conference paper at the ICLR 2024
  ML4RS Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DataCook: Crafting Anti-Adversarial Examples for Healthcare Data
  Copyright Protection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17755v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17755v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sihan Shang, Jiancheng Yang, Zhenglong Sun, Pascal Fua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the realm of healthcare, the challenges of copyright protection and
unauthorized third-party misuse are increasingly significant. Traditional
methods for data copyright protection are applied prior to data distribution,
implying that models trained on these data become uncontrollable. This paper
introduces a novel approach, named DataCook, designed to safeguard the
copyright of healthcare data during the deployment phase. DataCook operates by
"cooking" the raw data before distribution, enabling the development of models
that perform normally on this processed data. However, during the deployment
phase, the original test data must be also "cooked" through DataCook to ensure
normal model performance. This process grants copyright holders control over
authorization during the deployment phase. The mechanism behind DataCook is by
crafting anti-adversarial examples (AntiAdv), which are designed to enhance
model confidence, as opposed to standard adversarial examples (Adv) that aim to
confuse models. Similar to Adv, AntiAdv introduces imperceptible perturbations,
ensuring that the data processed by DataCook remains easily understandable. We
conducted extensive experiments on MedMNIST datasets, encompassing both 2D/3D
data and the high-resolution variants. The outcomes indicate that DataCook
effectively meets its objectives, preventing models trained on AntiAdv from
analyzing unauthorized data effectively, without compromising the validity and
accuracy of the data in legitimate scenarios. Code and data are available at
https://github.com/MedMNIST/DataCook.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Task Dense Prediction via Mixture of Low-Rank Experts <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17749v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17749v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuqi Yang, Peng-Tao Jiang, Qibin Hou, Hao Zhang, Jinwei Chen, Bo Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Previous multi-task dense prediction methods based on the Mixture of Experts
(MoE) have received great performance but they neglect the importance of
explicitly modeling the global relations among all tasks. In this paper, we
present a novel decoder-focused method for multi-task dense prediction, called
Mixture-of-Low-Rank-Experts (MLoRE). To model the global task relationships,
MLoRE adds a generic convolution path to the original MoE structure, where each
task feature can go through this path for explicit parameter sharing.
Furthermore, to control the parameters and computational cost brought by the
increase in the number of experts, we take inspiration from LoRA and propose to
leverage the low-rank format of a vanilla convolution in the expert network.
Since the low-rank experts have fewer parameters and can be dynamically
parameterized into the generic convolution, the parameters and computational
cost do not change much with the increase of experts. Benefiting from this
design, we increase the number of experts and its reception field to enlarge
the representation capacity, facilitating multiple dense tasks learning in a
unified network. Extensive experiments on the PASCAL-Context and NYUD-v2
benchmarks show that our MLoRE achieves superior performance compared to
previous state-of-the-art methods on all metrics. Our code is available at
https://github.com/YuqiYang213/MLoRE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Paired Diffusion: Generation of related, synthetic PET-CT-Segmentation
  scans using Linked Denoising Diffusion Probabilistic Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17734v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17734v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rowan Bradbury, Katherine A. Vallis, Bartlomiej W. Papiez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancement of Artificial Intelligence (AI) in biomedical imaging
and radiotherapy is hindered by the limited availability of large imaging data
repositories. With recent research and improvements in denoising diffusion
probabilistic models (DDPM), high quality synthetic medical scans are now
possible. Despite this, there is currently no way of generating multiple
related images, such as a corresponding ground truth which can be used to train
models, so synthetic scans are often manually annotated before use. This
research introduces a novel architecture that is able to generate multiple,
related PET-CT-tumour mask pairs using paired networks and conditional
encoders. Our approach includes innovative, time step-controlled mechanisms and
a `noise-seeding' strategy to improve DDPM sampling consistency. While our
model requires a modified perceptual loss function to ensure accurate feature
alignment we show generation of clearly aligned synthetic images and
improvement in segmentation accuracy with generated images.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>to be published in IEEE International Symposium on Biomedical Imaging
  2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FastPerson: Enhancing Video Learning through Effective Video
  Summarization that Preserves Linguistic and Visual Contexts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17727v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17727v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kazuki Kawamura, Jun Rekimoto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quickly understanding lengthy lecture videos is essential for learners with
limited time and interest in various topics to improve their learning
efficiency. To this end, video summarization has been actively researched to
enable users to view only important scenes from a video. However, these studies
focus on either the visual or audio information of a video and extract
important segments in the video. Therefore, there is a risk of missing
important information when both the teacher's speech and visual information on
the blackboard or slides are important, such as in a lecture video. To tackle
this issue, we propose FastPerson, a video summarization approach that
considers both the visual and auditory information in lecture videos.
FastPerson creates summary videos by utilizing audio transcriptions along with
on-screen images and text, minimizing the risk of overlooking crucial
information for learners. Further, it provides a feature that allows learners
to switch between the summary and original videos for each chapter of the
video, enabling them to adjust the pace of learning based on their interests
and level of understanding. We conducted an evaluation with 40 participants to
assess the effectiveness of our method and confirmed that it reduced viewing
time by 53\% at the same level of comprehension as that when using traditional
video playback methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Learning for Segmentation of Cracks in High-Resolution Images of
  Steel Bridges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17725v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17725v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrii Kompanets, Gautam Pai, Remco Duits, Davide Leonetti, Bert Snijder
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automating the current bridge visual inspection practices using drones and
image processing techniques is a prominent way to make these inspections more
effective, robust, and less expensive. In this paper, we investigate the
development of a novel deep-learning method for the detection of fatigue cracks
in high-resolution images of steel bridges. First, we present a novel and
challenging dataset comprising of images of cracks in steel bridges. Secondly,
we integrate the ConvNext neural network with a previous state- of-the-art
encoder-decoder network for crack segmentation. We study and report, the
effects of the use of background patches on the network performance when
applied to high-resolution images of cracks in steel bridges. Finally, we
introduce a loss function that allows the use of more background patches for
the training process, which yields a significant reduction in false positive
rates.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Invisible Gas Detection: An RGB-Thermal Cross Attention Network and A
  New Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17712v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17712v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jue Wang, Yuxiang Lin, Qi Zhao, Dong Luo, Shuaibao Chen, Wei Chen, Xiaojiang Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The widespread use of various chemical gases in industrial processes
necessitates effective measures to prevent their leakage during transportation
and storage, given their high toxicity. Thermal infrared-based computer vision
detection techniques provide a straightforward approach to identify gas leakage
areas. However, the development of high-quality algorithms has been challenging
due to the low texture in thermal images and the lack of open-source datasets.
In this paper, we present the RGB-Thermal Cross Attention Network (RT-CAN),
which employs an RGB-assisted two-stream network architecture to integrate
texture information from RGB images and gas area information from thermal
images. Additionally, to facilitate the research of invisible gas detection, we
introduce Gas-DB, an extensive open-source gas detection database including
about 1.3K well-annotated RGB-thermal images with eight variant collection
scenes. Experimental results demonstrate that our method successfully leverages
the advantages of both modalities, achieving state-of-the-art (SOTA)
performance among RGB-thermal methods, surpassing single-stream SOTA models in
terms of accuracy, Intersection of Union (IoU), and F2 metrics by 4.86%, 5.65%,
and 4.88%, respectively. The code and data will be made available soon.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Groupwise Query Specialization and Quality-Aware Multi-Assignment for
  <span class="highlight-title">Transformer</span>-based Visual Relationship Detection <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17709v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17709v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jongha Kim, Jihwan Park, Jinyoung Park, Jinyoung Kim, Sehyung Kim, Hyunwoo J. Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual Relationship Detection (VRD) has seen significant advancements with
Transformer-based architectures recently. However, we identify two key
limitations in a conventional label assignment for training Transformer-based
VRD models, which is a process of mapping a ground-truth (GT) to a prediction.
Under the conventional assignment, an unspecialized query is trained since a
query is expected to detect every relation, which makes it difficult for a
query to specialize in specific relations. Furthermore, a query is also
insufficiently trained since a GT is assigned only to a single prediction,
therefore near-correct or even correct predictions are suppressed by being
assigned no relation as a GT. To address these issues, we propose Groupwise
Query Specialization and Quality-Aware Multi-Assignment (SpeaQ). Groupwise
Query Specialization trains a specialized query by dividing queries and
relations into disjoint groups and directing a query in a specific query group
solely toward relations in the corresponding relation group. Quality-Aware
Multi-Assignment further facilitates the training by assigning a GT to multiple
predictions that are significantly close to a GT in terms of a subject, an
object, and the relation in between. Experimental results and analyses show
that SpeaQ effectively trains specialized queries, which better utilize the
capacity of a model, resulting in consistent performance gains with zero
additional inference cost across multiple VRD models and benchmarks. Code is
available at https://github.com/mlvlab/SpeaQ.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Panonut360: A Head and Eye Tracking <span class="highlight-title">Dataset</span> for Panoramic Video <span class="chip">ACM MM</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17708v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17708v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yutong Xu, Junhao Du, Jiahe Wang, Yuwei Ning, Sihan Zhou Yang Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid development and widespread application of VR/AR technology,
maximizing the quality of immersive panoramic video services that match users'
personal preferences and habits has become a long-standing challenge.
Understanding the saliency region where users focus, based on data collected
with HMDs, can promote multimedia encoding, transmission, and quality
assessment. At the same time, large-scale datasets are essential for
researchers and developers to explore short/long-term user behavior patterns
and train AI models related to panoramic videos. However, existing panoramic
video datasets often include low-frequency user head or eye movement data
through short-term videos only, lacking sufficient data for analyzing users'
Field of View (FoV) and generating video saliency regions.
  Driven by these practical factors, in this paper, we present a head and eye
tracking dataset involving 50 users (25 males and 25 females) watching 15
panoramic videos. The dataset provides details on the viewport and gaze
attention locations of users. Besides, we present some statistics samples
extracted from the dataset. For example, the deviation between head and eye
movements challenges the widely held assumption that gaze attention decreases
from the center of the FoV following a Gaussian distribution. Our analysis
reveals a consistent downward offset in gaze fixations relative to the FoV in
experimental settings involving multiple users and videos. That's why we name
the dataset Panonut, a saliency weighting shaped like a donut. Finally, we also
provide a script that generates saliency distributions based on given head or
eye coordinates and pre-generated saliency distribution map sets of each video
from the collected eye tracking data.
  The dataset is available on website: https://dianvrlab.github.io/Panonut360/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages,ACM MMSys'24 accepted</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Solution for the CVPR 2023 1st foundation model challenge-Track2 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17702v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17702v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haonan Xu, Yurui Huang, Sishun Pan, Zhihao Guan, Yi Xu, Yang Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a solution for cross-modal transportation
retrieval. Due to the cross-domain problem of traffic images, we divide the
problem into two sub-tasks of pedestrian retrieval and vehicle retrieval
through a simple strategy. In pedestrian retrieval tasks, we use IRRA as the
base model and specifically design an Attribute Classification to mine the
knowledge implied by attribute labels. More importantly, We use the strategy of
Inclusion Relation Matching to make the image-text pairs with inclusion
relation have similar representation in the feature space. For the vehicle
retrieval task, we use BLIP as the base model. Since aligning the color
attributes of vehicles is challenging, we introduce attribute-based object
detection techniques to add color patch blocks to vehicle images for color data
augmentation. This serves as strong prior information, helping the model
perform the image-text alignment. At the same time, we incorporate labeled
attributes into the image-text alignment loss to learn fine-grained alignment
and prevent similar images and texts from being incorrectly separated. Our
approach ranked first in the final B-board test with a score of 70.9.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rotate to Scan: UNet-like Mamba with Triplet SSM Module for Medical
  Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17701v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17701v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Tang, Lianglun Cheng, Guoheng Huang, Zhengguang Tan, Junhao Lu, Kaihong Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image segmentation holds a vital position in the realms of diagnosis and
treatment within the medical domain. Traditional convolutional neural networks
(CNNs) and Transformer models have made significant advancements in this realm,
but they still encounter challenges because of limited receptive field or high
computing complexity. Recently, State Space Models (SSMs), particularly Mamba
and its variants, have demonstrated notable performance in the field of vision.
However, their feature extraction methods may not be sufficiently effective and
retain some redundant structures, leaving room for parameter reduction.
Motivated by previous spatial and channel attention methods, we propose Triplet
Mamba-UNet. The method leverages residual VSS Blocks to extract intensive
contextual features, while Triplet SSM is employed to fuse features across
spatial and channel dimensions. We conducted experiments on ISIC17, ISIC18,
CVC-300, CVC-ClinicDB, Kvasir-SEG, CVC-ColonDB, and Kvasir-Instrument datasets,
demonstrating the superior segmentation performance of our proposed TM-UNet.
Additionally, compared to the previous VM-UNet, our model achieves a one-third
reduction in parameters.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PlainMamba: Improving Non-Hierarchical Mamba in Visual Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17695v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17695v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenhongyi Yang, Zehui Chen, Miguel Espinosa, Linus Ericsson, Zhenyu Wang, Jiaming Liu, Elliot J. Crowley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present PlainMamba: a simple non-hierarchical state space model (SSM)
designed for general visual recognition. The recent Mamba model has shown how
SSMs can be highly competitive with other architectures on sequential data and
initial attempts have been made to apply it to images. In this paper, we
further adapt the selective scanning process of Mamba to the visual domain,
enhancing its ability to learn features from two-dimensional images by (i) a
continuous 2D scanning process that improves spatial continuity by ensuring
adjacency of tokens in the scanning sequence, and (ii) direction-aware updating
which enables the model to discern the spatial relations of tokens by encoding
directional information. Our architecture is designed to be easy to use and
easy to scale, formed by stacking identical PlainMamba blocks, resulting in a
model with constant width throughout all layers. The architecture is further
simplified by removing the need for special tokens. We evaluate PlainMamba on a
variety of visual recognition tasks including image classification, semantic
segmentation, object detection, and instance segmentation. Our method achieves
performance gains over previous non-hierarchical models and is competitive with
hierarchical alternatives. For tasks requiring high-resolution inputs, in
particular, PlainMamba requires much less computing while maintaining high
performance. Code and models are available at
https://github.com/ChenhongyiYang/PlainMamba
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AniPortrait: Audio-Driven Synthesis of Photorealistic Portrait Animation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17694v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17694v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huawei Wei, Zejun Yang, Zhisheng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we propose AniPortrait, a novel framework for generating
high-quality animation driven by audio and a reference portrait image. Our
methodology is divided into two stages. Initially, we extract 3D intermediate
representations from audio and project them into a sequence of 2D facial
landmarks. Subsequently, we employ a robust diffusion model, coupled with a
motion module, to convert the landmark sequence into photorealistic and
temporally consistent portrait animation. Experimental results demonstrate the
superiority of AniPortrait in terms of facial naturalness, pose diversity, and
visual quality, thereby offering an enhanced perceptual experience. Moreover,
our methodology exhibits considerable potential in terms of flexibility and
controllability, which can be effectively applied in areas such as facial
motion editing or face reenactment. We release code and model weights at
https://github.com/scutzzj/AniPortrait
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Manifold-Guided Lyapunov Control with Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17692v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17692v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amartya Mukherjee, Thanin Quartz, Jun Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a novel approach to generating stabilizing controllers
for a large class of dynamical systems using diffusion models. The core
objective is to develop stabilizing control functions by identifying the
closest asymptotically stable vector field relative to a predetermined manifold
and adjusting the control function based on this finding. To achieve this, we
employ a diffusion model trained on pairs consisting of asymptotically stable
vector fields and their corresponding Lyapunov functions. Our numerical results
demonstrate that this pre-trained model can achieve stabilization over
previously unseen systems efficiently and rapidly, showcasing the potential of
our approach in fast zero-shot control and generalizability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Not All Similarities Are Created Equal: Leveraging Data-Driven Biases to
  Inform GenAI Copyright Disputes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17691v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17691v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Uri Hacohen, Adi Haviv, Shahar Sarfaty, Bruria Friedman, Niva Elkin-Koren, Roi Livni, Amit H Bermano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of Generative Artificial Intelligence (GenAI) models, including
GitHub Copilot, OpenAI GPT, and Stable Diffusion, has revolutionized content
creation, enabling non-professionals to produce high-quality content across
various domains. This transformative technology has led to a surge of synthetic
content and sparked legal disputes over copyright infringement. To address
these challenges, this paper introduces a novel approach that leverages the
learning capacity of GenAI models for copyright legal analysis, demonstrated
with GPT2 and Stable Diffusion models. Copyright law distinguishes between
original expressions and generic ones (Sc\`enes \`a faire), protecting the
former and permitting reproduction of the latter. However, this distinction has
historically been challenging to make consistently, leading to over-protection
of copyrighted works. GenAI offers an unprecedented opportunity to enhance this
legal analysis by revealing shared patterns in preexisting works. We propose a
data-driven approach to identify the genericity of works created by GenAI,
employing "data-driven bias" to assess the genericity of expressive
compositions. This approach aids in copyright scope determination by utilizing
the capabilities of GenAI to identify and prioritize expressive elements and
rank them according to their frequency in the model's dataset. The potential
implications of measuring expressive genericity for copyright law are profound.
Such scoring could assist courts in determining copyright scope during
litigation, inform the registration practices of Copyright Offices, allowing
registration of only highly original synthetic works, and help copyright owners
signal the value of their works and facilitate fairer licensing deals. More
generally, this approach offers valuable insights to policymakers grappling
with adapting copyright law to the challenges posed by the era of GenAI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at ACM CSLAW 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hierarchical Light <span class="highlight-title">Transformer</span> Ensembles for Multimodal Trajectory
  Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17678v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17678v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adrien Lafage, Mathieu Barbier, Gianni Franchi, David Filliat
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate trajectory forecasting is crucial for the performance of various
systems, such as advanced driver-assistance systems and self-driving vehicles.
These forecasts allow to anticipate events leading to collisions and,
therefore, to mitigate them. Deep Neural Networks have excelled in motion
forecasting, but issues like overconfidence and uncertainty quantification
persist. Deep Ensembles address these concerns, yet applying them to multimodal
distributions remains challenging. In this paper, we propose a novel approach
named Hierarchical Light Transformer Ensembles (HLT-Ens), aimed at efficiently
training an ensemble of Transformer architectures using a novel hierarchical
loss function. HLT-Ens leverages grouped fully connected layers, inspired by
grouped convolution techniques, to capture multimodal distributions,
effectively. Through extensive experimentation, we demonstrate that HLT-Ens
achieves state-of-the-art performance levels, offering a promising avenue for
improving trajectory forecasting techniques.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Predicting Perceived Gloss: Do Weak Labels Suffice? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17672v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17672v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julia Guerrero-Viu, J. Daniel Subias, Ana Serrano, Katherine R. Storrs, Roland W. Fleming, Belen Masia, Diego Gutierrez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Estimating perceptual attributes of materials directly from images is a
challenging task due to their complex, not fully-understood interactions with
external factors, such as geometry and lighting. Supervised deep learning
models have recently been shown to outperform traditional approaches, but rely
on large datasets of human-annotated images for accurate perception
predictions. Obtaining reliable annotations is a costly endeavor, aggravated by
the limited ability of these models to generalise to different aspects of
appearance. In this work, we show how a much smaller set of human annotations
("strong labels") can be effectively augmented with automatically derived "weak
labels" in the context of learning a low-dimensional image-computable gloss
metric. We evaluate three alternative weak labels for predicting human gloss
perception from limited annotated data. Incorporating weak labels enhances our
gloss prediction beyond the current state of the art. Moreover, it enables a
substantial reduction in human annotation costs without sacrificing accuracy,
whether working with rendered images or real photographs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Computer Graphics Forum (Eurographics 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DiffFAE: Advancing High-fidelity One-shot Facial Appearance Editing with
  Space-sensitive Customization and Semantic Preservation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17664v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17664v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qilin Wang, Jiangning Zhang, Chengming Xu, Weijian Cao, Ying Tai, Yue Han, Yanhao Ge, Hong Gu, Chengjie Wang, Yanwei Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Facial Appearance Editing (FAE) aims to modify physical attributes, such as
pose, expression and lighting, of human facial images while preserving
attributes like identity and background, showing great importance in
photograph. In spite of the great progress in this area, current researches
generally meet three challenges: low generation fidelity, poor attribute
preservation, and inefficient inference. To overcome above challenges, this
paper presents DiffFAE, a one-stage and highly-efficient diffusion-based
framework tailored for high-fidelity FAE. For high-fidelity query attributes
transfer, we adopt Space-sensitive Physical Customization (SPC), which ensures
the fidelity and generalization ability by utilizing rendering texture derived
from 3D Morphable Model (3DMM). In order to preserve source attributes, we
introduce the Region-responsive Semantic Composition (RSC). This module is
guided to learn decoupled source-regarding features, thereby better preserving
the identity and alleviating artifacts from non-facial attributes such as hair,
clothes, and background. We further introduce a consistency regularization for
our pipeline to enhance editing controllability by leveraging prior knowledge
in the attention matrices of diffusion model. Extensive experiments demonstrate
the superiority of DiffFAE over existing methods, achieving state-of-the-art
performance in facial appearance editing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Dynamic <span class="highlight-title">Transformer</span> for Efficient Object Tracking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17651v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17651v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiawen Zhu, Xin Chen, Haiwen Diao, Shuai Li, Jun-Yan He, Chenyang Li, Bin Luo, Dong Wang, Huchuan Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The speed-precision trade-off is a critical problem for visual object
tracking which usually requires low latency and deployment on constrained
resources. Existing solutions for efficient tracking mainly focus on adopting
light-weight backbones or modules, which nevertheless come at the cost of a
sacrifice in precision. In this paper, inspired by dynamic network routing, we
propose DyTrack, a dynamic transformer framework for efficient tracking.
Real-world tracking scenarios exhibit diverse levels of complexity. We argue
that a simple network is sufficient for easy frames in video sequences, while
more computation could be assigned to difficult ones. DyTrack automatically
learns to configure proper reasoning routes for various inputs, gaining better
utilization of the available computational budget. Thus, it can achieve higher
performance with the same running speed. We formulate instance-specific
tracking as a sequential decision problem and attach terminating branches to
intermediate layers of the entire model. Especially, to fully utilize the
computations, we introduce the feature recycling mechanism to reuse the outputs
of predecessors. Furthermore, a target-aware self-distillation strategy is
designed to enhance the discriminating capabilities of early predictions by
effectively mimicking the representation pattern of the deep model. Extensive
experiments on multiple benchmarks demonstrate that DyTrack achieves promising
speed-precision trade-offs with only a single model. For instance, DyTrack
obtains 64.9% AUC on LaSOT with a speed of 256 fps.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ High-Resolution Image Translation Model Based on Grayscale Redefinition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17639v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17639v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xixian Wu, Dian Chao, Yang Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image-to-image translation is a technique that focuses on transferring images
from one domain to another while maintaining the essential content
representations. In recent years, image-to-image translation has gained
significant attention and achieved remarkable advancements due to its diverse
applications in computer vision and image processing tasks. In this work, we
propose an innovative method for image translation between different domains.
For high-resolution image translation tasks, we use a grayscale adjustment
method to achieve pixel-level translation. For other tasks, we utilize the
Pix2PixHD model with a coarse-to-fine generator, multi-scale discriminator, and
improved loss to enhance the image translation performance. On the other hand,
to tackle the issue of sparse training data, we adopt model weight
initialization from other task to optimize the performance of the current task.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning with Unreliability: Fast Few-shot Voxel Radiance Fields with
  Relative Geometric Consistency <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17638v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17638v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingjie Xu, Bangzhen Liu, Hao Tang, Bailin Deng, Shengfeng He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a voxel-based optimization framework, ReVoRF, for few-shot
radiance fields that strategically address the unreliability in pseudo novel
view synthesis. Our method pivots on the insight that relative depth
relationships within neighboring regions are more reliable than the absolute
color values in disoccluded areas. Consequently, we devise a bilateral
geometric consistency loss that carefully navigates the trade-off between color
fidelity and geometric accuracy in the context of depth consistency for
uncertain regions. Moreover, we present a reliability-guided learning strategy
to discern and utilize the variable quality across synthesized views,
complemented by a reliability-aware voxel smoothing algorithm that smoothens
the transition between reliable and unreliable data patches. Our approach
allows for a more nuanced use of all available data, promoting enhanced
learning from regions previously considered unsuitable for high-quality
reconstruction. Extensive experiments across diverse datasets reveal that our
approach attains significant gains in efficiency and accuracy, delivering
rendering speeds of 3 FPS, 7 mins to train a $360^\circ$ scene, and a 5\%
improvement in PSNR over existing few-shot methods. Code is available at
https://github.com/HKCLynn/ReVoRF.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024 final version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UADA3D: Unsupervised Adversarial Domain Adaptation for 3D Object
  Detection with Sparse LiDAR and Large Domain Gaps 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17633v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17633v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maciej K Wozniak, Mattias Hansson, Marko Thiel, Patric Jensfelt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we address a gap in existing unsupervised domain adaptation
approaches on LiDAR-based 3D object detection, which have predominantly
concentrated on adapting between established, high-density autonomous driving
datasets. We focus on sparser point clouds, capturing scenarios from different
perspectives: not just from vehicles on the road but also from mobile robots on
sidewalks, which encounter significantly different environmental conditions and
sensor configurations. We introduce Unsupervised Adversarial Domain Adaptation
for 3D Object Detection (UADA3D). UADA3D does not depend on pre-trained source
models or teacher-student architectures. Instead, it uses an adversarial
approach to directly learn domain-invariant features. We demonstrate its
efficacy in various adaptation scenarios, showing significant improvements in
both self-driving car and mobile robot domains. Our code is open-source and
will be available soon.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AniArtAvatar: Animatable 3D Art Avatar from a Single Image 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17631v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17631v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaoxu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel approach for generating animatable 3D-aware art avatars
from a single image, with controllable facial expressions, head poses, and
shoulder movements. Unlike previous reenactment methods, our approach utilizes
a view-conditioned 2D diffusion model to synthesize multi-view images from a
single art portrait with a neutral expression. With the generated colors and
normals, we synthesize a static avatar using an SDF-based neural surface. For
avatar animation, we extract control points, transfer the motion with these
points, and deform the implicit canonical space. Firstly, we render the front
image of the avatar, extract the 2D landmarks, and project them to the 3D space
using a trained SDF network. We extract 3D driving landmarks using 3DMM and
transfer the motion to the avatar landmarks. To animate the avatar pose, we
manually set the body height and bound the head and torso of an avatar with two
cages. The head and torso can be animated by transforming the two cages. Our
approach is a one-shot pipeline that can be applied to various styles.
Experiments demonstrate that our method can generate high-quality 3D art
avatars with desired control over different motions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Grad-CAMO: Learning Interpretable Single-Cell Morphological Profiles
  from 3D Cell Painting Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17615v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17615v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vivek Gopalakrishnan, Jingzhe Ma, Zhiyong Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite their black-box nature, deep learning models are extensively used in
image-based drug discovery to extract feature vectors from single cells in
microscopy images. To better understand how these networks perform
representation learning, we employ visual explainability techniques (e.g.,
Grad-CAM). Our analyses reveal several mechanisms by which supervised models
cheat, exploiting biologically irrelevant pixels when extracting morphological
features from images, such as noise in the background. This raises doubts
regarding the fidelity of learned single-cell representations and their
relevance when investigating downstream biological questions. To address this
misalignment between researcher expectations and machine behavior, we introduce
Grad-CAMO, a novel single-cell interpretability score for supervised feature
extractors. Grad-CAMO measures the proportion of a model's attention that is
concentrated on the cell of interest versus the background. This metric can be
assessed per-cell or averaged across a validation set, offering a tool to audit
individual features vectors or guide the improved design of deep learning
architectures. Importantly, Grad-CAMO seamlessly integrates into existing
workflows, requiring no dataset or model modifications, and is compatible with
both 2D and 3D Cell Painting data. Additional results are available at
https://github.com/eigenvivek/Grad-CAMO.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MMVP: A Multimodal MoCap <span class="highlight-title">Dataset</span> with Vision and Pressure Sensors <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17610v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17610v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        He Zhang, Shenghao Ren, Haolei Yuan, Jianhui Zhao, Fan Li, Shuangpeng Sun, Zhenghao Liang, Tao Yu, Qiu Shen, Xun Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foot contact is an important cue not only for human motion capture but also
for motion understanding and physically plausible motion generation. However,
most of the foot-contact annotations in existing datasets are estimated by
purely visual matching and distance thresholding, which results in low accuracy
and coarse granularity. Even though existing multimodal datasets
synergistically capture plantar pressure (foot contact) and visual signals,
they are specifically designed for small-range and slow motion such as Taiji
Quan and Yoga. Therefore, there is still a lack of a vision-pressure multimodal
dataset with large-range and fast human motion, as well as accurate and dense
foot-contact annotation. To fill this gap, we propose a Multimodal MoCap
Dataset with Vision and Pressure sensors, named MMVP. MMVP provides accurate
and dense plantar pressure signals synchronized with RGBD observations, which
is especially useful for both plausible shape estimation, robust pose fitting
without foot drifting, and accurate global translation tracking. To validate
the dataset, we propose an RGBD-P SMPL fitting method and also a
monocular-video-based baseline framework, VP-MoCap, for human motion capture.
Experiments demonstrate that our RGBD-P SMPL Fitting results significantly
outperform pure visual motion capture. Moreover, VP-MoCap outperforms SOTA
methods in foot-contact and global translation estimation accuracy. We believe
the configuration of the dataset and the baseline frameworks will stimulate the
research in this direction and also provide a good reference for MoCap
applications in various domains. Project page:
https://haolyuan.github.io/MMVP-Dataset/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fake or JPEG? Revealing Common Biases in Generated Image Detection
  <span class="highlight-title">Dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17608v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17608v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Patrick Grommelt, Louis Weiss, Franz-Josef Pfreundt, Janis Keuper
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The widespread adoption of generative image models has highlighted the urgent
need to detect artificial content, which is a crucial step in combating
widespread manipulation and misinformation. Consequently, numerous detectors
and associated datasets have emerged. However, many of these datasets
inadvertently introduce undesirable biases, thereby impacting the effectiveness
and evaluation of detectors. In this paper, we emphasize that many datasets for
AI-generated image detection contain biases related to JPEG compression and
image size. Using the GenImage dataset, we demonstrate that detectors indeed
learn from these undesired factors. Furthermore, we show that removing the
named biases substantially increases robustness to JPEG compression and
significantly alters the cross-generator performance of evaluated detectors.
Specifically, it leads to more than 11 percentage points increase in
cross-generator performance for ResNet50 and Swin-T detectors on the GenImage
dataset, achieving state-of-the-art results.
  We provide the dataset and source codes of this paper on the anonymous
website: https://www.unbiased-genimage.org
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dual Memory Networks: A Versatile Adaptation Approach for
  Vision-Language Models <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17589v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17589v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yabin Zhang, Wenjie Zhu, Hui Tang, Zhiyuan Ma, Kaiyang Zhou, Lei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the emergence of pre-trained vision-language models like CLIP, how to
adapt them to various downstream classification tasks has garnered significant
attention in recent research. The adaptation strategies can be typically
categorized into three paradigms: zero-shot adaptation, few-shot adaptation,
and the recently-proposed training-free few-shot adaptation. Most existing
approaches are tailored for a specific setting and can only cater to one or two
of these paradigms. In this paper, we introduce a versatile adaptation approach
that can effectively work under all three settings. Specifically, we propose
the dual memory networks that comprise dynamic and static memory components.
The static memory caches training data knowledge, enabling training-free
few-shot adaptation, while the dynamic memory preserves historical test
features online during the testing process, allowing for the exploration of
additional data insights beyond the training set. This novel capability
enhances model performance in the few-shot setting and enables model usability
in the absence of training data. The two memory networks employ the same
flexible memory interactive strategy, which can operate in a training-free mode
and can be further enhanced by incorporating learnable projection layers. Our
approach is tested across 11 datasets under the three task settings.
Remarkably, in the zero-shot scenario, it outperforms existing methods by over
3\% and even shows superior results against methods utilizing external training
data. Additionally, our method exhibits robust performance against natural
distribution shifts. Codes are available at \url{https://github.com/YBZh/DMN}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR2024; Codes are available at \url{https://github.com/YBZh/DMN}</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DeepMIF: Deep Monotonic Implicit Fields for Large-Scale LiDAR 3D Mapping 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17550v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17550v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kutay Yılmaz, Matthias Nießner, Anastasiia Kornilova, Alexey Artemov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, significant progress has been achieved in sensing real large-scale
outdoor 3D environments, particularly by using modern acquisition equipment
such as LiDAR sensors. Unfortunately, they are fundamentally limited in their
ability to produce dense, complete 3D scenes. To address this issue, recent
learning-based methods integrate neural implicit representations and
optimizable feature grids to approximate surfaces of 3D scenes. However,
naively fitting samples along raw LiDAR rays leads to noisy 3D mapping results
due to the nature of sparse, conflicting LiDAR measurements. Instead, in this
work we depart from fitting LiDAR data exactly, instead letting the network
optimize a non-metric monotonic implicit field defined in 3D space. To fit our
field, we design a learning system integrating a monotonicity loss that enables
optimizing neural monotonic fields and leverages recent progress in large-scale
3D mapping. Our algorithm achieves high-quality dense 3D mapping performance as
captured by multiple quantitative and perceptual measures and visual results
obtained for Mai City, Newer College, and KITTI benchmarks. The code of our
approach will be made publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Practical Applications of Advanced Cloud Services and Generative AI
  Systems in Medical Image Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17549v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17549v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingyu Xu, Binbin Wu, Jiaxin Huang, Yulu Gong, Yifan Zhang, Bo Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The medical field is one of the important fields in the application of
artificial intelligence technology. With the explosive growth and
diversification of medical data, as well as the continuous improvement of
medical needs and challenges, artificial intelligence technology is playing an
increasingly important role in the medical field. Artificial intelligence
technologies represented by computer vision, natural language processing, and
machine learning have been widely penetrated into diverse scenarios such as
medical imaging, health management, medical information, and drug research and
development, and have become an important driving force for improving the level
and quality of medical services.The article explores the transformative
potential of generative AI in medical imaging, emphasizing its ability to
generate syntheticACM-2 data, enhance images, aid in anomaly detection, and
facilitate image-to-image translation. Despite challenges like model
complexity, the applications of generative models in healthcare, including
Med-PaLM 2 technology, show promising results. By addressing limitations in
dataset size and diversity, these models contribute to more accurate diagnoses
and improved patient outcomes. However, ethical considerations and
collaboration among stakeholders are essential for responsible implementation.
Through experiments leveraging GANs to augment brain tumor MRI datasets, the
study demonstrates how generative AI can enhance image quality and diversity,
ultimately advancing medical diagnostics and patient care.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Gaze-grounded Visual Question Answering <span class="highlight-title">Dataset</span> for Clarifying
  Ambiguous Japanese Questions <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17545v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17545v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shun Inadumi, Seiya Kawano, Akishige Yuguchi, Yasutomo Kawanishi, Koichiro Yoshino
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Situated conversations, which refer to visual information as visual question
answering (VQA), often contain ambiguities caused by reliance on directive
information. This problem is exacerbated because some languages, such as
Japanese, often omit subjective or objective terms. Such ambiguities in
questions are often clarified by the contexts in conversational situations,
such as joint attention with a user or user gaze information. In this study, we
propose the Gaze-grounded VQA dataset (GazeVQA) that clarifies ambiguous
questions using gaze information by focusing on a clarification process
complemented by gaze information. We also propose a method that utilizes gaze
target estimation results to improve the accuracy of GazeVQA tasks. Our
experimental results showed that the proposed method improved the performance
in some cases of a VQA system on GazeVQA and identified some typical problems
of GazeVQA tasks that need to be improved.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ WordRobe: Text-Guided Generation of Textured 3D Garments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17541v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17541v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Astitva Srivastava, Pranav Manu, Amit Raj, Varun Jampani, Avinash Sharma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we tackle a new and challenging problem of text-driven
generation of 3D garments with high-quality textures. We propose "WordRobe", a
novel framework for the generation of unposed & textured 3D garment meshes from
user-friendly text prompts. We achieve this by first learning a latent
representation of 3D garments using a novel coarse-to-fine training strategy
and a loss for latent disentanglement, promoting better latent interpolation.
Subsequently, we align the garment latent space to the CLIP embedding space in
a weakly supervised manner, enabling text-driven 3D garment generation and
editing. For appearance modeling, we leverage the zero-shot generation
capability of ControlNet to synthesize view-consistent texture maps in a single
feed-forward inference step, thereby drastically decreasing the generation time
as compared to existing methods. We demonstrate superior performance over
current SOTAs for learning 3D garment latent space, garment interpolation, and
text-driven texture synthesis, supported by quantitative evaluation and
qualitative user study. The unposed 3D garment meshes generated using WordRobe
can be directly fed to standard cloth simulation & animation pipelines without
any post-processing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NeRF-HuGS: Improved Neural Radiance Fields in Non-static Scenes Using
  Heuristics-Guided Segmentation <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17537v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17537v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahao Chen, Yipeng Qin, Lingjie Liu, Jiangbo Lu, Guanbin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Radiance Field (NeRF) has been widely recognized for its excellence in
novel view synthesis and 3D scene reconstruction. However, their effectiveness
is inherently tied to the assumption of static scenes, rendering them
susceptible to undesirable artifacts when confronted with transient distractors
such as moving objects or shadows. In this work, we propose a novel paradigm,
namely "Heuristics-Guided Segmentation" (HuGS), which significantly enhances
the separation of static scenes from transient distractors by harmoniously
combining the strengths of hand-crafted heuristics and state-of-the-art
segmentation models, thus significantly transcending the limitations of
previous solutions. Furthermore, we delve into the meticulous design of
heuristics, introducing a seamless fusion of Structure-from-Motion (SfM)-based
heuristics and color residual heuristics, catering to a diverse range of
texture profiles. Extensive experiments demonstrate the superiority and
robustness of our method in mitigating transient distractors for NeRFs trained
in non-static scenes. Project page: https://cnhaox.github.io/NeRF-HuGS/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in CVPR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Boosting Few-Shot Learning with Disentangled <span class="highlight-title">Self-Supervised</span> Learning
  and Meta-Learning for Medical Image Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17530v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17530v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eva Pachetti, Sotirios A. Tsaftaris, Sara Colantonio
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Background and objective: Employing deep learning models in critical domains
such as medical imaging poses challenges associated with the limited
availability of training data. We present a strategy for improving the
performance and generalization capabilities of models trained in low-data
regimes. Methods: The proposed method starts with a pre-training phase, where
features learned in a self-supervised learning setting are disentangled to
improve the robustness of the representations for downstream tasks. We then
introduce a meta-fine-tuning step, leveraging related classes between
meta-training and meta-testing phases but varying the granularity level. This
approach aims to enhance the model's generalization capabilities by exposing it
to more challenging classification tasks during meta-training and evaluating it
on easier tasks but holding greater clinical relevance during meta-testing. We
demonstrate the effectiveness of the proposed approach through a series of
experiments exploring several backbones, as well as diverse pre-training and
fine-tuning schemes, on two distinct medical tasks, i.e., classification of
prostate cancer aggressiveness from MRI data and classification of breast
cancer malignity from microscopic images. Results: Our results indicate that
the proposed approach consistently yields superior performance w.r.t. ablation
experiments, maintaining competitiveness even when a distribution shift between
training and evaluation data occurs. Conclusion: Extensive experiments
demonstrate the effectiveness and wide applicability of the proposed approach.
We hope that this work will add another solution to the arsenal of addressing
learning issues in data-scarce imaging domains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 4 figures, 4 tables. Submitted to Elsevier on 25 March 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Equipping Sketch Patches with Context-Aware Positional Encoding for
  Graphic Sketch Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17525v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17525v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sicong Zang, Zhijun Fang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The drawing order of a sketch records how it is created stroke-by-stroke by a
human being. For graphic sketch representation learning, recent studies have
injected sketch drawing orders into graph edge construction by linking each
patch to another in accordance to a temporal-based nearest neighboring
strategy. However, such constructed graph edges may be unreliable, since a
sketch could have variants of drawings. In this paper, we propose a
variant-drawing-protected method by equipping sketch patches with context-aware
positional encoding (PE) to make better use of drawing orders for learning
graphic sketch representation. Instead of injecting sketch drawings into graph
edges, we embed these sequential information into graph nodes only. More
specifically, each patch embedding is equipped with a sinusoidal absolute PE to
highlight the sequential position in the drawing order. And its neighboring
patches, ranked by the values of self-attention scores between patch
embeddings, are equipped with learnable relative PEs to restore the contextual
positions within a neighborhood. During message aggregation via graph
convolutional networks, a node receives both semantic contents from patch
embeddings and contextual patterns from PEs by its neighbors, arriving at
drawing-order-enhanced sketch representations. Experimental results indicate
that our method significantly improves sketch healing and controllable sketch
synthesis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Boosting Adversarial Training via Fisher-Rao Norm-based Regularization <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17520v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17520v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiangyu Yin, Wenjie Ruan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adversarial training is extensively utilized to improve the adversarial
robustness of deep neural networks. Yet, mitigating the degradation of standard
generalization performance in adversarial-trained models remains an open
problem. This paper attempts to resolve this issue through the lens of model
complexity. First, We leverage the Fisher-Rao norm, a geometrically invariant
metric for model complexity, to establish the non-trivial bounds of the
Cross-Entropy Loss-based Rademacher complexity for a ReLU-activated Multi-Layer
Perceptron. Then we generalize a complexity-related variable, which is
sensitive to the changes in model width and the trade-off factors in
adversarial training. Moreover, intensive empirical evidence validates that
this variable highly correlates with the generalization gap of Cross-Entropy
loss between adversarial-trained and standard-trained models, especially during
the initial and final phases of the training process. Building upon this
observation, we propose a novel regularization framework, called Logit-Oriented
Adversarial Training (LOAT), which can mitigate the trade-off between
robustness and accuracy while imposing only a negligible increase in
computational overhead. Our extensive experiments demonstrate that the proposed
regularization strategy can boost the performance of the prevalent adversarial
training algorithms, including PGD-AT, TRADES, TRADES (LSE), MART, and DM-AT,
across various network architectures. Our code will be available at
https://github.com/TrustAI/LOAT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted to CVPR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Random-coupled Neural Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17512v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17512v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoran Liu, Mingzhe Liu, Peng Li, Jiahui Wu, Xin Jiang, Zhuo Zuo, Bingqi Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Improving the efficiency of current neural networks and modeling them in
biological neural systems have become popular research directions in recent
years. Pulse-coupled neural network (PCNN) is a well applicated model for
imitating the computation characteristics of the human brain in computer vision
and neural network fields. However, differences between the PCNN and biological
neural systems remain: limited neural connection, high computational cost, and
lack of stochastic property. In this study, random-coupled neural network
(RCNN) is proposed. It overcomes these difficulties in PCNN's neuromorphic
computing via a random inactivation process. This process randomly closes some
neural connections in the RCNN model, realized by the random inactivation
weight matrix of link input. This releases the computational burden of PCNN,
making it affordable to achieve vast neural connections. Furthermore, the image
and video processing mechanisms of RCNN are researched. It encodes constant
stimuli as periodic spike trains and periodic stimuli as chaotic spike trains,
the same as biological neural information encoding characteristics. Finally,
the RCNN is applicated to image segmentation, fusion, and pulse shape
discrimination subtasks. It is demonstrated to be robust, efficient, and highly
anti-noised, with outstanding performance in all applications mentioned above.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DS-AL: A Dual-Stream Analytic Learning for Exemplar-Free
  Class-Incremental Learning <span class="chip">AAAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17503v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17503v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huiping Zhuang, Run He, Kai Tong, Ziqian Zeng, Cen Chen, Zhiping Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Class-incremental learning (CIL) under an exemplar-free constraint has
presented a significant challenge. Existing methods adhering to this constraint
are prone to catastrophic forgetting, far more so than replay-based techniques
that retain access to past samples. In this paper, to solve the exemplar-free
CIL problem, we propose a Dual-Stream Analytic Learning (DS-AL) approach. The
DS-AL contains a main stream offering an analytical (i.e., closed-form) linear
solution, and a compensation stream improving the inherent under-fitting
limitation due to adopting linear mapping. The main stream redefines the CIL
problem into a Concatenated Recursive Least Squares (C-RLS) task, allowing an
equivalence between the CIL and its joint-learning counterpart. The
compensation stream is governed by a Dual-Activation Compensation (DAC) module.
This module re-activates the embedding with a different activation function
from the main stream one, and seeks fitting compensation by projecting the
embedding to the null space of the main stream's linear mapping. Empirical
results demonstrate that the DS-AL, despite being an exemplar-free technique,
delivers performance comparable with or better than that of replay-based
methods across various datasets, including CIFAR-100, ImageNet-100 and
ImageNet-Full. Additionally, the C-RLS' equivalent property allows the DS-AL to
execute CIL in a phase-invariant manner. This is evidenced by a
never-before-seen 500-phase CIL ImageNet task, which performs on a level
identical to a 5-phase one. Our codes are available at
https://github.com/ZHUANGHP/Analytic-continual-learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in AAAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SeNM-VAE: Semi-Supervised Noise Modeling with Hierarchical Variational
  Autoencoder 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17502v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17502v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dihan Zheng, Yihang Zou, Xiaowen Zhang, Chenglong Bao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The data bottleneck has emerged as a fundamental challenge in learning based
image restoration methods. Researchers have attempted to generate synthesized
training data using paired or unpaired samples to address this challenge. This
study proposes SeNM-VAE, a semi-supervised noise modeling method that leverages
both paired and unpaired datasets to generate realistic degraded data. Our
approach is based on modeling the conditional distribution of degraded and
clean images with a specially designed graphical model. Under the variational
inference framework, we develop an objective function for handling both paired
and unpaired data. We employ our method to generate paired training samples for
real-world image denoising and super-resolution tasks. Our approach excels in
the quality of synthetic degraded images compared to other unpaired and paired
noise modeling methods. Furthermore, our approach demonstrates remarkable
performance in downstream image restoration tasks, even with limited paired
data. With more paired data, our method achieves the best performance on the
SIDD dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sharing the Cost of Success: A Game for Evaluating and Learning
  Collaborative Multi-Agent Instruction Giving and Following Policies <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17497v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17497v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philipp Sadler, Sherzod Hakimov, David Schlangen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In collaborative goal-oriented settings, the participants are not only
interested in achieving a successful outcome, but do also implicitly negotiate
the effort they put into the interaction (by adapting to each other). In this
work, we propose a challenging interactive reference game that requires two
players to coordinate on vision and language observations. The learning signal
in this game is a score (given after playing) that takes into account the
achieved goal and the players' assumed efforts during the interaction. We show
that a standard Proximal Policy Optimization (PPO) setup achieves a high
success rate when bootstrapped with heuristic partner behaviors that implement
insights from the analysis of human-human interactions. And we find that a
pairing of neural partners indeed reduces the measured joint effort when
playing together repeatedly. However, we observe that in comparison to a
reasonable heuristic pairing there is still room for improvement -- which
invites further research in the direction of cost-sharing in collaborative
interactions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, Accepted at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dr.Hair: Reconstructing Scalp-Connected Hair Strands without
  <span class="highlight-title">Pre-train</span>ing via Differentiable Rendering of Line Segments <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17496v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17496v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yusuke Takimoto, Hikari Takehara, Hiroyuki Sato, Zihao Zhu, Bo Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the film and gaming industries, achieving a realistic hair appearance
typically involves the use of strands originating from the scalp. However,
reconstructing these strands from observed surface images of hair presents
significant challenges. The difficulty in acquiring Ground Truth (GT) data has
led state-of-the-art learning-based methods to rely on pre-training with
manually prepared synthetic CG data. This process is not only labor-intensive
and costly but also introduces complications due to the domain gap when
compared to real-world data. In this study, we propose an optimization-based
approach that eliminates the need for pre-training. Our method represents hair
strands as line segments growing from the scalp and optimizes them using a
novel differentiable rendering algorithm. To robustly optimize a substantial
number of slender explicit geometries, we introduce 3D orientation estimation
utilizing global optimization, strand initialization based on Laplace's
equation, and reparameterization that leverages geometric connectivity and
spatial proximity. Unlike existing optimization-based methods, our method is
capable of reconstructing internal hair flow in an absolute direction. Our
method exhibits robust and accurate inverse rendering, surpassing the quality
of existing methods and significantly improving processing speed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DiffGaze: A Diffusion Model for Continuous Gaze Sequence Generation on
  360° Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17477v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17477v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuhan Jiao, Yao Wang, Guanhua Zhang, Mihai Bâce, Zhiming Hu, Andreas Bulling
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present DiffGaze, a novel method for generating realistic and diverse
continuous human gaze sequences on 360{\deg} images based on a conditional
score-based denoising diffusion model. Generating human gaze on 360{\deg}
images is important for various human-computer interaction and computer
graphics applications, e.g. for creating large-scale eye tracking datasets or
for realistic animation of virtual humans. However, existing methods are
limited to predicting discrete fixation sequences or aggregated saliency maps,
thereby neglecting crucial parts of natural gaze behaviour. Our method uses
features extracted from 360{\deg} images as condition and uses two transformers
to model the temporal and spatial dependencies of continuous human gaze. We
evaluate DiffGaze on two 360{\deg} image benchmarks for gaze sequence
generation as well as scanpath prediction and saliency prediction. Our
evaluations show that DiffGaze outperforms state-of-the-art methods on all
tasks on both benchmarks. We also report a 21-participant user study showing
that our method generates gaze sequences that are indistinguishable from real
human sequences.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LaRE^2: Latent Reconstruction Error Based Method for Diffusion-Generated
  Image Detection <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17465v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17465v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunpeng Luo, Junlong Du, Ke Yan, Shouhong Ding
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The evolution of Diffusion Models has dramatically improved image generation
quality, making it increasingly difficult to differentiate between real and
generated images. This development, while impressive, also raises significant
privacy and security concerns. In response to this, we propose a novel Latent
REconstruction error guided feature REfinement method (LaRE^2) for detecting
the diffusion-generated images. We come up with the Latent Reconstruction Error
(LaRE), the first reconstruction-error based feature in the latent space for
generated image detection. LaRE surpasses existing methods in terms of feature
extraction efficiency while preserving crucial cues required to differentiate
between the real and the fake. To exploit LaRE, we propose an Error-Guided
feature REfinement module (EGRE), which can refine the image feature guided by
LaRE to enhance the discriminativeness of the feature. Our EGRE utilizes an
align-then-refine mechanism, which effectively refines the image feature for
generated-image detection from both spatial and channel perspectives. Extensive
experiments on the large-scale GenImage benchmark demonstrate the superiority
of our LaRE^2, which surpasses the best SoTA method by up to 11.9%/12.1%
average ACC/AP across 8 different image generators. LaRE also surpasses
existing methods in terms of feature extraction cost, delivering an impressive
speed enhancement of 8 times.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Building Bridges across Spatial and Temporal Resolutions:
  Reference-Based Super-Resolution via Change Priors and Conditional Diffusion
  Model <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17460v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17460v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Runmin Dong, Shuai Yuan, Bin Luo, Mengxuan Chen, Jinxiao Zhang, Lixian Zhang, Weijia Li, Juepeng Zheng, Haohuan Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reference-based super-resolution (RefSR) has the potential to build bridges
across spatial and temporal resolutions of remote sensing images. However,
existing RefSR methods are limited by the faithfulness of content
reconstruction and the effectiveness of texture transfer in large scaling
factors. Conditional diffusion models have opened up new opportunities for
generating realistic high-resolution images, but effectively utilizing
reference images within these models remains an area for further exploration.
Furthermore, content fidelity is difficult to guarantee in areas without
relevant reference information. To solve these issues, we propose a
change-aware diffusion model named Ref-Diff for RefSR, using the land cover
change priors to guide the denoising process explicitly. Specifically, we
inject the priors into the denoising model to improve the utilization of
reference information in unchanged areas and regulate the reconstruction of
semantically relevant content in changed areas. With this powerful guidance, we
decouple the semantics-guided denoising and reference texture-guided denoising
processes to improve the model performance. Extensive experiments demonstrate
the superior effectiveness and robustness of the proposed method compared with
state-of-the-art RefSR methods in both quantitative and qualitative
evaluations. The code and data are available at
https://github.com/dongrunmin/RefDiff.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Chain of Compression: A Systematic Approach to Combinationally Compress
  Convolutional Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17447v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17447v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingtao Shen, Minqing Sun, Jie Zhao, An Zou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Convolutional neural networks (CNNs) have achieved significant popularity,
but their computational and memory intensity poses challenges for
resource-constrained computing systems, particularly with the prerequisite of
real-time performance. To release this burden, model compression has become an
important research focus. Many approaches like quantization, pruning, early
exit, and knowledge distillation have demonstrated the effect of reducing
redundancy in neural networks. Upon closer examination, it becomes apparent
that each approach capitalizes on its unique features to compress the neural
network, and they can also exhibit complementary behavior when combined. To
explore the interactions and reap the benefits from the complementary features,
we propose the Chain of Compression, which works on the combinational sequence
to apply these common techniques to compress the neural network. Validated on
the image-based regression and classification networks across different data
sets, our proposed Chain of Compression can significantly compress the
computation cost by 100-1000 times with ignorable accuracy loss compared with
the baseline model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 15 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Integrating Mamba Sequence Model and Hierarchical Upsampling Network for
  Accurate Semantic Segmentation of Multiple Sclerosis Legion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17432v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17432v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kazi Shahriar Sanjid, Md. Tanzim Hossain, Md. Shakib Shahariar Junayed, Dr. Mohammad Monir Uddin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Integrating components from convolutional neural networks and state space
models in medical image segmentation presents a compelling approach to enhance
accuracy and efficiency. We introduce Mamba HUNet, a novel architecture
tailored for robust and efficient segmentation tasks. Leveraging strengths from
Mamba UNet and the lighter version of Hierarchical Upsampling Network (HUNet),
Mamba HUNet combines convolutional neural networks local feature extraction
power with state space models long range dependency modeling capabilities. We
first converted HUNet into a lighter version, maintaining performance parity
and then integrated this lighter HUNet into Mamba HUNet, further enhancing its
efficiency. The architecture partitions input grayscale images into patches,
transforming them into 1D sequences for processing efficiency akin to Vision
Transformers and Mamba models. Through Visual State Space blocks and patch
merging layers, hierarchical features are extracted while preserving spatial
information. Experimental results on publicly available Magnetic Resonance
Imaging scans, notably in Multiple Sclerosis lesion segmentation, demonstrate
Mamba HUNet's effectiveness across diverse segmentation tasks. The model's
robustness and flexibility underscore its potential in handling complex
anatomical structures. These findings establish Mamba HUNet as a promising
solution in advancing medical image segmentation, with implications for
improving clinical decision making processes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Test-time Adaptation Meets Image Enhancement: Improving Accuracy via
  Uncertainty-aware Logit Switching <span class="chip">IJCNN2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17423v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17423v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shohei Enomoto, Naoya Hasegawa, Kazuki Adachi, Taku Sasaki, Shin'ya Yamaguchi, Satoshi Suzuki, Takeharu Eda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks have achieved remarkable success in a variety of
computer vision applications. However, there is a problem of degrading accuracy
when the data distribution shifts between training and testing. As a solution
of this problem, Test-time Adaptation~(TTA) has been well studied because of
its practicality. Although TTA methods increase accuracy under distribution
shift by updating the model at test time, using high-uncertainty predictions is
known to degrade accuracy. Since the input image is the root of the
distribution shift, we incorporate a new perspective on enhancing the input
image into TTA methods to reduce the prediction's uncertainty. We hypothesize
that enhancing the input image reduces prediction's uncertainty and increase
the accuracy of TTA methods. On the basis of our hypothesis, we propose a novel
method: Test-time Enhancer and Classifier Adaptation~(TECA). In TECA, the
classification model is combined with the image enhancement model that
transforms input images into recognition-friendly ones, and these models are
updated by existing TTA methods. Furthermore, we found that the prediction from
the enhanced image does not always have lower uncertainty than the prediction
from the original image. Thus, we propose logit switching, which compares the
uncertainty measure of these predictions and outputs the lower one. In our
experiments, we evaluate TECA with various TTA methods and show that TECA
reduces prediction's uncertainty and increases accuracy of TTA methods despite
having no hyperparameters and little parameter overhead.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IJCNN2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ InterHandGen: Two-Hand Interaction Generation via Cascaded Reverse
  Diffusion <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17422v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17422v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jihyun Lee, Shunsuke Saito, Giljoo Nam, Minhyuk Sung, Tae-Kyun Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present InterHandGen, a novel framework that learns the generative prior
of two-hand interaction. Sampling from our model yields plausible and diverse
two-hand shapes in close interaction with or without an object. Our prior can
be incorporated into any optimization or learning methods to reduce ambiguity
in an ill-posed setup. Our key observation is that directly modeling the joint
distribution of multiple instances imposes high learning complexity due to its
combinatorial nature. Thus, we propose to decompose the modeling of joint
distribution into the modeling of factored unconditional and conditional single
instance distribution. In particular, we introduce a diffusion model that
learns the single-hand distribution unconditional and conditional to another
hand via conditioning dropout. For sampling, we combine anti-penetration and
classifier-free guidance to enable plausible generation. Furthermore, we
establish the rigorous evaluation protocol of two-hand synthesis, where our
method significantly outperforms baseline generative models in terms of
plausibility and diversity. We also demonstrate that our diffusion prior can
boost the performance of two-hand reconstruction from monocular in-the-wild
images, achieving new state-of-the-art accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2024, project page:
  https://jyunlee.github.io/projects/interhandgen/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning to Visually Localize Sound Sources from Mixtures without Prior
  Source Knowledge <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17420v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17420v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongjin Kim, Sung Jin Um, Sangmin Lee, Jung Uk Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The goal of the multi-sound source localization task is to localize sound
sources from the mixture individually. While recent multi-sound source
localization methods have shown improved performance, they face challenges due
to their reliance on prior information about the number of objects to be
separated. In this paper, to overcome this limitation, we present a novel
multi-sound source localization method that can perform localization without
prior knowledge of the number of sound sources. To achieve this goal, we
propose an iterative object identification (IOI) module, which can recognize
sound-making objects in an iterative manner. After finding the regions of
sound-making objects, we devise object similarity-aware clustering (OSC) loss
to guide the IOI module to effectively combine regions of the same object but
also distinguish between different objects and backgrounds. It enables our
method to perform accurate localization of sound-making objects without any
prior knowledge. Extensive experimental results on the MUSIC and VGGSound
benchmarks show the significant performance improvements of the proposed method
over the existing methods for both single and multi-source. Our code is
available at: https://github.com/VisualAIKHU/NoPrior_MultiSSL
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural Clustering based Visual Representation Learning <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17409v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17409v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guikun Chen, Xia Li, Yi Yang, Wenguan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate a fundamental aspect of machine vision: the measurement of
features, by revisiting clustering, one of the most classic approaches in
machine learning and data analysis. Existing visual feature extractors,
including ConvNets, ViTs, and MLPs, represent an image as rectangular regions.
Though prevalent, such a grid-style paradigm is built upon engineering practice
and lacks explicit modeling of data distribution. In this work, we propose
feature extraction with clustering (FEC), a conceptually elegant yet
surprisingly ad-hoc interpretable neural clustering framework, which views
feature extraction as a process of selecting representatives from data and thus
automatically captures the underlying data distribution. Given an image, FEC
alternates between grouping pixels into individual clusters to abstract
representatives and updating the deep features of pixels with current
representatives. Such an iterative working mechanism is implemented in the form
of several neural layers and the final representatives can be used for
downstream tasks. The cluster assignments across layers, which can be viewed
and inspected by humans, make the forward process of FEC fully transparent and
empower it with promising ad-hoc interpretability. Extensive experiments on
various visual recognition models and tasks verify the effectiveness,
generality, and interpretability of FEC. We expect this work will provoke a
rethink of the current de facto grid-style paradigm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024. Code: https://github.com/guikunchen/FEC/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SSF3D: Strict Semi-Supervised 3D Object Detection with Switching Filter 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17390v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17390v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Songbur Wong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  SSF3D modified the semi-supervised 3D object detection (SS3DOD) framework,
which designed specifically for point cloud data. Leveraging the
characteristics of non-coincidence and weak correlation of target objects in
point cloud, we adopt a strategy of retaining only the truth-determining pseudo
labels and trimming the other fuzzy labels with points, instead of pursuing a
balance between the quantity and quality of pseudo labels. Besides, we notice
that changing the filter will make the model meet different distributed
targets, which is beneficial to break the training bottleneck. Two mechanism
are introduced to achieve above ideas: strict threshold and filter switching.
The experiments are conducted to analyze the effectiveness of above approaches
and their impact on the overall performance of the system. Evaluating on the
KITTI dataset, SSF3D exhibits superior performance compared to the current
state-of-the-art methods. The code will be released here.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Decoupled Pseudo-labeling for Semi-Supervised Monocular 3D Object
  Detection <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17387v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17387v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiacheng Zhang, Jiaming Li, Xiangru Lin, Wei Zhang, Xiao Tan, Junyu Han, Errui Ding, Jingdong Wang, Guanbin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We delve into pseudo-labeling for semi-supervised monocular 3D object
detection (SSM3OD) and discover two primary issues: a misalignment between the
prediction quality of 3D and 2D attributes and the tendency of depth
supervision derived from pseudo-labels to be noisy, leading to significant
optimization conflicts with other reliable forms of supervision. We introduce a
novel decoupled pseudo-labeling (DPL) approach for SSM3OD. Our approach
features a Decoupled Pseudo-label Generation (DPG) module, designed to
efficiently generate pseudo-labels by separately processing 2D and 3D
attributes. This module incorporates a unique homography-based method for
identifying dependable pseudo-labels in BEV space, specifically for 3D
attributes. Additionally, we present a DepthGradient Projection (DGP) module to
mitigate optimization conflicts caused by noisy depth supervision of
pseudo-labels, effectively decoupling the depth gradient and removing
conflicting gradients. This dual decoupling strategy-at both the pseudo-label
generation and gradient levels-significantly improves the utilization of
pseudo-labels in SSM3OD. Our comprehensive experiments on the KITTI benchmark
demonstrate the superiority of our method over existing approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in CVPR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self-Rectifying Diffusion Sampling with Perturbed-Attention Guidance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17377v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17377v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Donghoon Ahn, Hyoungwon Cho, Jaewon Min, Wooseok Jang, Jungwoo Kim, SeonHwa Kim, Hyun Hee Park, Kyong Hwan Jin, Seungryong Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies have demonstrated that diffusion models are capable of
generating high-quality samples, but their quality heavily depends on sampling
guidance techniques, such as classifier guidance (CG) and classifier-free
guidance (CFG). These techniques are often not applicable in unconditional
generation or in various downstream tasks such as image restoration. In this
paper, we propose a novel sampling guidance, called Perturbed-Attention
Guidance (PAG), which improves diffusion sample quality across both
unconditional and conditional settings, achieving this without requiring
additional training or the integration of external modules. PAG is designed to
progressively enhance the structure of samples throughout the denoising
process. It involves generating intermediate samples with degraded structure by
substituting selected self-attention maps in diffusion U-Net with an identity
matrix, by considering the self-attention mechanisms' ability to capture
structural information, and guiding the denoising process away from these
degraded samples. In both ADM and Stable Diffusion, PAG surprisingly improves
sample quality in conditional and even unconditional scenarios. Moreover, PAG
significantly improves the baseline performance in various downstream tasks
where existing guidances such as CG or CFG cannot be fully utilized, including
ControlNet with empty prompts and image restoration such as inpainting and
deblurring.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page is available at
  https://ku-cvlab.github.io/Perturbed-Attention-Guidance</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DiVa-360: The Dynamic Visual <span class="highlight-title">Dataset</span> for Immersive Neural Fields 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.16897v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.16897v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cheng-You Lu, Peisen Zhou, Angela Xing, Chandradeep Pokhariya, Arnab Dey, Ishaan Shah, Rugved Mavidipalli, Dylan Hu, Andrew Comport, Kefan Chen, Srinath Sridhar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advances in neural fields are enabling high-fidelity capture of the shape and
appearance of dynamic 3D scenes. However, their capabilities lag behind those
offered by conventional representations such as 2D videos because of
algorithmic challenges and the lack of large-scale multi-view real-world
datasets. We address the dataset limitation with DiVa-360, a real-world 360
dynamic visual dataset that contains synchronized high-resolution and
long-duration multi-view video sequences of table-scale scenes captured using a
customized low-cost system with 53 cameras. It contains 21 object-centric
sequences categorized by different motion types, 25 intricate hand-object
interaction sequences, and 8 long-duration sequences for a total of 17.4 M
image frames. In addition, we provide foreground-background segmentation masks,
synchronized audio, and text descriptions. We benchmark the state-of-the-art
dynamic neural field methods on DiVa-360 and provide insights about existing
methods and future challenges on long-duration neural field capture.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HoloVIC: Large-scale <span class="highlight-title">Dataset</span> and Benchmark for Multi-Sensor Holographic
  Intersection and Vehicle-Infrastructure Cooperative <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.02640v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.02640v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cong Ma, Lei Qiao, Chengkai Zhu, Kai Liu, Zelong Kong, Qing Li, Xueqi Zhou, Yuheng Kan, Wei Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vehicle-to-everything (V2X) is a popular topic in the field of Autonomous
Driving in recent years. Vehicle-infrastructure cooperation (VIC) becomes one
of the important research area. Due to the complexity of traffic conditions
such as blind spots and occlusion, it greatly limits the perception
capabilities of single-view roadside sensing systems. To further enhance the
accuracy of roadside perception and provide better information to the vehicle
side, in this paper, we constructed holographic intersections with various
layouts to build a large-scale multi-sensor holographic vehicle-infrastructure
cooperation dataset, called HoloVIC. Our dataset includes 3 different types of
sensors (Camera, Lidar, Fisheye) and employs 4 sensor-layouts based on the
different intersections. Each intersection is equipped with 6-18 sensors to
capture synchronous data. While autonomous vehicles pass through these
intersections for collecting VIC data. HoloVIC contains in total on 100k+
synchronous frames from different sensors. Additionally, we annotated 3D
bounding boxes based on Camera, Fisheye, and Lidar. We also associate the IDs
of the same objects across different devices and consecutive frames in
sequence. Based on HoloVIC, we formulated four tasks to facilitate the
development of related research. We also provide benchmarks for these tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accept to CVPR 2024, Benchmark Website: https://holovic.net</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TRIPS: Trilinear Point Splatting for Real-Time Radiance Field Rendering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.06003v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.06003v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linus Franke, Darius Rückert, Laura Fink, Marc Stamminger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Point-based radiance field rendering has demonstrated impressive results for
novel view synthesis, offering a compelling blend of rendering quality and
computational efficiency. However, also latest approaches in this domain are
not without their shortcomings. 3D Gaussian Splatting [Kerbl and Kopanas et al.
2023] struggles when tasked with rendering highly detailed scenes, due to
blurring and cloudy artifacts. On the other hand, ADOP [R\"uckert et al. 2022]
can accommodate crisper images, but the neural reconstruction network decreases
performance, it grapples with temporal instability and it is unable to
effectively address large gaps in the point cloud.
  In this paper, we present TRIPS (Trilinear Point Splatting), an approach that
combines ideas from both Gaussian Splatting and ADOP. The fundamental concept
behind our novel technique involves rasterizing points into a screen-space
image pyramid, with the selection of the pyramid layer determined by the
projected point size. This approach allows rendering arbitrarily large points
using a single trilinear write. A lightweight neural network is then used to
reconstruct a hole-free image including detail beyond splat resolution.
Importantly, our render pipeline is entirely differentiable, allowing for
automatic optimization of both point sizes and positions.
  Our evaluation demonstrate that TRIPS surpasses existing state-of-the-art
methods in terms of rendering quality while maintaining a real-time frame rate
of 60 frames per second on readily available hardware. This performance extends
to challenging scenarios, such as scenes featuring intricate geometry,
expansive landscapes, and auto-exposed footage.
  The project page is located at: https://lfranke.github.io/trips/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Semi-Supervised Crowd Counting from Unlabeled Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2108.13969v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2108.13969v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoran Duan, Fan Wan, Rui Sun, Zeyu Wang, Varun Ojha, Yu Guan, Hubert P. H. Shum, Bingzhang Hu, Yang Long
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic Crowd behavior analysis can be applied to effectively help the
daily transportation statistics and planning, which helps the smart city
construction. As one of the most important keys, crowd counting has drawn
increasing attention. Recent works achieved promising performance but relied on
the supervised paradigm with expensive crowd annotations. To alleviate the
annotation cost in real-world transportation scenarios, in this work we
proposed a semi-supervised learning framework $S^{4}\textit{Crowd}$, which can
leverage both unlabeled/labeled data for robust crowd counting. In the
unsupervised pathway, two \textit{self-supervised losses} were proposed to
simulate the crowd variations such as scale, illumination, based on which
supervised information pseudo labels were generated and gradually refined. We
also proposed a crowd-driven recurrent unit \textit{Gated-Crowd-Recurrent-Unit
(GCRU)}, which can preserve discriminant crowd information by extracting
second-order statistics, yielding pseudo labels with improved quality. A joint
loss including both unsupervised/supervised information was proposed, and a
dynamic weighting strategy was employed to balance the importance of the
unsupervised loss and supervised loss at different training stages. We
conducted extensive experiments on four popular crowd counting datasets in
semi-supervised settings. Experimental results supported the effectiveness of
each proposed component in our $S^{4}$Crowd framework. Our method achieved
competitive performance in semi-supervised learning approaches on these crowd
counting datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient <span class="highlight-title">Pre-train</span>ing for Localized Instruction Generation of Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.15964v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.15964v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anil Batra, Davide Moltisanti, Laura Sevilla-Lara, Marcus Rohrbach, Frank Keller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Procedural videos show step-by-step demonstrations of tasks like recipe
preparation. Understanding such videos is challenging, involving the precise
localization of steps and the generation of textual instructions. Manually
annotating steps and writing instructions is costly, which limits the size of
current datasets and hinders effective learning. Leveraging large but noisy
video-transcript datasets for pre-training can boost performance, but demands
significant computational resources. Furthermore, transcripts contain
irrelevant content and exhibit style variation compared to instructions written
by human annotators. To mitigate both issues, we propose a technique,
Sieve-&-Swap, to automatically curate a smaller dataset: (i) Sieve filters
irrelevant transcripts and (ii) Swap enhances the quality of the text
instruction by automatically replacing the transcripts with human-written
instructions from a text-only recipe dataset. The curated dataset, three orders
of magnitude smaller than current web-scale datasets, enables efficient
training of large-scale models with competitive performance. We complement our
Sieve-\&-Swap approach with a Procedure Transformer (ProcX) for end-to-end step
localization and instruction generation for procedural videos. When this model
is pre-trained on our curated dataset, it achieves state-of-the-art performance
in zero-shot and finetuning settings on YouCook2 and Tasty, while using a
fraction of the computational resources.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This version has some missing experiments and elaborative technical
  details</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SimLVSeg: Simplifying Left Ventricular Segmentation in 2D+Time
  Echocardiograms with Self- and Weakly-Supervised Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.00454v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.00454v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fadillah Maani, Asim Ukaye, Nada Saadi, Numan Saeed, Mohammad Yaqub
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Echocardiography has become an indispensable clinical imaging modality for
general heart health assessment. From calculating biomarkers such as ejection
fraction to the probability of a patient's heart failure, accurate segmentation
of the heart structures allows doctors to assess the heart's condition and
devise treatments with greater precision and accuracy. However, achieving
accurate and reliable left ventricle segmentation is time-consuming and
challenging due to different reasons. Hence, clinicians often rely on
segmenting the left ventricular (LV) in two specific echocardiogram frames to
make a diagnosis. This limited coverage in manual LV segmentation poses a
challenge for developing automatic LV segmentation with high temporal
consistency, as the resulting dataset is typically annotated sparsely. In
response to this challenge, this work introduces SimLVSeg, a novel paradigm
that enables video-based networks for consistent LV segmentation from sparsely
annotated echocardiogram videos. SimLVSeg consists of self-supervised
pre-training with temporal masking, followed by weakly supervised learning
tailored for LV segmentation from sparse annotations. We demonstrate how
SimLVSeg outperforms the state-of-the-art solutions by achieving a 93.32%
(95%CI 93.21-93.43%) dice score on the largest 2D+time echocardiography dataset
(EchoNet-Dynamic) while being more efficient. SimLVSeg is compatible with two
types of video segmentation networks: 2D super image and 3D segmentation. To
show the effectiveness of our approach, we provide extensive ablation studies,
including pre-training settings and various deep learning backbones. We further
conduct an out-of-distribution test to showcase SimLVSeg's generalizability on
unseen distribution (CAMUS dataset). The code is publicly available at
https://github.com/fadamsyah/SimLVSeg.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HIMap: HybrId Representation Learning for End-to-end Vectorized HD Map
  Construction <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08639v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08639v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Zhou, Hui Zhang, Jiaqian Yu, Yifan Yang, Sangil Jung, Seung-In Park, ByungIn Yoo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vectorized High-Definition (HD) map construction requires predictions of the
category and point coordinates of map elements (e.g. road boundary, lane
divider, pedestrian crossing, etc.). State-of-the-art methods are mainly based
on point-level representation learning for regressing accurate point
coordinates. However, this pipeline has limitations in obtaining element-level
information and handling element-level failures, e.g. erroneous element shape
or entanglement between elements. To tackle the above issues, we propose a
simple yet effective HybrId framework named HIMap to sufficiently learn and
interact both point-level and element-level information. Concretely, we
introduce a hybrid representation called HIQuery to represent all map elements,
and propose a point-element interactor to interactively extract and encode the
hybrid information of elements, e.g. point position and element shape, into the
HIQuery. Additionally, we present a point-element consistency constraint to
enhance the consistency between the point-level and element-level information.
Finally, the output point-element integrated HIQuery can be directly converted
into map elements' class, point coordinates, and mask. We conduct extensive
experiments and consistently outperform previous methods on both nuScenes and
Argoverse2 datasets. Notably, our method achieves $77.8$ mAP on the nuScenes
dataset, remarkably superior to previous SOTAs by $8.3$ mAP at least.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploiting Semantic Reconstruction to Mitigate Hallucinations in
  Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16167v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16167v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minchan Kim, Minyeong Kim, Junik Bae, Suhwan Choi, Sungkyung Kim, Buru Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hallucinations in vision-language models pose a significant challenge to
their reliability, particularly in the generation of long captions. Current
methods fall short of accurately identifying and mitigating these
hallucinations. To address this issue, we introduce ESREAL, a novel
unsupervised learning framework designed to suppress the generation of
hallucinations through accurate localization and penalization of hallucinated
tokens. Initially, ESREAL creates a reconstructed image based on the generated
caption and aligns its corresponding regions with those of the original image.
This semantic reconstruction aids in identifying both the presence and type of
token-level hallucinations within the generated caption. Subsequently, ESREAL
computes token-level hallucination scores by assessing the semantic similarity
of aligned regions based on the type of hallucination. Finally, ESREAL employs
a proximal policy optimization algorithm, where it selectively penalizes
hallucinated tokens according to their token-level hallucination scores. Our
framework notably reduces hallucinations in LLaVA, InstructBLIP, and mPLUG-Owl2
by 32.81%, 27.08%, and 7.46% on the CHAIR metric. This improvement is achieved
solely through signals derived from the image itself, without the need for any
image-text pairs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pushing Auto-regressive Models for 3D Shape Generation at Capacity and
  Scalability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.12225v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.12225v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuelin Qian, Yu Wang, Simian Luo, Yinda Zhang, Ying Tai, Zhenyu Zhang, Chengjie Wang, Xiangyang Xue, Bo Zhao, Tiejun Huang, Yunsheng Wu, Yanwei Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Auto-regressive models have achieved impressive results in 2D image
generation by modeling joint distributions in grid space. In this paper, we
extend auto-regressive models to 3D domains, and seek a stronger ability of 3D
shape generation by improving auto-regressive models at capacity and
scalability simultaneously. Firstly, we leverage an ensemble of publicly
available 3D datasets to facilitate the training of large-scale models. It
consists of a comprehensive collection of approximately 900,000 objects, with
multiple properties of meshes, points, voxels, rendered images, and text
captions. This diverse labeled dataset, termed Objaverse-Mix, empowers our
model to learn from a wide range of object variations. However, directly
applying 3D auto-regression encounters critical challenges of high
computational demands on volumetric grids and ambiguous auto-regressive order
along grid dimensions, resulting in inferior quality of 3D shapes. To this end,
we then present a novel framework Argus3D in terms of capacity. Concretely, our
approach introduces discrete representation learning based on a latent vector
instead of volumetric grids, which not only reduces computational costs but
also preserves essential geometric details by learning the joint distributions
in a more tractable order. The capacity of conditional generation can thus be
realized by simply concatenating various conditioning inputs to the latent
vector, such as point clouds, categories, images, and texts. In addition,
thanks to the simplicity of our model architecture, we naturally scale up our
approach to a larger model with an impressive 3.6 billion parameters, further
enhancing the quality of versatile 3D generation. Extensive experiments on four
generation tasks demonstrate that Argus3D can synthesize diverse and faithful
shapes across multiple categories, achieving remarkable performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://argus-3d.github.io/ . Datasets:
  https://huggingface.co/datasets/BAAI/Objaverse-MIX. arXiv admin note:
  substantial text overlap with arXiv:2303.14700</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ReMoS: 3D Motion-Conditioned Reaction Synthesis for Two-Person
  Interactions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.17057v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.17057v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anindita Ghosh, Rishabh Dabral, Vladislav Golyanik, Christian Theobalt, Philipp Slusallek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current approaches for 3D human motion synthesis generate high-quality
animations of digital humans performing a wide variety of actions and gestures.
However, a notable technological gap exists in addressing the complex dynamics
of multi-human interactions within this paradigm. In this work, we present
ReMoS, a denoising diffusion-based model that synthesizes full-body reactive
motion of a person in a two-person interaction scenario. Assuming the motion of
one person is given, we employ a combined spatio-temporal cross-attention
mechanism to synthesize the reactive body and hand motion of the second person,
thereby completing the interactions between the two. We demonstrate ReMoS
across challenging two-person scenarios such as pair-dancing, Ninjutsu,
kickboxing, and acrobatics, where one person's movements have complex and
diverse influences on the other. We also contribute the ReMoCap dataset for
two-person interactions containing full-body and finger motions. We evaluate
ReMoS through multiple quantitative metrics, qualitative visualizations, and a
user study, and also indicate usability in interactive motion editing
applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 7 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Med<span class="highlight-title">Prompt</span>X: Grounded Multimodal <span class="highlight-title">Prompt</span>ing for Chest X-ray Diagnosis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15585v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15585v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mai A. Shaaban, Adnan Khan, Mohammad Yaqub
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chest X-ray images are commonly used for predicting acute and chronic
cardiopulmonary conditions, but efforts to integrate them with structured
clinical data face challenges due to incomplete electronic health records
(EHR). This paper introduces \textbf{MedPromptX}, the first model to integrate
multimodal large language models (MLLMs), few-shot prompting (FP) and visual
grounding (VG) to combine imagery with EHR data for chest X-ray diagnosis. A
pre-trained MLLM is utilized to complement the missing EHR information,
providing a comprehensive understanding of patients' medical history.
Additionally, FP reduces the necessity for extensive training of MLLMs while
effectively tackling the issue of hallucination. Nevertheless, the process of
determining the optimal number of few-shot examples and selecting high-quality
candidates can be burdensome, yet it profoundly influences model performance.
Hence, we propose a new technique that dynamically refines few-shot data for
real-time adjustment to new patient scenarios. Moreover, VG aids in focusing
the model's attention on relevant regions of interest in X-ray images,
enhancing the identification of abnormalities. We release MedPromptX-VQA, a new
in-context visual question answering dataset encompassing interleaved image and
EHR data derived from MIMIC-IV and MIMIC-CXR databases. Results demonstrate the
SOTA performance of MedPromptX, achieving an 11% improvement in F1-score
compared to the baselines. Code and data are available at
https://github.com/BioMedIA-MBZUAI/MedPromptX
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Text-Guided Variational Image Generation for Industrial Anomaly
  Detection and Segmentation <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06247v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06247v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingyu Lee, Jongwon Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a text-guided variational image generation method to address the
challenge of getting clean data for anomaly detection in industrial
manufacturing. Our method utilizes text information about the target object,
learned from extensive text library documents, to generate non-defective data
images resembling the input image. The proposed framework ensures that the
generated non-defective images align with anticipated distributions derived
from textual and image-based knowledge, ensuring stability and generality.
Experimental results demonstrate the effectiveness of our approach, surpassing
previous methods even with limited non-defective data. Our approach is
validated through generalization tests across four baseline models and three
distinct datasets. We present an additional analysis to enhance the
effectiveness of anomaly detection models by utilizing the generated images.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, Accepted to CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Identity-aware Dual-constraint Network for Cloth-Changing Person
  Re-identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08270v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08270v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peini Guo, Mengyuan Liu, Hong Liu, Ruijia Fan, Guoquan Wang, Bin He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cloth-Changing Person Re-Identification (CC-ReID) aims to accurately identify
the target person in more realistic surveillance scenarios, where pedestrians
usually change their clothing. Despite great progress, limited cloth-changing
training samples in existing CC-ReID datasets still prevent the model from
adequately learning cloth-irrelevant features. In addition, due to the absence
of explicit supervision to keep the model constantly focused on
cloth-irrelevant areas, existing methods are still hampered by the disruption
of clothing variations. To solve the above issues, we propose an Identity-aware
Dual-constraint Network (IDNet) for the CC-ReID task. Specifically, to help the
model extract cloth-irrelevant clues, we propose a Clothes Diversity
Augmentation (CDA), which generates more realistic cloth-changing samples by
enriching the clothing color while preserving the texture. In addition, a
Multi-scale Constraint Block (MCB) is designed, which extracts fine-grained
identity-related features and effectively transfers cloth-irrelevant knowledge.
Moreover, a Counterfactual-guided Attention Module (CAM) is presented, which
learns cloth-irrelevant features from channel and space dimensions and utilizes
the counterfactual intervention for supervising the attention map to highlight
identity-related regions. Finally, a Semantic Alignment Constraint (SAC) is
designed to facilitate high-level semantic feature interaction. Comprehensive
experiments on four CC-ReID datasets indicate that our method outperforms prior
state-of-the-art approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unveiling the Pitfalls of Knowledge Editing for Large Language Models <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02129v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02129v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhoubo Li, Ningyu Zhang, Yunzhi Yao, Mengru Wang, Xi Chen, Huajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the cost associated with fine-tuning Large Language Models (LLMs)
continues to rise, recent research efforts have pivoted towards developing
methodologies to edit implicit knowledge embedded within LLMs. Yet, there's
still a dark cloud lingering overhead -- will knowledge editing trigger
butterfly effect? since it is still unclear whether knowledge editing might
introduce side effects that pose potential risks or not. This paper pioneers
the investigation into the potential pitfalls associated with knowledge editing
for LLMs. To achieve this, we introduce new benchmark datasets and propose
innovative evaluation metrics. Our results underline two pivotal concerns: (1)
Knowledge Conflict: Editing groups of facts that logically clash can magnify
the inherent inconsistencies in LLMs-a facet neglected by previous methods. (2)
Knowledge Distortion: Altering parameters with the aim of editing factual
knowledge can irrevocably warp the innate knowledge structure of LLMs.
Experimental results vividly demonstrate that knowledge editing might
inadvertently cast a shadow of unintended consequences on LLMs, which warrant
attention and efforts for future works. Code and data are available at
https://github.com/zjunlp/PitfallsKnowledgeEditing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generative 3D Part Assembly via Part-Whole-Hierarchy Message Passing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17464v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17464v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bi'an Du, Xiang Gao, Wei Hu, Renjie Liao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative 3D part assembly involves understanding part relationships and
predicting their 6-DoF poses for assembling a realistic 3D shape. Prior work
often focus on the geometry of individual parts, neglecting part-whole
hierarchies of objects. Leveraging two key observations: 1) super-part poses
provide strong hints about part poses, and 2) predicting super-part poses is
easier due to fewer superparts, we propose a part-whole-hierarchy message
passing network for efficient 3D part assembly. We first introduce super-parts
by grouping geometrically similar parts without any semantic labels. Then we
employ a part-whole hierarchical encoder, wherein a super-part encoder predicts
latent super-part poses based on input parts. Subsequently, we transform the
point cloud using the latent poses, feeding it to the part encoder for
aggregating super-part information and reasoning about part relationships to
predict all part poses. In training, only ground-truth part poses are required.
During inference, the predicted latent poses of super-parts enhance
interpretability. Experimental results on the PartNet dataset show that our
method achieves state-of-the-art performance in part and connectivity accuracy
and enables an interpretable hierarchical part assembly.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ InNeRF360: Text-Guided 3D-Consistent Object Inpainting on 360-degree
  Neural Radiance Fields <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.15094v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.15094v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongqing Wang, Tong Zhang, Alaa Abboud, Sabine Süsstrunk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose InNeRF360, an automatic system that accurately removes
text-specified objects from 360-degree Neural Radiance Fields (NeRF). The
challenge is to effectively remove objects while inpainting perceptually
consistent content for the missing regions, which is particularly demanding for
existing NeRF models due to their implicit volumetric representation. Moreover,
unbounded scenes are more prone to floater artifacts in the inpainted region
than frontal-facing scenes, as the change of object appearance and background
across views is more sensitive to inaccurate segmentations and inconsistent
inpainting. With a trained NeRF and a text description, our method efficiently
removes specified objects and inpaints visually consistent content without
artifacts. We apply depth-space warping to enforce consistency across multiview
text-encoded segmentations, and then refine the inpainted NeRF model using
perceptual priors and 3D diffusion-based geometric priors to ensure visual
plausibility. Through extensive experiments in segmentation and inpainting on
360-degree and frontal-facing NeRFs, we show that our approach is effective and
enhances NeRF's editability. Project page: https://ivrl.github.io/InNeRF360.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Passive Non-Line-of-Sight Imaging with Light Transport Modulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.16014v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.16014v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiarui Zhang, Ruixu Geng, Xiaolong Du, Yan Chen, Houqiang Li, Yang Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Passive non-line-of-sight (NLOS) imaging has witnessed rapid development in
recent years, due to its ability to image objects that are out of sight. The
light transport condition plays an important role in this task since changing
the conditions will lead to different imaging models. Existing learning-based
NLOS methods usually train independent models for different light transport
conditions, which is computationally inefficient and impairs the practicality
of the models. In this work, we propose NLOS-LTM, a novel passive NLOS imaging
method that effectively handles multiple light transport conditions with a
single network. We achieve this by inferring a latent light transport
representation from the projection image and using this representation to
modulate the network that reconstructs the hidden image from the projection
image. We train a light transport encoder together with a vector quantizer to
obtain the light transport representation. To further regulate this
representation, we jointly learn both the reconstruction network and the
reprojection network during training. A set of light transport modulation
blocks is used to modulate the two jointly trained networks in a multi-scale
way. Extensive experiments on a large-scale passive NLOS dataset demonstrate
the superiority of the proposed method. The code is available at
https://github.com/JerryOctopus/NLOS-LTM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ViT-Lens: Towards Omni-modal Representations <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.16081v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.16081v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weixian Lei, Yixiao Ge, Kun Yi, Jianfeng Zhang, Difei Gao, Dylan Sun, Yuying Ge, Ying Shan, Mike Zheng Shou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aiming to advance AI agents, large foundation models significantly improve
reasoning and instruction execution, yet the current focus on vision and
language neglects the potential of perceiving diverse modalities in open-world
environments. However, the success of data-driven vision and language models is
costly or even infeasible to be reproduced for rare modalities. In this paper,
we present ViT-Lens-2 that facilitates efficient omni-modal representation
learning by perceiving novel modalities with a pretrained ViT and aligning them
to a pre-defined space. Specifically, the modality-specific lens is tuned to
project any-modal signals to an intermediate embedding space, which are then
processed by a strong ViT with pre-trained visual knowledge. The encoded
representations are optimized toward aligning with the modal-independent space,
pre-defined by off-the-shelf foundation models. ViT-Lens-2 provides a unified
solution for representation learning of increasing modalities with two
appealing advantages: (i) Unlocking the great potential of pretrained ViTs to
novel modalities effectively with efficient data regime; (ii) Enabling emergent
downstream capabilities through modality alignment and shared ViT parameters.
We tailor ViT-Lens-2 to learn representations for 3D point cloud, depth, audio,
tactile and EEG, and set new state-of-the-art results across various
understanding tasks, such as zero-shot classification. By seamlessly
integrating ViT-Lens-2 into Multimodal Foundation Models, we enable
Any-modality to Text and Image Generation in a zero-shot manner. Code and
models are available at https://github.com/TencentARC/ViT-Lens.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work is a follow-up of arXiv:2308.10185. Accepted to CVPR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Implicit Discriminative Knowledge Learning for Visible-Infrared Person
  Re-Identification <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11708v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11708v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaijie Ren, Lei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visible-Infrared Person Re-identification (VI-ReID) is a challenging
cross-modal pedestrian retrieval task, due to significant intra-class
variations and cross-modal discrepancies among different cameras. Existing
works mainly focus on embedding images of different modalities into a unified
space to mine modality-shared features. They only seek distinctive information
within these shared features, while ignoring the identity-aware useful
information that is implicit in the modality-specific features. To address this
issue, we propose a novel Implicit Discriminative Knowledge Learning (IDKL)
network to uncover and leverage the implicit discriminative information
contained within the modality-specific. First, we extract modality-specific and
modality-shared features using a novel dual-stream network. Then, the
modality-specific features undergo purification to reduce their modality style
discrepancies while preserving identity-aware discriminative knowledge.
Subsequently, this kind of implicit knowledge is distilled into the
modality-shared feature to enhance its distinctiveness. Finally, an alignment
loss is proposed to minimize modality discrepancy on enhanced modality-shared
features. Extensive experiments on multiple public datasets demonstrate the
superiority of IDKL network over the state-of-the-art methods. Code is
available at https://github.com/1KK077/IDKL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ In Search of a Data Transformation That Accelerates Neural Field
  Training <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.17094v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.17094v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junwon Seo, Sangyoon Lee, Kwang In Kim, Jaeho Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural field is an emerging paradigm in data representation that trains a
neural network to approximate the given signal. A key obstacle that prevents
its widespread adoption is the encoding speed-generating neural fields requires
an overfitting of a neural network, which can take a significant number of SGD
steps to reach the desired fidelity level. In this paper, we delve into the
impacts of data transformations on the speed of neural field training,
specifically focusing on how permuting pixel locations affect the convergence
speed of SGD. Counterintuitively, we find that randomly permuting the pixel
locations can considerably accelerate the training. To explain this phenomenon,
we examine the neural field training through the lens of PSNR curves, loss
landscapes, and error patterns. Our analyses suggest that the random pixel
permutations remove the easy-to-fit patterns, which facilitate easy
optimization in the early stage but hinder capturing fine details of the
signal.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AV2AV: Direct Audio-Visual Speech to Audio-Visual Speech Translation
  with Unified Audio-Visual Speech Representation <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.02512v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.02512v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeongsoo Choi, Se Jin Park, Minsu Kim, Yong Man Ro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a novel direct Audio-Visual Speech to Audio-Visual Speech
Translation (AV2AV) framework, where the input and output of the system are
multimodal (i.e., audio and visual speech). With the proposed AV2AV, two key
advantages can be brought: 1) We can perform real-like conversations with
individuals worldwide in a virtual meeting by utilizing our own primary
languages. In contrast to Speech-to-Speech Translation (A2A), which solely
translates between audio modalities, the proposed AV2AV directly translates
between audio-visual speech. This capability enhances the dialogue experience
by presenting synchronized lip movements along with the translated speech. 2)
We can improve the robustness of the spoken language translation system. By
employing the complementary information of audio-visual speech, the system can
effectively translate spoken language even in the presence of acoustic noise,
showcasing robust performance. To mitigate the problem of the absence of a
parallel AV2AV translation dataset, we propose to train our spoken language
translation system with the audio-only dataset of A2A. This is done by learning
unified audio-visual speech representations through self-supervised learning in
advance to train the translation system. Moreover, we propose an AV-Renderer
that can generate raw audio and video in parallel. It is designed with
zero-shot speaker modeling, thus the speaker in source audio-visual speech can
be maintained at the target translated audio-visual speech. The effectiveness
of AV2AV is evaluated with extensive experiments in a many-to-many language
translation setting. Demo page is available on
https://choijeongsoo.github.io/av2av.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024. Code & Demo: https://choijeongsoo.github.io/av2av</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SINC: Spatial Composition of 3D Human Motions for Simultaneous Action
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.10417v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.10417v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikos Athanasiou, Mathis Petrovich, Michael J. Black, Gül Varol
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Our goal is to synthesize 3D human motions given textual inputs describing
simultaneous actions, for example 'waving hand' while 'walking' at the same
time. We refer to generating such simultaneous movements as performing 'spatial
compositions'. In contrast to temporal compositions that seek to transition
from one action to another, spatial compositing requires understanding which
body parts are involved in which action, to be able to move them
simultaneously. Motivated by the observation that the correspondence between
actions and body parts is encoded in powerful language models, we extract this
knowledge by prompting GPT-3 with text such as "what are the body parts
involved in the action <action name>?", while also providing the parts list and
few-shot examples. Given this action-part mapping, we combine body parts from
two motions together and establish the first automated method to spatially
compose two actions. However, training data with compositional actions is
always limited by the combinatorics. Hence, we further create synthetic data
with this approach, and use it to train a new state-of-the-art text-to-motion
generation model, called SINC ("SImultaneous actioN Compositions for 3D human
motions"). In our experiments, that training with such GPT-guided synthetic
data improves spatial composition generation over baselines. Our code is
publicly available at https://sinc.is.tue.mpg.de/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Teaser Fixed</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Powerful Lossy Compression for Noisy Images <span class="chip">ICME 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14135v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14135v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shilv Cai, Xiaoguo Liang, Shuning Cao, Luxin Yan, Sheng Zhong, Liqun Chen, Xu Zou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image compression and denoising represent fundamental challenges in image
processing with many real-world applications. To address practical demands,
current solutions can be categorized into two main strategies: 1) sequential
method; and 2) joint method. However, sequential methods have the disadvantage
of error accumulation as there is information loss between multiple individual
models. Recently, the academic community began to make some attempts to tackle
this problem through end-to-end joint methods. Most of them ignore that
different regions of noisy images have different characteristics. To solve
these problems, in this paper, our proposed signal-to-noise ratio~(SNR) aware
joint solution exploits local and non-local features for image compression and
denoising simultaneously. We design an end-to-end trainable network, which
includes the main encoder branch, the guidance branch, and the signal-to-noise
ratio~(SNR) aware branch. We conducted extensive experiments on both synthetic
and real-world datasets, demonstrating that our joint solution outperforms
existing state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICME 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ViT-Lens: Initiating Omni-Modal Exploration through 3D Insights 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.10185v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.10185v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weixian Lei, Yixiao Ge, Jianfeng Zhang, Dylan Sun, Kun Yi, Ying Shan, Mike Zheng Shou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Though the success of CLIP-based training recipes in vision-language models,
their scalability to more modalities (e.g., 3D, audio, etc.) is limited to
large-scale data, which is expensive or even inapplicable for rare modalities.
In this paper, we present ViT-Lens that facilitates efficient omni-modal
representation learning by perceiving novel modalities with a pretrained ViT
and aligning to a pre-defined space. Specifically, the modality-specific lens
is tuned to project multimodal signals to the shared embedding space, which are
then processed by a strong ViT that carries pre-trained image knowledge. The
encoded multimodal representations are optimized toward aligning with the
modal-independent space, pre-defined by off-the-shelf foundation models. A
well-trained lens with a ViT backbone has the potential to serve as one of
these foundation models, supervising the learning of subsequent modalities.
ViT-Lens provides a unified solution for representation learning of increasing
modalities with two appealing benefits: (i) Exploiting the pretrained ViT
across tasks and domains effectively with efficient data regime; (ii) Emergent
downstream capabilities of novel modalities are demonstrated due to the
modality alignment space. We evaluate ViT-Lens in the context of 3D as an
initial verification. In zero-shot 3D classification, ViT-Lens achieves
substantial improvements over previous state-of-the-art, showing 52.0% accuracy
on Objaverse-LVIS, 87.4% on ModelNet40, and 60.6% on ScanObjectNN. Furthermore,
we enable zero-shot 3D question-answering by simply integrating the trained 3D
lens into the InstructBLIP model without any adaptation. We will release the
results of ViT-Lens on more modalities in the near future.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 4 figures and 9 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TP2O: Creative Text Pair-to-Object Generation using Balance
  Swap-Sampling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.01819v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.01819v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun Li, Zedong Zhang, Jian Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating creative combinatorial objects from two seemingly unrelated object
texts is a challenging task in text-to-image synthesis, often hindered by a
focus on emulating existing data distributions. In this paper, we develop a
straightforward yet highly effective method, called \textbf{balance
swap-sampling}. First, we propose a swapping mechanism that generates a novel
combinatorial object image set by randomly exchanging intrinsic elements of two
text embeddings through a cutting-edge diffusion model. Second, we introduce a
balance swapping region to efficiently sample a small subset from the newly
generated image set by balancing CLIP distances between the new images and
their original generations, increasing the likelihood of accepting the
high-quality combinations. Last, we employ a segmentation method to compare
CLIP distances among the segmented components, ultimately selecting the most
promising object from the sampled subset. Extensive experiments demonstrate
that our approach outperforms recent SOTA T2I methods. Surprisingly, our
results even rival those of human artists, such as frog-broccoli.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://tp2o.github.io/anon/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Segment and Caption Anything <span class="chip">CVPR 24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.00869v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.00869v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoke Huang, Jianfeng Wang, Yansong Tang, Zheng Zhang, Han Hu, Jiwen Lu, Lijuan Wang, Zicheng Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a method to efficiently equip the Segment Anything Model (SAM)
with the ability to generate regional captions. SAM presents strong
generalizability to segment anything while is short for semantic understanding.
By introducing a lightweight query-based feature mixer, we align the
region-specific features with the embedding space of language models for later
caption generation. As the number of trainable parameters is small (typically
in the order of tens of millions), it costs less computation, less memory
usage, and less communication bandwidth, resulting in both fast and scalable
training. To address the scarcity problem of regional caption data, we propose
to first pre-train our model on objection detection and segmentation tasks. We
call this step weak supervision pretraining since the pre-training data only
contains category names instead of full-sentence descriptions. The weak
supervision pretraining allows us to leverage many publicly available object
detection and segmentation datasets. We conduct extensive experiments to
demonstrate the superiority of our method and validate each design choice. This
work serves as a stepping stone towards scaling up regional captioning data and
sheds light on exploring efficient ways to augment SAM with regional semantics.
The project page, along with the associated code, can be accessed via
https://xk-huang.github.io/segment-caption-anything/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The project page, along with the associated code, can be accessed via
  https://xk-huang.github.io/segment-caption-anything/; Update author
  information; Accepted by CVPR 24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TagAlign: Improving Vision-Language Alignment with Multi-Tag
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.14149v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.14149v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qinying Liu, Wei Wu, Kecheng Zheng, Zhan Tong, Jiawei Liu, Yu Liu, Wei Chen, Zilei Wang, Yujun Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The crux of learning vision-language models is to extract semantically
aligned information from visual and linguistic data. Existing attempts usually
face the problem of coarse alignment, e.g., the vision encoder struggles in
localizing an attribute-specified object. In this work, we propose an
embarrassingly simple approach to better align image and text features with no
need of additional data formats other than image-text pairs. Concretely, given
an image and its paired text, we manage to parse objects (e.g., cat) and
attributes (e.g., black) from the description, which are highly likely to exist
in the image. It is noteworthy that the parsing pipeline is fully automatic and
thus enjoys good scalability. With these parsed semantics as supervision
signals, we can complement the commonly used image-text contrastive loss with
the multi-tag classification loss. Extensive experimental results on a broad
suite of semantic segmentation datasets substantiate the average 5.2\%
improvement of our framework over existing alternatives. Furthermore, the
visualization results indicate that attribute supervision makes vision-language
models accurately localize attribute-specified objects. Project page can be
found at https://qinying-liu.github.io/Tag-Align.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SGS-SLAM: Semantic Gaussian Splatting For Neural Dense SLAM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.03246v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.03246v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingrui Li, Shuhong Liu, Heng Zhou, Guohao Zhu, Na Cheng, Tianchen Deng, Hongyu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present SGS-SLAM, the first semantic visual SLAM system based on Gaussian
Splatting. It incorporates appearance, geometry, and semantic features through
multi-channel optimization, addressing the oversmoothing limitations of neural
implicit SLAM systems in high-quality rendering, scene understanding, and
object-level geometry. We introduce a unique semantic feature loss that
effectively compensates for the shortcomings of traditional depth and color
losses in object optimization. Through a semantic-guided keyframe selection
strategy, we prevent erroneous reconstructions caused by cumulative errors.
Extensive experiments demonstrate that SGS-SLAM delivers state-of-the-art
performance in camera pose estimation, map reconstruction, precise semantic
segmentation, and object-level geometric accuracy, while ensuring real-time
rendering capabilities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ArtAdapter: Text-to-Image Style Transfer using Multi-Level Style Encoder
  and Explicit Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.02109v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.02109v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dar-Yen Chen, Hamish Tennent, Ching-Wen Hsu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work introduces ArtAdapter, a transformative text-to-image (T2I) style
transfer framework that transcends traditional limitations of color,
brushstrokes, and object shape, capturing high-level style elements such as
composition and distinctive artistic expression. The integration of a
multi-level style encoder with our proposed explicit adaptation mechanism
enables ArtAdapter to achieve unprecedented fidelity in style transfer,
ensuring close alignment with textual descriptions. Additionally, the
incorporation of an Auxiliary Content Adapter (ACA) effectively separates
content from style, alleviating the borrowing of content from style references.
Moreover, our novel fast finetuning approach could further enhance zero-shot
style representation while mitigating the risk of overfitting. Comprehensive
evaluations confirm that ArtAdapter surpasses current state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Clean-image Backdoor Attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15010v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15010v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dazhong Rong, Guoyao Yu, Shuheng Shen, Xinyi Fu, Peng Qian, Jianhai Chen, Qinming He, Xing Fu, Weiqiang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To gather a significant quantity of annotated training data for
high-performance image classification models, numerous companies opt to enlist
third-party providers to label their unlabeled data. This practice is widely
regarded as secure, even in cases where some annotated errors occur, as the
impact of these minor inaccuracies on the final performance of the models is
negligible and existing backdoor attacks require attacker's ability to poison
the training images. Nevertheless, in this paper, we propose clean-image
backdoor attacks which uncover that backdoors can still be injected via a
fraction of incorrect labels without modifying the training images.
Specifically, in our attacks, the attacker first seeks a trigger feature to
divide the training images into two parts: those with the feature and those
without it. Subsequently, the attacker falsifies the labels of the former part
to a backdoor class. The backdoor will be finally implanted into the target
model after it is trained on the poisoned data. During the inference phase, the
attacker can activate the backdoor in two ways: slightly modifying the input
image to obtain the trigger feature, or taking an image that naturally has the
trigger feature as input. We conduct extensive experiments to demonstrate the
effectiveness and practicality of our attacks. According to the experimental
results, we conclude that our attacks seriously jeopardize the fairness and
robustness of image classification models, and it is necessary to be vigilant
about the incorrect labels in outsourced labeling.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Transferring Relative Monocular Depth to Surgical Vision with Temporal
  Consistency 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06683v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06683v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Charlie Budd, Tom Vercauteren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Relative monocular depth, inferring depth up to shift and scale from a single
image, is an active research topic. Recent deep learning models, trained on
large and varied meta-datasets, now provide excellent performance in the domain
of natural images. However, few datasets exist which provide ground truth depth
for endoscopic images, making training such models from scratch unfeasible.
This work investigates the transfer of these models into the surgical domain,
and presents an effective and simple way to improve on standard supervision
through the use of temporal consistency self-supervision. We show temporal
consistency significantly improves supervised training alone when transferring
to the low-data regime of endoscopy, and outperforms the prevalent
self-supervision technique for this task. In addition we show our method
drastically outperforms the state-of-the-art method from within the domain of
endoscopy. We also release our code, model and ensembled meta-dataset,
Meta-MED, establishing a strong benchmark for future work.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Source-free Domain Adaptive Semantic Segmentation via
  Importance-aware and Prototype-contrast Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.01598v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.01598v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yihong Cao, Hui Zhang, Xiao Lu, Zheng Xiao, Kailun Yang, Yaonan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Domain adaptive semantic segmentation enables robust pixel-wise understanding
in real-world driving scenes. Source-free domain adaptation, as a more
practical technique, addresses the concerns of data privacy and storage
limitations in typical unsupervised domain adaptation methods, making it
especially relevant in the context of intelligent vehicles. It utilizes a
well-trained source model and unlabeled target data to achieve adaptation in
the target domain. However, in the absence of source data and target labels,
current solutions cannot sufficiently reduce the impact of domain shift and
fully leverage the information from the target data. In this paper, we propose
an end-to-end source-free domain adaptation semantic segmentation method via
Importance-Aware and Prototype-Contrast (IAPC) learning. The proposed IAPC
framework effectively extracts domain-invariant knowledge from the well-trained
source model and learns domain-specific knowledge from the unlabeled target
domain. Specifically, considering the problem of domain shift in the prediction
of the target domain by the source model, we put forward an importance-aware
mechanism for the biased target prediction probability distribution to extract
domain-invariant knowledge from the source model. We further introduce a
prototype-contrast strategy, which includes a prototype-symmetric cross-entropy
loss and a prototype-enhanced cross-entropy loss, to learn target intra-domain
knowledge without relying on labels. A comprehensive variety of experiments on
two domain adaptive semantic segmentation benchmarks demonstrates that the
proposed end-to-end IAPC solution outperforms existing state-of-the-art
methods. The source code is publicly available at
https://github.com/yihong-97/Source-free-IAPC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEEE Transactions on Intelligent Vehicles (T-IV). The
  source code is publicly available at
  https://github.com/yihong-97/Source-free-IAPC</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SD4Match: Learning to <span class="highlight-title">Prompt</span> Stable Diffusion Model for Semantic
  Matching <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.17569v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.17569v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinghui Li, Jingyi Lu, Kai Han, Victor Prisacariu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we address the challenge of matching semantically similar
keypoints across image pairs. Existing research indicates that the intermediate
output of the UNet within the Stable Diffusion (SD) can serve as robust image
feature maps for such a matching task. We demonstrate that by employing a basic
prompt tuning technique, the inherent potential of Stable Diffusion can be
harnessed, resulting in a significant enhancement in accuracy over previous
approaches. We further introduce a novel conditional prompting module that
conditions the prompt on the local details of the input image pairs, leading to
a further improvement in performance. We designate our approach as SD4Match,
short for Stable Diffusion for Semantic Matching. Comprehensive evaluations of
SD4Match on the PF-Pascal, PF-Willow, and SPair-71k datasets show that it sets
new benchmarks in accuracy across all these datasets. Particularly, SD4Match
outperforms the previous state-of-the-art by a margin of 12 percentage points
on the challenging SPair-71k dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2024. Project website:
  https://sd4match.active.vision/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ObjectCompose: Evaluating Resilience of Vision-Based Models on
  Object-to-Background Compositional Changes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.04701v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.04701v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hashmat Shadab Malik, Muhammad Huzaifa, Muzammal Naseer, Salman Khan, Fahad Shahbaz Khan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given the large-scale multi-modal training of recent vision-based models and
their generalization capabilities, understanding the extent of their robustness
is critical for their real-world deployment. In this work, we evaluate the
resilience of current vision-based models against diverse object-to-background
context variations. The majority of robustness evaluation methods have
introduced synthetic datasets to induce changes to object characteristics
(viewpoints, scale, color) or utilized image transformation techniques
(adversarial changes, common corruptions) on real images to simulate shifts in
distributions. Recent works have explored leveraging large language models and
diffusion models to generate changes in the background. However, these methods
either lack in offering control over the changes to be made or distort the
object semantics, making them unsuitable for the task. Our method, on the other
hand, can induce diverse object-to-background changes while preserving the
original semantics and appearance of the object. To achieve this goal, we
harness the generative capabilities of text-to-image, image-to-text, and
image-to-segment models to automatically generate a broad spectrum of
object-to-background changes. We induce both natural and adversarial background
changes by either modifying the textual prompts or optimizing the latents and
textual embedding of text-to-image models. We produce various versions of
standard vision datasets (ImageNet, COCO), incorporating either diverse and
realistic backgrounds into the images or introducing color, texture, and
adversarial changes in the background. We conduct extensive experiment to
analyze the robustness of vision-based models against object-to-background
context variations across diverse tasks. Code
https://github.com/Muhammad-Huzaifaa/ObjectCompose.git
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Motion Generation from Fine-grained Textual Descriptions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13518v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13518v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kunhang Li, Yansong Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The task of text2motion is to generate human motion sequences from given
textual descriptions, where the model explores diverse mappings from natural
language instructions to human body movements. While most existing works are
confined to coarse-grained motion descriptions, e.g., "A man squats.",
fine-grained descriptions specifying movements of relevant body parts are
barely explored. Models trained with coarse-grained texts may not be able to
learn mappings from fine-grained motion-related words to motion primitives,
resulting in the failure to generate motions from unseen descriptions. In this
paper, we build a large-scale language-motion dataset specializing in
fine-grained textual descriptions, FineHumanML3D, by feeding GPT-3.5-turbo with
step-by-step instructions with pseudo-code compulsory checks. Accordingly, we
design a new text2motion model, FineMotionDiffuse, making full use of
fine-grained textual information. Our quantitative evaluation shows that
FineMotionDiffuse trained on FineHumanML3D improves FID by a large margin of
0.38, compared with competitive baselines. According to the qualitative
evaluation and case study, our model outperforms MotionDiffuse in generating
spatially or chronologically composite motions, by learning the implicit
mappings from fine-grained descriptions to the corresponding basic motions. We
release our data at https://github.com/KunhangL/finemotiondiffuse.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Low-Energy Adaptive Personalization for Resource-Constrained
  Devices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15905v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15905v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yushan Huang, Josh Millar, Yuxuan Long, Yuchen Zhao, Hamed Hadaddi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The personalization of machine learning (ML) models to address data drift is
a significant challenge in the context of Internet of Things (IoT)
applications. Presently, most approaches focus on fine-tuning either the full
base model or its last few layers to adapt to new data, while often neglecting
energy costs. However, various types of data drift exist, and fine-tuning the
full base model or the last few layers may not result in optimal performance in
certain scenarios. We propose Target Block Fine-Tuning (TBFT), a low-energy
adaptive personalization framework designed for resource-constrained devices.
We categorize data drift and personalization into three types: input-level,
feature-level, and output-level. For each type, we fine-tune different blocks
of the model to achieve optimal performance with reduced energy costs.
Specifically, input-, feature-, and output-level correspond to fine-tuning the
front, middle, and rear blocks of the model. We evaluate TBFT on a ResNet
model, three datasets, three different training sizes, and a Raspberry Pi.
Compared with the $Block Avg$, where each block is fine-tuned individually and
their performance improvements are averaged, TBFT exhibits an improvement in
model accuracy by an average of 15.30% whilst saving 41.57% energy consumption
on average compared with full fine-tuning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepetd to The 4th Workshop on Machine Learning and Systems
  (EuroMLSys '24)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FPT: Fine-grained <span class="highlight-title">Prompt</span> Tuning for Parameter and Memory Efficient Fine
  Tuning in High-resolution Medical Image Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07576v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07576v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yijin Huang, Pujin Cheng, Roger Tam, Xiaoying Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Parameter-efficient fine-tuning (PEFT) is proposed as a cost-effective way to
transfer pre-trained models to downstream tasks, avoiding the high cost of
updating entire large-scale pre-trained models (LPMs). In this work, we present
Fine-grained Prompt Tuning (FPT), a novel PEFT method for medical image
classification. FPT significantly reduces memory consumption compared to other
PEFT methods, especially in high-resolution contexts. To achieve this, we first
freeze the weights of the LPM and construct a learnable lightweight side
network. The frozen LPM takes high-resolution images as input to extract
fine-grained features, while the side network is fed low-resolution images to
reduce memory usage. To allow the side network to access pre-trained knowledge,
we introduce fine-grained prompts that summarize information from the LPM
through a fusion module. Important tokens selection and preloading techniques
are employed to further reduce training cost and memory requirements. We
evaluate FPT on four medical datasets with varying sizes, modalities, and
complexities. Experimental results demonstrate that FPT achieves comparable
performance to fine-tuning the entire LPM while using only 1.8% of the
learnable parameters and 13% of the memory costs of an encoder ViT-B model with
a 512 x 512 input resolution.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SegVol: Universal and Interactive Volumetric Medical Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13385v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13385v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxin Du, Fan Bai, Tiejun Huang, Bo Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Precise image segmentation provides clinical study with instructive
information. Despite the remarkable progress achieved in medical image
segmentation, there is still an absence of 3D foundation segmentation model
that can segment a wide range of anatomical categories with easy user
interaction. In this paper, we propose a 3D foundation segmentation model,
named SegVol, supporting universal and interactive volumetric medical image
segmentation. By scaling up training data to 90K unlabeled Computed Tomography
(CT) volumes and 6K labeled CT volumes, this foundation model supports the
segmentation of over 200 anatomical categories using semantic and spatial
prompts. Extensive experiments on 10 internal validation tasks and 18 external
validation tasks verify that SegVol outperforms the state of the art by a large
margin. Through its capacity to provide precise volumetric segmentation across
various anatomical categories, SegVol has the potential to accelerate
advancements in medical imaging diagnosis and facilitate treatment
optimization. The model and code are publicly available at:
https://github.com/BAAI-DCAI/SegVol.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DreamComposer: Controllable 3D Object Generation via Multi-View
  Conditions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.03611v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.03611v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunhan Yang, Yukun Huang, Xiaoyang Wu, Yuan-Chen Guo, Song-Hai Zhang, Hengshuang Zhao, Tong He, Xihui Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Utilizing pre-trained 2D large-scale generative models, recent works are
capable of generating high-quality novel views from a single in-the-wild image.
However, due to the lack of information from multiple views, these works
encounter difficulties in generating controllable novel views. In this paper,
we present DreamComposer, a flexible and scalable framework that can enhance
existing view-aware diffusion models by injecting multi-view conditions.
Specifically, DreamComposer first uses a view-aware 3D lifting module to obtain
3D representations of an object from multiple views. Then, it renders the
latent features of the target view from 3D representations with the multi-view
feature fusion module. Finally the target view features extracted from
multi-view inputs are injected into a pre-trained diffusion model. Experiments
show that DreamComposer is compatible with state-of-the-art diffusion models
for zero-shot novel view synthesis, further enhancing them to generate
high-fidelity novel view images with multi-view conditions, ready for
controllable 3D object reconstruction and various other applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://yhyang-myron.github.io/DreamComposer/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Regularizing <span class="highlight-title">Self-supervised</span> 3D Scene Flows with Surface Awareness and
  Cyclic Consistency 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.08879v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.08879v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Patrik Vacek, David Hurych, Karel Zimmermann, Patrick Perez, Tomas Svoboda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning without supervision how to predict 3D scene flows from point clouds
is essential to many perception systems. We propose a novel learning framework
for this task which improves the necessary regularization. Relying on the
assumption that scene elements are mostly rigid, current smoothness losses are
built on the definition of ``rigid clusters" in the input point clouds. The
definition of these clusters is challenging and has a significant impact on the
quality of predicted flows. We introduce two new consistency losses that
enlarge clusters while preventing them from spreading over distinct objects. In
particular, we enforce \emph{temporal} consistency with a forward-backward
cyclic loss and \emph{spatial} consistency by considering surface orientation
similarity in addition to spatial proximity. The proposed losses are
model-independent and can thus be used in a plug-and-play fashion to
significantly improve the performance of existing models, as demonstrated on
two most widely used architectures. We also showcase the effectiveness and
generalization capability of our framework on four standard sensor-unique
driving datasets, achieving state-of-the-art performance in 3D scene flow
estimation. Our codes are available on https://github.com/ctu-vras/sac-flow.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ P2ANet: A <span class="highlight-title">Dataset</span> and Benchmark for Dense Action Detection from Table
  Tennis Match Broadcasting Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.12730v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.12730v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiang Bian, Xuhong Li, Tao Wang, Qingzhong Wang, Jun Huang, Chen Liu, Jun Zhao, Feixiang Lu, Dejing Dou, Haoyi Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While deep learning has been widely used for video analytics, such as video
classification and action detection, dense action detection with fast-moving
subjects from sports videos is still challenging. In this work, we release yet
another sports video benchmark \TheName{} for \emph{\underline{P}}ing
\emph{\underline{P}}ong-\emph{\underline{A}}ction detection, which consists of
2,721 video clips collected from the broadcasting videos of professional table
tennis matches in World Table Tennis Championships and Olympiads. We work with
a crew of table tennis professionals and referees on a specially designed
annotation toolbox to obtain fine-grained action labels (in 14 classes) for
every ping-pong action that appeared in the dataset, and formulate two sets of
action detection problems -- \emph{action localization} and \emph{action
recognition}. We evaluate a number of commonly-seen action recognition (e.g.,
TSM, TSN, Video SwinTransformer, and Slowfast) and action localization models
(e.g., BSN, BSN++, BMN, TCANet), using \TheName{} for both problems, under
various settings. These models can only achieve 48\% area under the AR-AN curve
for localization and 82\% top-one accuracy for recognition since the ping-pong
actions are dense with fast-moving subjects but broadcasting videos are with
only 25 FPS. The results confirm that \TheName{} is still a challenging task
and can be used as a special benchmark for dense action detection from videos.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unsupervised Semantic Segmentation Through Depth-Guided Feature
  Correlation and Sampling <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.12378v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.12378v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leon Sick, Dominik Engel, Pedro Hermosilla, Timo Ropinski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditionally, training neural networks to perform semantic segmentation
required expensive human-made annotations. But more recently, advances in the
field of unsupervised learning have made significant progress on this issue and
towards closing the gap to supervised algorithms. To achieve this, semantic
knowledge is distilled by learning to correlate randomly sampled features from
images across an entire dataset. In this work, we build upon these advances by
incorporating information about the structure of the scene into the training
process through the use of depth information. We achieve this by (1) learning
depth-feature correlation by spatially correlate the feature maps with the
depth maps to induce knowledge about the structure of the scene and (2)
implementing farthest-point sampling to more effectively select relevant
features by utilizing 3D sampling techniques on depth information of the scene.
Finally, we demonstrate the effectiveness of our technical contributions
through extensive experimentation and present significant improvements in
performance across multiple benchmark datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Diffusion Reflectance Map: Single-Image Stochastic Inverse Rendering of
  Illumination and Reflectance <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.04529v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.04529v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuto Enyo, Ko Nishino
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reflectance bounds the frequency spectrum of illumination in the object
appearance. In this paper, we introduce the first stochastic inverse rendering
method, which recovers the attenuated frequency spectrum of an illumination
jointly with the reflectance of an object of known geometry from a single
image. Our key idea is to solve this blind inverse problem in the reflectance
map, an appearance representation invariant to the underlying geometry, by
learning to reverse the image formation with a novel diffusion model which we
refer to as the Diffusion Reflectance Map Network (DRMNet). Given an observed
reflectance map converted and completed from the single input image, DRMNet
generates a reflectance map corresponding to a perfect mirror sphere while
jointly estimating the reflectance. The forward process can be understood as
gradually filtering a natural illumination with lower and lower frequency
reflectance and additive Gaussian noise. DRMNet learns to invert this process
with two subnetworks, IllNet and RefNet, which work in concert towards this
joint estimation. The network is trained on an extensive synthetic dataset and
is demonstrated to generalize to real images, showing state-of-the-art accuracy
on established datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>to be published in CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ProMamba: <span class="highlight-title">Prompt</span>-Mamba for polyp segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13660v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13660v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianhao Xie, Ruofan Liao, Ziang Zhang, Sida Yi, Yuesheng Zhu, Guibo Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting polyps through colonoscopy is an important task in medical image
segmentation, which provides significant assistance and reference value for
clinical surgery. However, accurate segmentation of polyps is a challenging
task due to two main reasons. Firstly, polyps exhibit various shapes and
colors. Secondly, the boundaries between polyps and their normal surroundings
are often unclear. Additionally, significant differences between different
datasets lead to limited generalization capabilities of existing methods. To
address these issues, we propose a segmentation model based on Prompt-Mamba,
which incorporates the latest Vision-Mamba and prompt technologies. Compared to
previous models trained on the same dataset, our model not only maintains high
segmentation accuracy on the validation part of the same dataset but also
demonstrates superior accuracy on unseen datasets, exhibiting excellent
generalization capabilities. Notably, we are the first to apply the
Vision-Mamba architecture to polyp segmentation and the first to utilize prompt
technology in a polyp segmentation model. Our model efficiently accomplishes
segmentation tasks, surpassing previous state-of-the-art methods by an average
of 5% across six datasets. Furthermore, we have developed multiple versions of
our model with scaled parameter counts, achieving better performance than
previous models even with fewer parameters. Our code and trained weights will
be released soon.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 2 figures,3 tabels</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SocialCircle: Learning the Angle-based Social Interaction Representation
  for Pedestrian Trajectory Prediction <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05370v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05370v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Conghao Wong, Beihao Xia, Ziqian Zou, Yulong Wang, Xinge You
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Analyzing and forecasting trajectories of agents like pedestrians and cars in
complex scenes has become more and more significant in many intelligent systems
and applications. The diversity and uncertainty in socially interactive
behaviors among a rich variety of agents make this task more challenging than
other deterministic computer vision tasks. Researchers have made a lot of
efforts to quantify the effects of these interactions on future trajectories
through different mathematical models and network structures, but this problem
has not been well solved. Inspired by marine animals that localize the
positions of their companions underwater through echoes, we build a new
anglebased trainable social interaction representation, named SocialCircle, for
continuously reflecting the context of social interactions at different angular
orientations relative to the target agent. We validate the effect of the
proposed SocialCircle by training it along with several newly released
trajectory prediction models, and experiments show that the SocialCircle not
only quantitatively improves the prediction performance, but also qualitatively
helps better simulate social interactions when forecasting pedestrian
trajectories in a way that is consistent with human intuitions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024 accepted</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Emotic Masked Autoencoder with Attention Fusion for Facial Expression
  Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13039v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13039v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bach Nguyen-Xuan, Thien Nguyen-Hoang, Nhu Tai-Do
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Facial Expression Recognition (FER) is a critical task within computer vision
with diverse applications across various domains. Addressing the challenge of
limited FER datasets, which hampers the generalization capability of expression
recognition models, is imperative for enhancing performance. Our paper presents
an innovative approach integrating the MAE-Face self-supervised learning (SSL)
method and Fusion Attention mechanism for expression classification,
particularly showcased in the 6th Affective Behavior 32 pages harvmac; added
references for section 5Analysis in-the-wild (ABAW) competition. Additionally,
we propose preprocessing techniques to emphasize essential facial features,
thereby enhancing model performance on both training and validation sets,
notably demonstrated on the Aff-wild2 dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages; added references for section 1; corrected typo for email
  author</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning User Embeddings from Human Gaze for Personalised Saliency
  Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13653v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13653v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florian Strohm, Mihai Bâce, Andreas Bulling
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reusable embeddings of user behaviour have shown significant performance
improvements for the personalised saliency prediction task. However, prior
works require explicit user characteristics and preferences as input, which are
often difficult to obtain. We present a novel method to extract user embeddings
from pairs of natural images and corresponding saliency maps generated from a
small amount of user-specific eye tracking data. At the core of our method is a
Siamese convolutional neural encoder that learns the user embeddings by
contrasting the image and personal saliency map pairs of different users.
Evaluations on two public saliency datasets show that the generated embeddings
have high discriminative power, are effective at refining universal saliency
maps to the individual users, and generalise well across users and images.
Finally, based on our model's ability to encode individual user
characteristics, our work points towards other applications that can benefit
from reusable embeddings of gaze behaviour.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VRP-SAM: SAM with Visual Reference <span class="highlight-title">Prompt</span> <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17726v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17726v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanpeng Sun, Jiahui Chen, Shan Zhang, Xinyu Zhang, Qiang Chen, Gang Zhang, Errui Ding, Jingdong Wang, Zechao Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a novel Visual Reference Prompt (VRP) encoder that
empowers the Segment Anything Model (SAM) to utilize annotated reference images
as prompts for segmentation, creating the VRP-SAM model. In essence, VRP-SAM
can utilize annotated reference images to comprehend specific objects and
perform segmentation of specific objects in target image. It is note that the
VRP encoder can support a variety of annotation formats for reference images,
including \textbf{point}, \textbf{box}, \textbf{scribble}, and \textbf{mask}.
VRP-SAM achieves a breakthrough within the SAM framework by extending its
versatility and applicability while preserving SAM's inherent strengths, thus
enhancing user-friendliness. To enhance the generalization ability of VRP-SAM,
the VRP encoder adopts a meta-learning strategy. To validate the effectiveness
of VRP-SAM, we conducted extensive empirical studies on the Pascal and COCO
datasets. Remarkably, VRP-SAM achieved state-of-the-art performance in visual
reference segmentation with minimal learnable parameters. Furthermore, VRP-SAM
demonstrates strong generalization capabilities, allowing it to perform
segmentation of unseen objects and enabling cross-domain segmentation. The
source code and models will be available at
\url{https://github.com/syp2ysy/VRP-SAM}
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2024; The camera-ready version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SeFFeC: Semantic Facial Feature Control for Fine-grained Face Editing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13972v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13972v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florian Strohm, Mihai Bâce, Markus Kaltenecker, Andreas Bulling
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose Semantic Facial Feature Control (SeFFeC) - a novel method for
fine-grained face shape editing. Our method enables the manipulation of
human-understandable, semantic face features, such as nose length or mouth
width, which are defined by different groups of facial landmarks. In contrast
to existing methods, the use of facial landmarks enables precise measurement of
the facial features, which then enables training SeFFeC without any manually
annotated labels. SeFFeC consists of a transformer-based encoder network that
takes a latent vector of a pre-trained generative model and a facial feature
embedding as input, and learns to modify the latent vector to perform the
desired face edit operation. To ensure that the desired feature measurement is
changed towards the target value without altering uncorrelated features, we
introduced a novel semantic face feature loss. Qualitative and quantitative
results show that SeFFeC enables precise and fine-grained control of 23 facial
features, some of which could not previously be controlled by other methods,
without requiring manual annotations. Unlike existing methods, SeFFeC also
provides deterministic control over the exact values of the facial features and
more localised and disentangled face edits.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dual Prototype Attention for Unsupervised Video Object Segmentation <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.12036v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.12036v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Suhwan Cho, Minhyeok Lee, Seunghoon Lee, Dogyoon Lee, Heeseung Choi, Ig-Jae Kim, Sangyoun Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised video object segmentation (VOS) aims to detect and segment the
most salient object in videos. The primary techniques used in unsupervised VOS
are 1) the collaboration of appearance and motion information; and 2) temporal
fusion between different frames. This paper proposes two novel prototype-based
attention mechanisms, inter-modality attention (IMA) and inter-frame attention
(IFA), to incorporate these techniques via dense propagation across different
modalities and frames. IMA densely integrates context information from
different modalities based on a mutual refinement. IFA injects global context
of a video to the query frame, enabling a full utilization of useful properties
from multiple frames. Experimental results on public benchmark datasets
demonstrate that our proposed approach outperforms all existing methods by a
substantial margin. The proposed two components are also thoroughly validated
via ablative study.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Pretext to Purpose: Batch-Adaptive <span class="highlight-title">Self-Supervised</span> Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.09974v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.09974v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiansong Zhang, Linlin Shen, Peizhong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, self-supervised contrastive learning has emerged as a
distinguished paradigm in the artificial intelligence landscape. It facilitates
unsupervised feature learning through contrastive delineations at the instance
level. However, crafting an effective self-supervised paradigm remains a
pivotal challenge within this field. This paper delves into two crucial factors
impacting self-supervised contrastive learning-bach size and pretext tasks, and
from a data processing standpoint, proposes an adaptive technique of batch
fusion. The proposed method, via dimensionality reduction and reconstruction of
batch data, enables formerly isolated individual data to partake in intra-batch
communication through the Embedding Layer. Moreover, it adaptively amplifies
the self-supervised feature encoding capability as the training progresses. We
conducted a linear classification test of this method based on the classic
contrastive learning framework on ImageNet-1k. The empirical findings
illustrate that our approach achieves state-of-the-art performance under
equitable comparisons. Benefiting from its "plug-and-play" characteristics, we
further explored other contrastive learning methods. On the ImageNet-100,
compared to the original performance, the top1 has seen a maximum increase of
1.25%. We suggest that the proposed method may contribute to the advancement of
data-driven self-supervised learning research, bringing a fresh perspective to
this community.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 2 figures, the code of this paper will be released soon</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLaFS: When Large Language Models Meet Few-Shot Segmentation <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.16926v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.16926v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lanyun Zhu, Tianrun Chen, Deyi Ji, Jieping Ye, Jun Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes LLaFS, the first attempt to leverage large language
models (LLMs) in few-shot segmentation. In contrast to the conventional
few-shot segmentation methods that only rely on the limited and biased
information from the annotated support images, LLaFS leverages the vast prior
knowledge gained by LLM as an effective supplement and directly uses the LLM to
segment images in a few-shot manner. To enable the text-based LLM to handle
image-related tasks, we carefully design an input instruction that allows the
LLM to produce segmentation results represented as polygons, and propose a
region-attribute table to simulate the human visual mechanism and provide
multi-modal guidance. We also synthesize pseudo samples and use curriculum
learning for pretraining to augment data and achieve better optimization. LLaFS
achieves state-of-the-art results on multiple datasets, showing the potential
of using LLMs for few-shot computer vision tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EcoSense: Energy-Efficient Intelligent Sensing for In-Shore Ship
  Detection through Edge-Cloud Collaboration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14027v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14027v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenjun Huang, Hanning Chen, Yang Ni, Arghavan Rezvani, Sanggeon Yun, Sungheon Jeon, Eric Pedley, Mohsen Imani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting marine objects inshore presents challenges owing to algorithmic
intricacies and complexities in system deployment. We propose a
difficulty-aware edge-cloud collaborative sensing system that splits the task
into object localization and fine-grained classification. Objects are
classified either at the edge or within the cloud, based on their estimated
difficulty. The framework comprises a low-power device-tailored front-end model
for object localization, classification, and difficulty estimation, along with
a transformer-graph convolutional network-based back-end model for fine-grained
classification. Our system demonstrates superior performance (mAP@0.5 +4.3%})
on widely used marine object detection datasets, significantly reducing both
data transmission volume (by 95.43%) and energy consumption (by 72.7%}) at the
system level. We validate the proposed system across various embedded system
platforms and in real-world scenarios involving drone deployment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Vision <span class="highlight-title">Transformer</span>s with Hierarchical Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2106.03180v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2106.03180v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yun Liu, Yu-Huan Wu, Guolei Sun, Le Zhang, Ajad Chhatkuli, Luc Van Gool
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper tackles the high computational/space complexity associated with
Multi-Head Self-Attention (MHSA) in vanilla vision transformers. To this end,
we propose Hierarchical MHSA (H-MHSA), a novel approach that computes
self-attention in a hierarchical fashion. Specifically, we first divide the
input image into patches as commonly done, and each patch is viewed as a token.
Then, the proposed H-MHSA learns token relationships within local patches,
serving as local relationship modeling. Then, the small patches are merged into
larger ones, and H-MHSA models the global dependencies for the small number of
the merged tokens. At last, the local and global attentive features are
aggregated to obtain features with powerful representation capacity. Since we
only calculate attention for a limited number of tokens at each step, the
computational load is reduced dramatically. Hence, H-MHSA can efficiently model
global relationships among tokens without sacrificing fine-grained information.
With the H-MHSA module incorporated, we build a family of
Hierarchical-Attention-based Transformer Networks, namely HAT-Net. To
demonstrate the superiority of HAT-Net in scene understanding, we conduct
extensive experiments on fundamental vision tasks, including image
classification, semantic segmentation, object detection, and instance
segmentation. Therefore, HAT-Net provides a new perspective for vision
transformers. Code and pretrained models are available at
https://github.com/yun-liu/HAT-Net.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Machine Intelligence Research (MIR), DOI: 10.1007/s11633-024-1393-8</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Domain-Aware Fine-Tuning: Enhancing Neural Network Adaptability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.07728v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.07728v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seokhyeon Ha, Sunbeom Jung, Jungwoo Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning pre-trained neural network models has become a widely adopted
approach across various domains. However, it can lead to the distortion of
pre-trained feature extractors that already possess strong generalization
capabilities. Mitigating feature distortion during adaptation to new target
domains is crucial. Recent studies have shown promising results in handling
feature distortion by aligning the head layer on in-distribution datasets
before performing fine-tuning. Nonetheless, a significant limitation arises
from the treatment of batch normalization layers during fine-tuning, leading to
suboptimal performance. In this paper, we propose Domain-Aware Fine-Tuning
(DAFT), a novel approach that incorporates batch normalization conversion and
the integration of linear probing and fine-tuning. Our batch normalization
conversion method effectively mitigates feature distortion by reducing
modifications to the neural network during fine-tuning. Additionally, we
introduce the integration of linear probing and fine-tuning to optimize the
head layer with gradual adaptation of the feature extractor. By leveraging
batch normalization layers and integrating linear probing and fine-tuning, our
DAFT significantly mitigates feature distortion and achieves improved model
performance on both in-distribution and out-of-distribution datasets. Extensive
experiments demonstrate that our method outperforms other baseline methods,
demonstrating its effectiveness in not only improving performance but also
mitigating feature distortion.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NeuS-PIR: Learning Relightable Neural Surface using Pre-Integrated
  Rendering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.07632v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.07632v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shi Mao, Chenming Wu, Zhelun Shen, Yifan Wang, Dayan Wu, Liangjun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a method, namely NeuS-PIR, for recovering relightable
neural surfaces using pre-integrated rendering from multi-view images or video.
Unlike methods based on NeRF and discrete meshes, our method utilizes implicit
neural surface representation to reconstruct high-quality geometry, which
facilitates the factorization of the radiance field into two components: a
spatially varying material field and an all-frequency lighting representation.
This factorization, jointly optimized using an adapted differentiable
pre-integrated rendering framework with material encoding regularization, in
turn addresses the ambiguity of geometry reconstruction and leads to better
disentanglement and refinement of each scene property. Additionally, we
introduced a method to distil indirect illumination fields from the learned
representations, further recovering the complex illumination effect like
inter-reflection. Consequently, our method enables advanced applications such
as relighting, which can be seamlessly integrated with modern graphics engines.
Qualitative and quantitative experiments have shown that NeuS-PIR outperforms
existing methods across various tasks on both synthetic and real datasets.
Source code is available at https://github.com/Sheldonmao/NeuSPIR
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Gaze-guided Hand-Object Interaction Synthesis: Benchmark and Method 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16169v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16169v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jie Tian, Lingxiao Yang, Ran Ji, Yuexin Ma, Lan Xu, Jingyi Yu, Ye Shi, Jingya Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gaze plays a crucial role in revealing human attention and intention,
shedding light on the cognitive processes behind human actions. The integration
of gaze guidance with the dynamics of hand-object interactions boosts the
accuracy of human motion prediction. However, the lack of datasets that capture
the intricate relationship and consistency among gaze, hand, and object
movements remains a substantial hurdle. In this paper, we introduce the first
Gaze-guided Hand-Object Interaction dataset, GazeHOI, and present a novel task
for synthesizing gaze-guided hand-object interactions. Our dataset, GazeHOI,
features simultaneous 3D modeling of gaze, hand, and object interactions,
comprising 479 sequences with an average duration of 19.1 seconds, 812
sub-sequences, and 33 objects of various sizes. We propose a hierarchical
framework centered on a gaze-guided hand-object interaction diffusion model,
named GHO-Diffusion. In the pre-diffusion phase, we separate gaze conditions
into spatial-temporal features and goal pose conditions at different levels of
information granularity. During the diffusion phase, two gaze-conditioned
diffusion models are stacked to simplify the complex synthesis of hand-object
motions. Here, the object motion diffusion model generates sequences of object
motions based on gaze conditions, while the hand motion diffusion model
produces hand motions based on the generated object motion. To improve
fine-grained goal pose alignment, we introduce a Spherical Gaussian constraint
to guide the denoising step. In the subsequent post-diffusion phase, we
optimize the generated hand motions using contact consistency. Our extensive
experiments highlight the uniqueness of our dataset and the effectiveness of
our approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning-based Axial Video Motion Magnification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.09551v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.09551v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kwon Byung-Ki, Oh Hyun-Bin, Kim Jun-Seong, Hyunwoo Ha, Tae-Hyun Oh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video motion magnification amplifies invisible small motions to be
perceptible, which provides humans with a spatially dense and holistic
understanding of small motions in the scene of interest. This is based on the
premise that magnifying small motions enhances the legibility of motions. In
the real world, however, vibrating objects often possess convoluted systems
that have complex natural frequencies, modes, and directions. Existing motion
magnification often fails to improve legibility since the intricate motions
still retain complex characteristics even after being magnified, which may
distract us from analyzing them. In this work, we focus on improving legibility
by proposing a new concept, axial motion magnification, which magnifies
decomposed motions along the user-specified direction. Axial motion
magnification can be applied to various applications where motions of specific
axes are critical, by providing simplified and easily readable motion
information. To achieve this, we propose a novel Motion Separation Module that
enables to disentangle and magnify the motion representation along axes of
interest. Furthermore, we build a new synthetic training dataset for the axial
motion magnification task. Our proposed method improves the legibility of
resulting motions along certain axes by adding a new feature: user
controllability. Axial motion magnification is a more generalized concept;
thus, our method can be directly adapted to the generic motion magnification
and achieves favorable performance against competing methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>main paper: 12 pages, supplementary: 10 pages, 20 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Search and Society: Reimagining Information Access for Radical Futures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17901v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17901v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bhaskar Mitra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Information retrieval (IR) technologies and research are undergoing
transformative changes. It is our perspective that the community should accept
this opportunity to re-center our research agendas on societal needs while
dismantling the artificial separation between the work on fairness,
accountability, transparency, and ethics in IR and the rest of IR research.
Instead of adopting a reactionary strategy of trying to mitigate potential
social harms from emerging technologies, the community should aim to
proactively set the research agenda for the kinds of systems we should build
inspired by diverse explicitly stated sociotechnical imaginaries. The
sociotechnical imaginaries that underpin the design and development of
information access technologies needs to be explicitly articulated, and we need
to develop theories of change in context of these diverse perspectives. Our
guiding future imaginaries must be informed by other academic fields, such as
democratic theory and critical theory, and should be co-developed with social
science scholars, legal scholars, civil rights and social justice activists,
and artists, among others. In this perspective paper, we motivate why the
community must consider this radical shift in how we do research and what we
work on, and sketch a path forward towards this transformation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MIND Your Language: A Multilingual <span class="highlight-title">Dataset</span> for Cross-lingual News
  Recommendation <span class="chip">SIGIR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17876v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17876v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andreea Iana, Goran Glavaš, Heiko Paulheim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Digital news platforms use news recommenders as the main instrument to cater
to the individual information needs of readers. Despite an increasingly
language-diverse online community, in which many Internet users consume news in
multiple languages, the majority of news recommendation focuses on major,
resource-rich languages, and English in particular. Moreover, nearly all news
recommendation efforts assume monolingual news consumption, whereas more and
more users tend to consume information in at least two languages. Accordingly,
the existing body of work on news recommendation suffers from a lack of
publicly available multilingual benchmarks that would catalyze development of
news recommenders effective in multilingual settings and for low-resource
languages. Aiming to fill this gap, we introduce xMIND, an open, multilingual
news recommendation dataset derived from the English MIND dataset using machine
translation, covering a set of 14 linguistically and geographically diverse
languages, with digital footprints of varying sizes. Using xMIND, we
systematically benchmark several state-of-the-art content-based neural news
recommenders (NNRs) in both zero-shot (ZS-XLT) and few-shot (FS-XLT)
cross-lingual transfer scenarios, considering both monolingual and bilingual
news consumption patterns. Our findings reveal that (i) current NNRs, even when
based on a multilingual language model, suffer from substantial performance
losses under ZS-XLT and that (ii) inclusion of target-language data in FS-XLT
training has limited benefits, particularly when combined with a bilingual news
consumption. Our findings thus warrant a broader research effort in
multilingual and cross-lingual news recommendation. The xMIND dataset is
available at https://github.com/andreeaiana/xMIND.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the 47th International ACM SIGIR Conference on Research
  and Development in Information Retrieval (SIGIR 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ArabicaQA: A Comprehensive <span class="highlight-title">Dataset</span> for Arabic Question Answering <span class="chip">SIGIR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17848v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17848v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdelrahman Abdallah, Mahmoud Kasem, Mahmoud Abdalla, Mohamed Mahmoud, Mohamed Elkasaby, Yasser Elbendary, Adam Jatowt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we address the significant gap in Arabic natural language
processing (NLP) resources by introducing ArabicaQA, the first large-scale
dataset for machine reading comprehension and open-domain question answering in
Arabic. This comprehensive dataset, consisting of 89,095 answerable and 3,701
unanswerable questions created by crowdworkers to look similar to answerable
ones, along with additional labels of open-domain questions marks a crucial
advancement in Arabic NLP resources. We also present AraDPR, the first dense
passage retrieval model trained on the Arabic Wikipedia corpus, specifically
designed to tackle the unique challenges of Arabic text retrieval. Furthermore,
our study includes extensive benchmarking of large language models (LLMs) for
Arabic question answering, critically evaluating their performance in the
Arabic language context. In conclusion, ArabicaQA, AraDPR, and the benchmarking
of LLMs in Arabic question answering offer significant advancements in the
field of Arabic NLP. The dataset and code are publicly accessible for further
research https://github.com/DataScienceUIBK/ArabicaQA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at SIGIR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CaseLink: Inductive Graph Learning for Legal Case Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17780v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17780v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanran Tang, Ruihong Qiu, Hongzhi Yin, Xue Li, Zi Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In case law, the precedents are the relevant cases that are used to support
the decisions made by the judges and the opinions of lawyers towards a given
case. This relevance is referred to as the case-to-case reference relation. To
efficiently find relevant cases from a large case pool, retrieval tools are
widely used by legal practitioners. Existing legal case retrieval models mainly
work by comparing the text representations of individual cases. Although they
obtain a decent retrieval accuracy, the intrinsic case connectivity
relationships among cases have not been well exploited for case encoding,
therefore limiting the further improvement of retrieval performance. In a case
pool, there are three types of case connectivity relationships: the case
reference relationship, the case semantic relationship, and the case legal
charge relationship. Due to the inductive manner in the task of legal case
retrieval, using case reference as input is not applicable for testing. Thus,
in this paper, a CaseLink model based on inductive graph learning is proposed
to utilise the intrinsic case connectivity for legal case retrieval, a novel
Global Case Graph is incorporated to represent both the case semantic
relationship and the case legal charge relationship. A novel contrastive
objective with a regularisation on the degree of case nodes is proposed to
leverage the information carried by the case reference relationship to optimise
the model. Extensive experiments have been conducted on two benchmark datasets,
which demonstrate the state-of-the-art performance of CaseLink. The code has
been released on https://github.com/yanran-tang/CaseLink.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TWOLAR: a TWO-step LLM-Augmented distillation method for passage
  Reranking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17759v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17759v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Davide Baldelli, Junfeng Jiang, Akiko Aizawa, Paolo Torroni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present TWOLAR: a two-stage pipeline for passage reranking
based on the distillation of knowledge from Large Language Models (LLM). TWOLAR
introduces a new scoring strategy and a distillation process consisting in the
creation of a novel and diverse training dataset. The dataset consists of 20K
queries, each associated with a set of documents retrieved via four distinct
retrieval methods to ensure diversity, and then reranked by exploiting the
zero-shot reranking capabilities of an LLM. Our ablation studies demonstrate
the contribution of each new component we introduced. Our experimental results
show that TWOLAR significantly enhances the document reranking ability of the
underlying model, matching and in some cases even outperforming
state-of-the-art models with three orders of magnitude more parameters on the
TREC-DL test sets and the zero-shot evaluation benchmark BEIR. To facilitate
future work we release our data set, finetuned models, and code.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ All-in-One: Heterogeneous Interaction Modeling for Cold-Start Rating
  Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17740v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17740v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuheng Fang, Kangfei Zhao, Yu Rong, Zhixun Li, Jeffrey Xu Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cold-start rating prediction is a fundamental problem in recommender systems
that has been extensively studied. Many methods have been proposed that exploit
explicit relations among existing data, such as collaborative filtering, social
recommendations and heterogeneous information network, to alleviate the data
insufficiency issue for cold-start users and items. However, the explicit
relations constructed based on data between different roles may be unreliable
and irrelevant, which limits the performance ceiling of the specific
recommendation task. Motivated by this, in this paper, we propose a flexible
framework dubbed heterogeneous interaction rating network (HIRE). HIRE dose not
solely rely on the pre-defined interaction pattern or the manually constructed
heterogeneous information network. Instead, we devise a Heterogeneous
Interaction Module (HIM) to jointly model the heterogeneous interactions and
directly infer the important interactions via the observed data. In the
experiments, we evaluate our model under three cold-start settings on three
real-world datasets. The experimental results show that HIRE outperforms other
baselines by a large margin. Furthermore, we visualize the inferred
interactions of HIRE to confirm the contribution of our model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EulerFormer: Sequential User Behavior Modeling with Complex Vector
  Attention <span class="chip">SIGIR'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17729v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17729v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhen Tian, Wayne Xin Zhao, Changwang Zhang, Xin Zhao, Zhongrui Ma, Ji-Rong Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To capture user preference, transformer models have been widely applied to
model sequential user behavior data. The core of transformer architecture lies
in the self-attention mechanism, which computes the pairwise attention scores
in a sequence. Due to the permutation-equivariant nature, positional encoding
is used to enhance the attention between token representations. In this
setting, the pairwise attention scores can be derived by both semantic
difference and positional difference. However, prior studies often model the
two kinds of difference measurements in different ways, which potentially
limits the expressive capacity of sequence modeling. To address this issue,
this paper proposes a novel transformer variant with complex vector attention,
named EulerFormer, which provides a unified theoretical framework to formulate
both semantic difference and positional difference. The EulerFormer involves
two key technical improvements. First, it employs a new transformation function
for efficiently transforming the sequence tokens into polar-form complex
vectors using Euler's formula, enabling the unified modeling of both semantic
and positional information in a complex rotation form.Secondly, it develops a
differential rotation mechanism, where the semantic rotation angles can be
controlled by an adaptation function, enabling the adaptive integration of the
semantic and positional information according to the semantic
contexts.Furthermore, a phase contrastive learning task is proposed to improve
the anisotropy of contextual representations in EulerFormer. Our theoretical
framework possesses a high degree of completeness and generality. It is more
robust to semantic variations and possesses moresuperior theoretical properties
in principle. Extensive experiments conducted on four public datasets
demonstrate the effectiveness and efficiency of our approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in SIGIR'24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Language Models Enhanced Collaborative Filtering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17688v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17688v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhongxiang Sun, Zihua Si, Xiaoxue Zang, Kai Zheng, Yang Song, Xiao Zhang, Jun Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in Large Language Models (LLMs) have attracted
considerable interest among researchers to leverage these models to enhance
Recommender Systems (RSs). Existing work predominantly utilizes LLMs to
generate knowledge-rich texts or utilizes LLM-derived embeddings as features to
improve RSs. Al- though the extensive world knowledge embedded in LLMs
generally benefits RSs, the application can only take limited number of users
and items as inputs, without adequately exploiting collaborative filtering
information. Considering its crucial role in RSs, one key challenge in
enhancing RSs with LLMs lies in providing better collaborative filtering
information through LLMs. In this paper, drawing inspiration from the
in-context learning and chain of thought reasoning in LLMs, we propose the
Large Language Models enhanced Collaborative Filtering (LLM-CF) framework,
which distils the world knowledge and reasoning capabilities of LLMs into
collaborative filtering. We also explored a concise and efficient
instruction-tuning method, which improves the recommendation capabilities of
LLMs while preserving their general functionalities (e.g., not decreasing on
the LLM benchmark). Comprehensive experiments on three real-world datasets
demonstrate that LLM-CF significantly enhances several backbone recommendation
models and consistently outperforms competitive baselines, showcasing its
effectiveness in distilling the world knowledge and reasoning capabilities of
LLM into collaborative filtering.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ S+t-SNE - Bringing dimensionality reduction to data streams 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17643v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17643v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pedro C. Vieira, João P. Montrezol, João T. Vieira, João Gama
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present S+t-SNE, an adaptation of the t-SNE algorithm designed to handle
infinite data streams. The core idea behind S+t-SNE is to update the t-SNE
embedding incrementally as new data arrives, ensuring scalability and
adaptability to handle streaming scenarios. By selecting the most important
points at each step, the algorithm ensures scalability while keeping
informative visualisations. Employing a blind method for drift management
adjusts the embedding space, facilitating continuous visualisation of evolving
data dynamics. Our experimental evaluations demonstrate the effectiveness and
efficiency of S+t-SNE. The results highlight its ability to capture patterns in
a streaming scenario. We hope our approach offers researchers and practitioners
a real-time tool for understanding and interpreting high-dimensional data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This preprint has not undergone peer review or any post-submission
  improvements or corrections. We will soon add a link to the final version of
  this contribution that underwent peer-review and post-acceptance improvements
  and was presented at IDA2024 (https://ida2024.org/)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Retentive Decision <span class="highlight-title">Transformer</span> with Adaptive Masking for Reinforcement
  Learning based Recommendation Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17634v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17634v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siyu Wang, Xiaocong Chen, Lina Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning-based Recommender Systems (RLRS) have shown promise
across a spectrum of applications, from e-commerce platforms to streaming
services. Yet, they grapple with challenges, notably in crafting reward
functions and harnessing large pre-existing datasets within the RL framework.
Recent advancements in offline RLRS provide a solution for how to address these
two challenges. However, existing methods mainly rely on the transformer
architecture, which, as sequence lengths increase, can introduce challenges
associated with computational resources and training costs. Additionally, the
prevalent methods employ fixed-length input trajectories, restricting their
capacity to capture evolving user preferences. In this study, we introduce a
new offline RLRS method to deal with the above problems. We reinterpret the
RLRS challenge by modeling sequential decision-making as an inference task,
leveraging adaptive masking configurations. This adaptive approach selectively
masks input tokens, transforming the recommendation task into an inference
challenge based on varying token subsets, thereby enhancing the agent's ability
to infer across diverse trajectory lengths. Furthermore, we incorporate a
multi-scale segmented retention mechanism that facilitates efficient modeling
of long sequences, significantly enhancing computational efficiency. Our
experimental analysis, conducted on both online simulator and offline datasets,
clearly demonstrates the advantages of our proposed method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ END4Rec: Efficient Noise-Decoupling for Multi-Behavior Sequential
  Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17603v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17603v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongqiang Han, Hao Wang, Kefan Wang, Likang Wu, Zhi Li, Wei Guo, Yong Liu, Defu Lian, Enhong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recommendation systems, users frequently engage in multiple types of
behaviors, such as clicking, adding to a cart, and purchasing. However, with
diversified behavior data, user behavior sequences will become very long in the
short term, which brings challenges to the efficiency of the sequence
recommendation model. Meanwhile, some behavior data will also bring inevitable
noise to the modeling of user interests. To address the aforementioned issues,
firstly, we develop the Efficient Behavior Sequence Miner (EBM) that
efficiently captures intricate patterns in user behavior while maintaining low
time complexity and parameter count. Secondly, we design hard and soft
denoising modules for different noise types and fully explore the relationship
between behaviors and noise. Finally, we introduce a contrastive loss function
along with a guided training strategy to compare the valid information in the
data with the noisy signal, and seamlessly integrate the two denoising
processes to achieve a high degree of decoupling of the noisy signal.
Sufficient experiments on real-world datasets demonstrate the effectiveness and
efficiency of our approach in dealing with multi-behavior sequential
recommendation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Document Set Expansion with Positive-Unlabelled Learning Using
  Intractable Density Estimation <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17473v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17473v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haiyang Zhang, Qiuyi Chen, Yuanjie Zou, Yushan Pan, Jia Wang, Mark Stevenson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Document Set Expansion (DSE) task involves identifying relevant documents
from large collections based on a limited set of example documents. Previous
research has highlighted Positive and Unlabeled (PU) learning as a promising
approach for this task. However, most PU methods rely on the unrealistic
assumption of knowing the class prior for positive samples in the collection.
To address this limitation, this paper introduces a novel PU learning framework
that utilizes intractable density estimation models. Experiments conducted on
PubMed and Covid datasets in a transductive setting showcase the effectiveness
of the proposed method for DSE. Code is available from
https://github.com/Beautifuldog01/Document-set-expansion-puDE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-COLING 2024. arXiv admin note: text overlap with
  arXiv:2401.11145</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Touch the Core: Exploring Task Dependence Among Hybrid Targets for
  Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17442v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17442v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xing Tang, Yang Qiao, Fuyuan Lyu, Dugang Liu, Xiuqiang He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As user behaviors become complicated on business platforms, online
recommendations focus more on how to touch the core conversions, which are
highly related to the interests of platforms. These core conversions are
usually continuous targets, such as \textit{watch time}, \textit{revenue}, and
so on, whose predictions can be enhanced by previous discrete conversion
actions. Therefore, multi-task learning (MTL) can be adopted as the paradigm to
learn these hybrid targets. However, existing works mainly emphasize
investigating the sequential dependence among discrete conversion actions,
which neglects the complexity of dependence between discrete conversions and
the final continuous conversion. Moreover, simultaneously optimizing hybrid
tasks with stronger task dependence will suffer from volatile issues where the
core regression task might have a larger influence on other tasks. In this
paper, we study the MTL problem with hybrid targets for the first time and
propose the model named Hybrid Targets Learning Network (HTLNet) to explore
task dependence and enhance optimization. Specifically, we introduce label
embedding for each task to explicitly transfer the label information among
these tasks, which can effectively explore logical task dependence. We also
further design the gradient adjustment regime between the final regression task
and other classification tasks to enhance the optimization. Extensive
experiments on two offline public datasets and one real-world industrial
dataset are conducted to validate the effectiveness of HTLNet. Moreover, online
A/B tests on the financial recommender system also show our model has superior
improvement.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Masked Multi-Domain Network: Multi-Type and Multi-Scenario Conversion
  Rate Prediction with a Single Model <span class="chip">CIKM 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17425v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17425v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wentao Ouyang, Xiuwu Zhang, Chaofeng Guo, Shukui Ren, Yupei Sui, Kun Zhang, Jinmei Luo, Yunfeng Chen, Dongbo Xu, Xiangzheng Liu, Yanlong Du
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In real-world advertising systems, conversions have different types in nature
and ads can be shown in different display scenarios, both of which highly
impact the actual conversion rate (CVR). This results in the multi-type and
multi-scenario CVR prediction problem. A desired model for this problem should
satisfy the following requirements: 1) Accuracy: the model should achieve
fine-grained accuracy with respect to any conversion type in any display
scenario. 2) Scalability: the model parameter size should be affordable. 3)
Convenience: the model should not require a large amount of effort in data
partitioning, subset processing and separate storage. Existing approaches
cannot simultaneously satisfy these requirements. For example, building a
separate model for each (conversion type, display scenario) pair is neither
scalable nor convenient. Building a unified model trained on all the data with
conversion type and display scenario included as two features is not accurate
enough. In this paper, we propose the Masked Multi-domain Network (MMN) to
solve this problem. To achieve the accuracy requirement, we model
domain-specific parameters and propose a dynamically weighted loss to account
for the loss scale imbalance issue within each mini-batch. To achieve the
scalability requirement, we propose a parameter sharing and composition
strategy to reduce model parameters from a product space to a sum space. To
achieve the convenience requirement, we propose an auto-masking strategy which
can take mixed data from all the domains as input. It avoids the overhead
caused by data partitioning, individual processing and separate storage. Both
offline and online experimental results validate the superiority of MMN for
multi-type and multi-scenario CVR prediction. MMN is now the serving model for
real-time CVR prediction in UC Toutiao.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CIKM 2023 (larger figures)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MA4DIV: Multi-Agent Reinforcement Learning for Search Result
  Diversification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17421v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17421v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiqun Chen, Jiaxin Mao, Yi Zhang, Dehong MA, Long Xia, Jun Fan, Daiting Shi, Zhicong Cheng, Dawei Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The objective of search result diversification (SRD) is to ensure that
selected documents cover as many different subtopics as possible. Existing
methods primarily utilize a paradigm of "greedy selection", i.e., selecting one
document with the highest diversity score at a time. These approaches tend to
be inefficient and are easily trapped in a suboptimal state. In addition, some
other methods aim to approximately optimize the diversity metric, such as
$\alpha$-NDCG, but the results still remain suboptimal. To address these
challenges, we introduce Multi-Agent reinforcement learning (MARL) for search
result DIVersity, which called MA4DIV. In this approach, each document is an
agent and the search result diversification is modeled as a cooperative task
among multiple agents. This approach allows for directly optimizing the
diversity metrics, such as $\alpha$-NDCG, while achieving high training
efficiency. We conducted preliminary experiments on public TREC datasets to
demonstrate the effectiveness and potential of MA4DIV. Considering the limited
number of queries in public TREC datasets, we construct a large-scale dataset
from industry sources and show that MA4DIV achieves substantial improvements in
both effectiveness and efficiency than existing baselines on a industrial scale
dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AFDGCF: Adaptive Feature De-correlation Graph Collaborative Filtering
  for Recommendations <span class="chip">SIGIR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17416v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17416v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Wu, Chao Wang, Dazhong Shen, Chuan Qin, Liyi Chen, Hui Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Collaborative filtering methods based on graph neural networks (GNNs) have
witnessed significant success in recommender systems (RS), capitalizing on
their ability to capture collaborative signals within intricate user-item
relationships via message-passing mechanisms. However, these GNN-based RS
inadvertently introduce excess linear correlation between user and item
embeddings, contradicting the goal of providing personalized recommendations.
While existing research predominantly ascribes this flaw to the over-smoothing
problem, this paper underscores the critical, often overlooked role of the
over-correlation issue in diminishing the effectiveness of GNN representations
and subsequent recommendation performance. Up to now, the over-correlation
issue remains unexplored in RS. Meanwhile, how to mitigate the impact of
over-correlation while preserving collaborative filtering signals is a
significant challenge. To this end, this paper aims to address the
aforementioned gap by undertaking a comprehensive study of the over-correlation
issue in graph collaborative filtering models. Firstly, we present empirical
evidence to demonstrate the widespread prevalence of over-correlation in these
models. Subsequently, we dive into a theoretical analysis which establishes a
pivotal connection between the over-correlation and over-smoothing issues.
Leveraging these insights, we introduce the Adaptive Feature De-correlation
Graph Collaborative Filtering (AFDGCF) framework, which dynamically applies
correlation penalties to the feature dimensions of the representation matrix,
effectively alleviating both over-correlation and over-smoothing issues. The
efficacy of the proposed framework is corroborated through extensive
experiments conducted with four representative graph collaborative filtering
models across four publicly available datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by SIGIR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Domain Recommendation to Attract Users via Domain Preference
  Modeling <span class="chip">AAAI'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17374v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17374v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyuunjun Ju, SeongKu Kang, Dongha Lee, Junyoung Hwang, Sanghwan Jang, Hwanjo Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, web platforms have been operating various service domains
simultaneously. Targeting a platform that operates multiple service domains, we
introduce a new task, Multi-Domain Recommendation to Attract Users (MDRAU),
which recommends items from multiple ``unseen'' domains with which each user
has not interacted yet, by using knowledge from the user's ``seen'' domains. In
this paper, we point out two challenges of MDRAU task. First, there are
numerous possible combinations of mappings from seen to unseen domains because
users have usually interacted with a different subset of service domains.
Second, a user might have different preferences for each of the target unseen
domains, which requires that recommendations reflect the user's preferences on
domains as well as items. To tackle these challenges, we propose DRIP framework
that models users' preferences at two levels (i.e., domain and item) and learns
various seen-unseen domain mappings in a unified way with masked domain
modeling. Our extensive experiments demonstrate the effectiveness of DRIP in
MDRAU task and its ability to capture users' domain-level preferences.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to AAAI'24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Empirical Study of Training ID-Agnostic Multi-modal Sequential
  Recommenders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17372v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17372v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Youhua Li, Hanwen Du, Yongxin Ni, Yuanqi He, Junchen Fu, Xiangyan Liu, Qi Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequential Recommendation (SR) aims to predict future user-item interactions
based on historical interactions. While many SR approaches concentrate on user
IDs and item IDs, the human perception of the world through multi-modal
signals, like text and images, has inspired researchers to delve into
constructing SR from multi-modal information without using IDs. However, the
complexity of multi-modal learning manifests in diverse feature extractors,
fusion methods, and pre-trained models. Consequently, designing a simple and
universal \textbf{M}ulti-\textbf{M}odal \textbf{S}equential
\textbf{R}ecommendation (\textbf{MMSR}) framework remains a formidable
challenge. We systematically summarize the existing multi-modal related SR
methods and distill the essence into four core components: visual encoder, text
encoder, multimodal fusion module, and sequential architecture. Along these
dimensions, we dissect the model designs, and answer the following
sub-questions: First, we explore how to construct MMSR from scratch, ensuring
its performance either on par with or exceeds existing SR methods without
complex techniques. Second, we examine if MMSR can benefit from existing
multi-modal pre-training paradigms. Third, we assess MMSR's capability in
tackling common challenges like cold start and domain transferring. Our
experiment results across four real-world recommendation scenarios demonstrate
the great potential ID-agnostic multi-modal sequential recommendation. Our
framework can be found at: https://github.com/MMSR23/MMSR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>An Empirical Study of Training ID-Agnostic Multi-modal Sequential
  Recommenders</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cognitively Biased Users Interacting with Algorithmically Biased Results
  in Whole-Session Search on Controversial Topics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17286v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17286v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ben Wang, Jiqun Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When interacting with information retrieval (IR) systems, users, affected by
confirmation biases, tend to select search results that confirm their existing
beliefs on socially significant contentious issues. To understand the judgments
and attitude changes of users searching online, our study examined how
cognitively biased users interact with algorithmically biased search engine
result pages (SERPs). We designed three-query search sessions on debated topics
under various bias conditions. We recruited 1,321 crowdsourcing participants
and explored their attitude changes, search interactions, and the effects of
confirmation bias. Three key findings emerged: 1) most attitude changes occur
in the initial query of a search session; 2) confirmation bias and result
presentation on SERPs affect search behaviors in the current query and
perceived familiarity with clicked results in subsequent queries. The bias
position also affect attitude changes of users with lower perceived openness to
conflicting opinions; 3) Interactions in the first query and and dwell time
throughout the session are associated with users' attitude changes in different
forms. Our study goes beyond traditional simulation-based evaluation settings
and simulated rational users, sheds light on the mixed effects of human biases
and algorithmic biases in controversial information retrieval tasks, and can
inform the design of bias-aware user models, human-centered bias mitigation
techniques, and socially responsible intelligent IR systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Decade of Scholarly Research on Open Knowledge Graphs <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.13186v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.13186v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Houcemeddine Turki, Abraham Toluwase Owodunni, Mohamed Ali Hadj Taieb, René Fabrice Bile, Mohamed Ben Aouicha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The proliferation of open knowledge graphs has led to a surge in scholarly
research on the topic over the past decade. This paper presents a bibliometric
analysis of the scholarly literature on open knowledge graphs published between
2013 and 2023. The study aims to identify the trends, patterns, and impact of
research in this field, as well as the key topics and research questions that
have emerged. The work uses bibliometric techniques to analyze a sample of 4445
scholarly articles retrieved from Scopus. The findings reveal an
ever-increasing number of publications on open knowledge graphs published every
year, particularly in developed countries (+50 per year). These outputs are
published in highly-referred scholarly journals and conferences. The study
identifies three main research themes: (1) knowledge graph construction and
enrichment, (2) evaluation and reuse, and (3) fusion of knowledge graphs into
NLP systems. Within these themes, the study identifies specific tasks that have
received considerable attention, including entity linking, knowledge graph
embedding, and graph neural networks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Camera-ready edition for LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unsupervised Large Language Model Alignment for Information Retrieval
  via Contrastive Feedback <span class="chip">SIGIR24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.17078v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.17078v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qian Dong, Yiding Liu, Qingyao Ai, Zhijing Wu, Haitao Li, Yiqun Liu, Shuaiqiang Wang, Dawei Yin, Shaoping Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated remarkable capabilities across
various research domains, including the field of Information Retrieval (IR).
However, the responses generated by off-the-shelf LLMs tend to be generic,
i.e., cannot capture the distinctiveness of each document with similar content.
This limits the performance of LLMs in IR because finding and distinguishing
relevant documents from substantial similar documents is a typical problem in
many IR tasks. To address this issue, we propose an unsupervised alignment
method, namely Reinforcement Learning from Contrastive Feedback (RLCF),
empowering LLMs to generate both high-quality and context-specific responses.
Our approach constructs unsupervised contrastive feedback signals based on
similar document groups, and adopts a reward function, named group-wise
reciprocal rank, to optimize LLMs within a standard Proximal Policy
Optimization. We conduct extensive experiments to evaluate the effectiveness of
RLCF on LLMs built with different languages and parameter sizes on multiple
downstream IR applications. RLCF significantly outperforms existing alignment
methods, and RLCF-optimized LLMs demonstrate considerable improvement in
generating responses with distinctiveness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by SIGIR24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Coarse-Tuning for Ad-hoc Document Retrieval Using <span class="highlight-title">Pre-train</span>ed Language
  Models <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16915v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16915v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Atsushi Keyaki, Ribeka Keyaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning in information retrieval systems using pre-trained language
models (PLM-based IR) requires learning query representations and
query-document relations, in addition to downstream task-specific learning.
This study introduces coarse-tuning as an intermediate learning stage that
bridges pre-training and fine-tuning. By learning query representations and
query-document relations in coarse-tuning, we aim to reduce the load of
fine-tuning and improve the learning effect of downstream IR tasks. We propose
Query-Document Pair Prediction (QDPP) for coarse-tuning, which predicts the
appropriateness of query-document pairs. Evaluation experiments show that the
proposed method significantly improves MRR and/or nDCG@5 in four ad-hoc
document retrieval datasets. Furthermore, the results of the query prediction
task suggested that coarse-tuning facilitated learning of query representation
and query-document relations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimizing Feature Set for Click-Through Rate Prediction <span class="chip">WWW 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.10909v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.10909v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fuyuan Lyu, Xing Tang, Dugang Liu, Liang Chen, Xiuqiang He, Xue Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Click-through prediction (CTR) models transform features into latent vectors
and enumerate possible feature interactions to improve performance based on the
input feature set. Therefore, when selecting an optimal feature set, we should
consider the influence of both feature and its interaction. However, most
previous works focus on either feature field selection or only select feature
interaction based on the fixed feature set to produce the feature set. The
former restricts search space to the feature field, which is too coarse to
determine subtle features. They also do not filter useless feature
interactions, leading to higher computation costs and degraded model
performance. The latter identifies useful feature interaction from all
available features, resulting in many redundant features in the feature set. In
this paper, we propose a novel method named OptFS to address these problems. To
unify the selection of feature and its interaction, we decompose the selection
of each feature interaction into the selection of two correlated features. Such
a decomposition makes the model end-to-end trainable given various feature
interaction operations. By adopting feature-level search space, we set a
learnable gate to determine whether each feature should be within the feature
set. Because of the large-scale search space, we develop a
learning-by-continuation training scheme to learn such gates. Hence, OptFS
generates the feature set only containing features which improve the final
prediction results. Experimentally, we evaluate OptFS on three public datasets,
demonstrating OptFS can optimize feature sets which enhance the model
performance and further reduce both the storage and computational cost.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by WWW 2023 Research Tracks</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Graph Signal Diffusion Model for Collaborative Filtering <span class="chip">SIGIR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.08744v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.08744v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunqin Zhu, Chao Wang, Qi Zhang, Hui Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Collaborative filtering is a critical technique in recommender systems. Among
various methods, an increasingly popular paradigm is to reconstruct user-item
interactions based on the historical observations. This can be viewed as a
conditional generative task, where recently developed diffusion model
demonstrates great potential. However, existing studies on diffusion models
lack effective solutions for modeling implicit feedback data. Particularly, the
isotropic nature of the standard diffusion process fails to account for the
heterogeneous dependencies among items, leading to a misalignment with the
graphical structure of the interaction space. Meanwhile, random noise
destroying personalized information in interaction vectors, causing difficulty
in reverse reconstruction. In this paper, we make novel adaptions of diffusion
model and propose Graph Signal Diffusion Model for Collaborative Filtering
(named GiffCF). To better represent the high-dimensional and sparse
distribution of implicit feedback, we define a generalized form of denoising
diffusion using heat equation on the item-item similarity graph. Our forward
process smooths interaction signals with an advanced family of graph filters.
Hence, instead of losing information, it involves item-item similarities as
beneficial prior knowledge for recommendation. To reconstruct high-quality
interactions, our reverse process iteratively refines and sharpens preference
signals in a deterministic manner, where the update direction is conditioned on
the user history and computed from a carefully designed two-stage denoiser.
Finally, through extensive experiments, we show that GiffCF effectively
leverages the advantages of both diffusion model and graph signal processing,
and achieves state-of-the-art performance on three benchmark datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 8 figures, Accepted by SIGIR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Understanding Performance of Long-Document Ranking Models through
  Comprehensive Evaluation and Leaderboarding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.01262v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.01262v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leonid Boytsov, David Akinpelu, Tianyi Lin, Fangwei Gao, Yutian Zhao, Jeffrey Huang, Eric Nyberg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We evaluated 20+ Transformer models for ranking of long documents (including
recent LongP models trained with FlashAttention) and compared them with simple
FirstP baselines (applying the same model to input truncated to the first 512
tokens). We used MS MARCO Documents v1 as a primary training set and evaluated
models in the zero-shot scenario as well as after fine-tuning on other
collections.
  In our initial experiments with standard collections we found that
long-document models underperformed FirstP or outperformed it by at most 5% on
average in terms of MRR or NDCG. We then conjectured that this was not due to
models inability to process long context but rather due to a positional bias of
relevant passages, which tended to be among the first 512 document tokens. We
found evidence that this bias was, indeed, present in at least two test sets,
which motivated us to create a new collection MS MARCO FarRelevant where the
relevant passages were not present among the first 512 tokens.
  Unlike standard collections where we observed both little benefit from
incorporating longer contexts and limited variability in model performance
(within a few %), experiments on MS MARCO FarRelevant uncovered dramatic
differences among models. FirstP models performed roughly at the
random-baseline level in both zero-shot and fine-tuning scenarios. Simple
aggregation models (e.g., MaxP) had good zero-shot accuracy but benefited
little from fine-tuning. Most other models had poor zero-shot performance
(sometimes at a random baseline level) but outstripped MaxP by as much 13-28\%
after finetuning. Thus, positional bias not only diminishes benefits of
processing longer document contexts but also leads to model overfitting to this
bias and performing poorly in a zero-shot setting when a distribution of
relevant passages changes substantially.
  We make our software and MS MARCO FarRelevant available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Take Care of Your <span class="highlight-title">Prompt</span> Bias! Investigating and Mitigating <span class="highlight-title">Prompt</span> Bias
  in Factual Knowledge Extraction <span class="chip">COLING 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09963v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09963v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyang Xu, Keqin Peng, Liang Ding, Dacheng Tao, Xiliang Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent research shows that pre-trained language models (PLMs) suffer from
"prompt bias" in factual knowledge extraction, i.e., prompts tend to introduce
biases toward specific labels. Prompt bias presents a significant challenge in
assessing the factual knowledge within PLMs. Therefore, this paper aims to
improve the reliability of existing benchmarks by thoroughly investigating and
mitigating prompt bias. We show that: 1) all prompts in the experiments exhibit
non-negligible bias, with gradient-based prompts like AutoPrompt and OptiPrompt
displaying significantly higher levels of bias; 2) prompt bias can amplify
benchmark accuracy unreasonably by overfitting the test datasets, especially on
imbalanced datasets like LAMA. Based on these findings, we propose a
representation-based approach to mitigate the prompt bias during inference
time. Specifically, we first estimate the biased representation using
prompt-only querying, and then remove it from the model's internal
representations to generate the debiased representations, which are used to
produce the final debiased outputs. Experiments across various prompts, PLMs,
and benchmarks show that our approach can not only correct the overfitted
performance caused by prompt bias, but also significantly improve the prompt
retrieval capability (up to 10% absolute performance gain). These results
indicate that our approach effectively alleviates prompt bias in knowledge
evaluation, thereby enhancing the reliability of benchmark assessments.
Hopefully, our plug-and-play approach can be a golden standard to strengthen
PLMs toward reliable knowledge bases. Code and data are released in
https://github.com/FelliYang/PromptBias.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Sequential Recommendations via Bidirectional Temporal Data
  Augmentation with <span class="highlight-title">Pre-train</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.06460v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.06460v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juyong Jiang, Peiyan Zhang, Yingtao Luo, Chaozhuo Li, Jaeboum Kim, Kai Zhang, Senzhang Wang, Sunghun Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequential recommendation systems are integral to discerning temporal user
preferences. Yet, the task of learning from abbreviated user interaction
sequences poses a notable challenge. Data augmentation has been identified as a
potent strategy to enhance the informational richness of these sequences.
Traditional augmentation techniques, such as item randomization, may disrupt
the inherent temporal dynamics. Although recent advancements in reverse
chronological pseudo-item generation have shown promise, they can introduce
temporal discrepancies when assessed in a natural chronological context. In
response, we introduce a sophisticated approach, Bidirectional temporal data
Augmentation with pre-training (BARec). Our approach leverages bidirectional
temporal augmentation and knowledge-enhanced fine-tuning to synthesize
authentic pseudo-prior items that \emph{retain user preferences and capture
deeper item semantic correlations}, thus boosting the model's expressive power.
Our comprehensive experimental analysis confirms the superiority of BARec
across both short and elongated sequence contexts. Moreover, theoretical
examination and visual representation of item embeddings offer further insight
into the model's logical processes and interpretability. The source code for
our study is available at
\textcolor{blue}{\href{https://github.com/juyongjiang/BARec}{https://github.com/juyongjiang/BARec}}.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SLEDGE: Synthesizing Simulation Environments for Driving Agents with
  Generative Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17933v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17933v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kashyap Chitta, Daniel Dauner, Andreas Geiger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  SLEDGE is the first generative simulator for vehicle motion planning trained
on real-world driving logs. Its core component is a learned model that is able
to generate agent bounding boxes and lane graphs. The model's outputs serve as
an initial state for traffic simulation. The unique properties of the entities
to be generated for SLEDGE, such as their connectivity and variable count per
scene, render the naive application of most modern generative models to this
task non-trivial. Therefore, together with a systematic study of existing lane
graph representations, we introduce a novel raster-to-vector autoencoder
(RVAE). It encodes agents and the lane graph into distinct channels in a
rasterized latent map. This facilitates both lane-conditioned agent generation
and combined generation of lanes and agents with a Diffusion Transformer. Using
generated entities in SLEDGE enables greater control over the simulation, e.g.
upsampling turns or increasing traffic density. Further, SLEDGE can support
500m long routes, a capability not found in existing data-driven simulators
like nuPlan. It presents new challenges for planning algorithms, evidenced by
failure rates of over 40% for PDM, the winner of the 2023 nuPlan challenge,
when tested on hard routes and dense traffic generated by our model. Compared
to nuPlan, SLEDGE requires 500$\times$ less storage to set up (<4GB), making it
a more accessible option and helping with democratizing future research in this
field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Need for Speed: Pruning <span class="highlight-title">Transformer</span>s with One Recipe <span class="chip">ICLR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17921v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17921v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samir Khaki, Konstantinos N. Plataniotis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce the $\textbf{O}$ne-shot $\textbf{P}$runing $\textbf{T}$echnique
for $\textbf{I}$nterchangeable $\textbf{N}$etworks ($\textbf{OPTIN}$) framework
as a tool to increase the efficiency of pre-trained transformer architectures
$\textit{without requiring re-training}$. Recent works have explored improving
transformer efficiency, however often incur computationally expensive
re-training procedures or depend on architecture-specific characteristics, thus
impeding practical wide-scale adoption. To address these shortcomings, the
OPTIN framework leverages intermediate feature distillation, capturing the
long-range dependencies of model parameters (coined $\textit{trajectory}$), to
produce state-of-the-art results on natural language, image classification,
transfer learning, and semantic segmentation tasks $\textit{without
re-training}$. Given a FLOP constraint, the OPTIN framework will compress the
network while maintaining competitive accuracy performance and improved
throughput. Particularly, we show a $\leq 2$% accuracy degradation from NLP
baselines and a $0.5$% improvement from state-of-the-art methods on image
classification at competitive FLOPs reductions. We further demonstrate the
generalization of tasks and architecture with comparative performance using
Mask2Former for semantic segmentation and cnn-style networks. OPTIN presents
one of the first one-shot efficient frameworks for compressing transformer
architectures that generalizes well across different class domains, in
particular: natural language and image-related tasks, without
$\textit{re-training}$.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in the International Conference on Learning Representations
  (ICLR) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LISA: Layerwise Importance Sampling for Memory-Efficient Large Language
  Model Fine-Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17919v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17919v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Pan, Xiang Liu, Shizhe Diao, Renjie Pi, Jipeng Zhang, Chi Han, Tong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The machine learning community has witnessed impressive advancements since
the first appearance of large language models (LLMs), yet their huge memory
consumption has become a major roadblock to large-scale training. Parameter
Efficient Fine-Tuning techniques such as Low-Rank Adaptation (LoRA) have been
proposed to alleviate this problem, but their performance still fails to match
full parameter training in most large-scale fine-tuning settings. Attempting to
complement this deficiency, we investigate layerwise properties of LoRA on
fine-tuning tasks and observe an uncommon skewness of weight norms across
different layers. Utilizing this key observation, a surprisingly simple
training strategy is discovered, which outperforms both LoRA and full parameter
training in a wide range of settings with memory costs as low as LoRA. We name
it Layerwise Importance Sampled AdamW (LISA), a promising alternative for LoRA,
which applies the idea of importance sampling to different layers in LLMs and
randomly freeze most middle layers during optimization. Experimental results
show that with similar or less GPU memory consumption, LISA surpasses LoRA or
even full parameter tuning in downstream fine-tuning tasks, where LISA
consistently outperforms LoRA by over $11\%$-$37\%$ in terms of MT-Bench
scores. On large models, specifically LLaMA-2-70B, LISA achieves on-par or
better performance than LoRA on MT-Bench, GSM8K, and PubMedQA, demonstrating
its effectiveness across different domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CMP: Cooperative Motion Prediction with Multi-Agent Communication 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17916v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17916v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuoyuan Wu, Yuping Wang, Hengbo Ma, Zhaowei Li, Hang Qiu, Jiachen Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The confluence of the advancement of Autonomous Vehicles (AVs) and the
maturity of Vehicle-to-Everything (V2X) communication has enabled the
capability of cooperative connected and automated vehicles (CAVs). Building on
top of cooperative perception, this paper explores the feasibility and
effectiveness of cooperative motion prediction. Our method, CMP, takes LiDAR
signals as input to enhance tracking and prediction capabilities. Unlike
previous work that focuses separately on either cooperative perception or
motion prediction, our framework, to the best of our knowledge, is the first to
address the unified problem where CAVs share information in both perception and
prediction modules. Incorporated into our design is the unique capability to
tolerate realistic V2X bandwidth limitations and transmission delays, while
dealing with bulky perception representations. We also propose a prediction
aggregation module, which unifies the predictions obtained by different CAVs
and generates the final prediction. Through extensive experiments and ablation
studies, we demonstrate the effectiveness of our method in cooperative
perception, tracking, and motion prediction tasks. In particular, CMP reduces
the average prediction error by 17.2\% with fewer missing detections compared
with the no cooperation setting. Our work marks a significant step forward in
the cooperative capabilities of CAVs, showcasing enhanced performance in
complex scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scalable Non-Cartesian Magnetic Resonance Imaging with R2D2 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17905v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17905v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Yiwei, Tang Chao, Aghabiglou Amir, Chu Chung San, Wiaux Yves
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a new approach for non-Cartesian magnetic resonance image
reconstruction. While unrolled architectures provide robustness via
data-consistency layers, embedding measurement operators in Deep Neural Network
(DNN) can become impractical at large scale. Alternative Plug-and-Play (PnP)
approaches, where the denoising DNNs are blind to the measurement setting, are
not affected by this limitation and have also proven effective, but their
highly iterative nature also affects scalability. To address this scalability
challenge, we leverage the "Residual-to-Residual DNN series for high-Dynamic
range imaging (R2D2)" approach recently introduced in astronomical imaging.
R2D2's reconstruction is formed as a series of residual images, iteratively
estimated as outputs of DNNs taking the previous iteration's image estimate and
associated data residual as inputs. The method can be interpreted as a learned
version of the Matching Pursuit algorithm. We demonstrate R2D2 in simulation,
considering radial k-space sampling acquisition sequences. Our preliminary
results suggest that R2D2 achieves: (i) suboptimal performance compared to its
unrolled incarnation R2D2-Net, which is however non-scalable due to the
necessary embedding of NUFFT-based data-consistency layers; (ii) superior
reconstruction quality to a scalable version of R2D2-Net embedding an FFT-based
approximation for data consistency; (iii) superior reconstruction quality to
PnP, while only requiring few iterations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to IEEE EUSIPCO 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Serpent: Scalable and Efficient Image Restoration via Multi-scale
  Structured State Space Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17902v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17902v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Shahab Sepehri, Zalan Fabian, Mahdi Soltanolkotabi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The landscape of computational building blocks of efficient image restoration
architectures is dominated by a combination of convolutional processing and
various attention mechanisms. However, convolutional filters are inherently
local and therefore struggle at modeling long-range dependencies in images. On
the other hand, attention excels at capturing global interactions between
arbitrary image regions, however at a quadratic cost in image dimension. In
this work, we propose Serpent, an architecture that leverages recent advances
in state space models (SSMs) in its core computational block. SSMs, originally
introduced for sequence modeling, can maintain a global receptive field with a
favorable linear scaling in input size. Our preliminary results demonstrate
that Serpent can achieve reconstruction quality on par with state-of-the-art
techniques, while requiring orders of magnitude less compute (up to $150$ fold
reduction in FLOPS) and a factor of up to $5\times$ less GPU memory while
maintaining a compact model size.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 5 figures, preliminary workshop submission of a
  comprehensive work to be released soon</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Image-based Novel Fault Detection with Deep Learning Classifiers using
  Hierarchical Labels 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17891v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17891v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nurettin Sergin, Jiayu Huang, Tzyy-Shuh Chang, Hao Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One important characteristic of modern fault classification systems is the
ability to flag the system when faced with previously unseen fault types. This
work considers the unknown fault detection capabilities of deep neural
network-based fault classifiers. Specifically, we propose a methodology on how,
when available, labels regarding the fault taxonomy can be used to increase
unknown fault detection performance without sacrificing model performance. To
achieve this, we propose to utilize soft label techniques to improve the
state-of-the-art deep novel fault detection techniques during the training
process and novel hierarchically consistent detection statistics for online
novel fault detection. Finally, we demonstrated increased detection performance
on novel fault detection in inspection images from the hot steel rolling
process, with results well replicated across multiple scenarios and baseline
detection methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in IISE Transaction</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large scale paired antibody language models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17889v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17889v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Henry Kenlay, Frédéric A. Dreyer, Aleksandr Kovaltsuk, Dom Miketa, Douglas Pires, Charlotte M. Deane
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Antibodies are proteins produced by the immune system that can identify and
neutralise a wide variety of antigens with high specificity and affinity, and
constitute the most successful class of biotherapeutics. With the advent of
next-generation sequencing, billions of antibody sequences have been collected
in recent years, though their application in the design of better therapeutics
has been constrained by the sheer volume and complexity of the data. To address
this challenge, we present IgBert and IgT5, the best performing
antibody-specific language models developed to date which can consistently
handle both paired and unpaired variable region sequences as input. These
models are trained comprehensively using the more than two billion unpaired
sequences and two million paired sequences of light and heavy chains present in
the Observed Antibody Space dataset. We show that our models outperform
existing antibody and protein language models on a diverse range of design and
regression tasks relevant to antibody engineering. This advancement marks a
significant leap forward in leveraging machine learning, large scale data sets
and high-performance computing for enhancing antibody design for therapeutic
development.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 2 figures, 6 tables, model weights available at
  https://zenodo.org/doi/10.5281/zenodo.10876908</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Unreasonable Ineffectiveness of the Deeper Layers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17887v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17887v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrey Gromov, Kushal Tirumala, Hassan Shapourian, Paolo Glorioso, Daniel A. Roberts
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We empirically study a simple layer-pruning strategy for popular families of
open-weight pretrained LLMs, finding minimal degradation of performance on
different question-answering benchmarks until after a large fraction (up to
half) of the layers are removed. To prune these models, we identify the optimal
block of layers to prune by considering similarity across layers; then, to
"heal" the damage, we perform a small amount of finetuning. In particular, we
use parameter-efficient finetuning (PEFT) methods, specifically quantization
and Low Rank Adapters (QLoRA), such that each of our experiments can be
performed on a single A100 GPU. From a practical perspective, these results
suggest that layer pruning methods can complement other PEFT strategies to
further reduce computational resources of finetuning on the one hand, and can
improve the memory and latency of inference on the other hand. From a
scientific perspective, the robustness of these LLMs to the deletion of layers
implies either that current pretraining methods are not properly leveraging the
parameters in the deeper layers of the network or that the shallow layers play
a critical role in storing knowledge.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 + 10 pages, 5 + 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Compressed Multi-task embeddings for Data-Efficient Downstream training
  and inference in Earth Observation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17886v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17886v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Carlos Gomes, Thomas Brunschwiler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As repositories of large scale data in earth observation (EO) have grown, so
have transfer and storage costs for model training and inference, expending
significant resources. We introduce Neural Embedding Compression (NEC), based
on the transfer of compressed embeddings to data consumers instead of raw data.
We adapt foundation models (FM) through learned neural compression to generate
multi-task embeddings while navigating the tradeoff between compression rate
and embedding utility. We update only a small fraction of the FM parameters
(10%) for a short training period (1% of the iterations of pre-training). We
evaluate NEC on two EO tasks: scene classification and semantic segmentation.
Compared with applying traditional compression to the raw data, NEC achieves
similar accuracy with a 75% to 90% reduction in data. Even at 99.7%
compression, performance drops by only 5% on the scene classification task.
Overall, NEC is a data-efficient yet performant approach for multi-task EO
modelling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at IGARSS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Empowering Data Mesh with Federated Learning <span class="chip">KDD</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17878v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17878v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyuan Li, Salman Toor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The evolution of data architecture has seen the rise of data lakes, aiming to
solve the bottlenecks of data management and promote intelligent
decision-making. However, this centralized architecture is limited by the
proliferation of data sources and the growing demand for timely analysis and
processing. A new data paradigm, Data Mesh, is proposed to overcome these
challenges. Data Mesh treats domains as a first-class concern by distributing
the data ownership from the central team to each data domain, while keeping the
federated governance to monitor domains and their data products. Many
multi-million dollar organizations like Paypal, Netflix, and Zalando have
already transformed their data analysis pipelines based on this new
architecture. In this decentralized architecture where data is locally
preserved by each domain team, traditional centralized machine learning is
incapable of conducting effective analysis across multiple domains, especially
for security-sensitive organizations. To this end, we introduce a pioneering
approach that incorporates Federated Learning into Data Mesh. To the best of
our knowledge, this is the first open-source applied work that represents a
critical advancement toward the integration of federated learning methods into
the Data Mesh paradigm, underscoring the promising prospects for
privacy-preserving and decentralized data analysis strategies within Data Mesh
architecture.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In Proceedings of ACM Knowledge Discovery and Data Mining, Barcelona,
  Spain, 25th - 29th August, 2024 (Conference acronym KDD), 9 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sample complexity of quantum hypothesis testing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17868v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17868v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao-Chung Cheng, Nilanjana Datta, Nana Liu, Theshani Nuradha, Robert Salzmann, Mark M. Wilde
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantum hypothesis testing has been traditionally studied from the
information-theoretic perspective, wherein one is interested in the optimal
decay rate of error probabilities as a function of the number of samples of an
unknown state. In this paper, we study the sample complexity of quantum
hypothesis testing, wherein the goal is to determine the minimum number of
samples needed to reach a desired error probability. By making use of the
wealth of knowledge that already exists in the literature on quantum hypothesis
testing, we characterize the sample complexity of binary quantum hypothesis
testing in the symmetric and asymmetric settings, and we provide bounds on the
sample complexity of multiple quantum hypothesis testing. In more detail, we
prove that the sample complexity of symmetric binary quantum hypothesis testing
depends logarithmically on the inverse error probability and inversely on the
negative logarithm of the fidelity. As a counterpart of the quantum Stein's
lemma, we also find that the sample complexity of asymmetric binary quantum
hypothesis testing depends logarithmically on the inverse type~II error
probability and inversely on the quantum relative entropy. Finally, we provide
lower and upper bounds on the sample complexity of multiple quantum hypothesis
testing, with it remaining an intriguing open question to improve these bounds.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>38 pages, 1 figure, preliminary version; see independent and
  concurrent work of Pensia, Jog, Loh at arXiv:2403.16981</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Using Domain Knowledge to Guide Dialog Structure Induction via Neural
  Probabilistic Soft Logic 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17853v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17853v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Connor Pryor, Quan Yuan, Jeremiah Liu, Mehran Kazemi, Deepak Ramachandran, Tania Bedrax-Weiss, Lise Getoor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dialog Structure Induction (DSI) is the task of inferring the latent dialog
structure (i.e., a set of dialog states and their temporal transitions) of a
given goal-oriented dialog. It is a critical component for modern dialog system
design and discourse analysis. Existing DSI approaches are often purely
data-driven, deploy models that infer latent states without access to domain
knowledge, underperform when the training corpus is limited/noisy, or have
difficulty when test dialogs exhibit distributional shifts from the training
domain. This work explores a neural-symbolic approach as a potential solution
to these problems. We introduce Neural Probabilistic Soft Logic Dialogue
Structure Induction (NEUPSL DSI), a principled approach that injects symbolic
knowledge into the latent space of a generative neural model. We conduct a
thorough empirical investigation on the effect of NEUPSL DSI learning on hidden
representation quality, few-shot learning, and out-of-domain generalization
performance. Over three dialog structure induction datasets and across
unsupervised and semi-supervised settings for standard and cross-domain
generalization, the injection of symbolic knowledge using NEUPSL DSI provides a
consistent boost in performance over the canonical baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Counterfactual Fairness through Transforming Data Orthogonal to Bias 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17852v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17852v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuyi Chen, Shixiang Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning models have shown exceptional prowess in solving complex
issues across various domains. Nonetheless, these models can sometimes exhibit
biased decision-making, leading to disparities in treatment across different
groups. Despite the extensive research on fairness, the nuanced effects of
multivariate and continuous sensitive variables on decision-making outcomes
remain insufficiently studied. We introduce a novel data pre-processing
algorithm, Orthogonal to Bias (OB), designed to remove the influence of a group
of continuous sensitive variables, thereby facilitating counterfactual fairness
in machine learning applications. Our approach is grounded in the assumption of
a jointly normal distribution within a structural causal model (SCM), proving
that counterfactual fairness can be achieved by ensuring the data is
uncorrelated with sensitive variables. The OB algorithm is model-agnostic,
catering to a wide array of machine learning models and tasks, and includes a
sparse variant to enhance numerical stability through regularization. Through
empirical evaluation on simulated and real-world datasets - including the adult
income and the COMPAS recidivism datasets - our methodology demonstrates its
capacity to enable fairer outcomes without compromising accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Climate Downscaling: A Deep-Learning Based Super-resolution Model of
  Precipitation Data with Attention Block and Skip Connections 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17847v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17847v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chia-Hao Chiang, Zheng-Han Huang, Liwen Liu, Hsin-Chien Liang, Yi-Chi Wang, Wan-Ling Tseng, Chao Wang, Che-Ta Chen, Ko-Chih Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human activities accelerate consumption of fossil fuels and produce
greenhouse gases, resulting in urgent issues today: global warming and the
climate change. These indirectly cause severe natural disasters, plenty of
lives suffering and huge losses of agricultural properties. To mitigate impacts
on our lands, scientists are developing renewable, reusable, and clean energies
and climatologists are trying to predict the extremes. Meanwhile, governments
are publicizing resource-saving policies for a more eco-friendly society and
arousing environment awareness. One of the most influencing factors is the
precipitation, bringing condensed water vapor onto lands. Water resources are
the most significant but basic needs in society, not only supporting our
livings, but also economics. In Taiwan, although the average annual
precipitation is up to 2,500 millimeter (mm), the water allocation for each
person is lower than the global average due to drastically geographical
elevation changes and uneven distribution through the year. Thus, it is crucial
to track and predict the rainfall to make the most use of it and to prevent the
floods. However, climate models have limited resolution and require intensive
computational power for local-scale use. Therefore, we proposed a deep
convolutional neural network with skip connections, attention blocks, and
auxiliary data concatenation, in order to downscale the low-resolution
precipitation data into high-resolution one. Eventually, we compare with other
climate downscaling methods and show better performance in metrics of Mean
Absolute Error (MAE), Root Mean Square Error (RMSE), Pearson Correlation,
structural similarity index (SSIM), and forecast indicators.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hierarchical Open-Vocabulary 3D Scene Graphs for Language-Grounded Robot
  Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17846v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17846v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdelrhman Werby, Chenguang Huang, Martin Büchner, Abhinav Valada, Wolfram Burgard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent open-vocabulary robot mapping methods enrich dense geometric maps with
pre-trained visual-language features. While these maps allow for the prediction
of point-wise saliency maps when queried for a certain language concept,
large-scale environments and abstract queries beyond the object level still
pose a considerable hurdle, ultimately limiting language-grounded robotic
navigation. In this work, we present HOV-SG, a hierarchical open-vocabulary 3D
scene graph mapping approach for language-grounded robot navigation. Leveraging
open-vocabulary vision foundation models, we first obtain state-of-the-art
open-vocabulary segment-level maps in 3D and subsequently construct a 3D scene
graph hierarchy consisting of floor, room, and object concepts, each enriched
with open-vocabulary features. Our approach is able to represent multi-story
buildings and allows robotic traversal of those using a cross-floor Voronoi
graph. HOV-SG is evaluated on three distinct datasets and surpasses previous
baselines in open-vocabulary semantic accuracy on the object, room, and floor
level while producing a 75% reduction in representation size compared to dense
open-vocabulary maps. In order to prove the efficacy and generalization
capabilities of HOV-SG, we showcase successful long-horizon
language-conditioned robot navigation within real-world multi-storage
environments. We provide code and trial video data at http://hovsg.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code and video are available at http://hovsg.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TractOracle: towards an anatomically-informed reward function for
  RL-based tractography 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17845v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17845v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antoine Théberge, Maxime Descoteaux, Pierre-Marc Jodoin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning (RL)-based tractography is a competitive alternative
to machine learning and classical tractography algorithms due to its high
anatomical accuracy obtained without the need for any annotated data. However,
the reward functions so far used to train RL agents do not encapsulate
anatomical knowledge which causes agents to generate spurious false positives
tracts. In this paper, we propose a new RL tractography system, TractOracle,
which relies on a reward network trained for streamline classification. This
network is used both as a reward function during training as well as a mean for
stopping the tracking process early and thus reduce the number of false
positive streamlines. This makes our system a unique method that evaluates and
reconstructs WM streamlines at the same time. We report an improvement of true
positive ratios by almost 20\% and a reduction of 3x of false positive ratios
on one dataset and an increase between 2x and 7x in the number true positive
streamlines on another dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mechanistic Design and Scaling of Hybrid Architectures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17844v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17844v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Poli, Armin W Thomas, Eric Nguyen, Pragaash Ponnusamy, Björn Deiseroth, Kristian Kersting, Taiji Suzuki, Brian Hie, Stefano Ermon, Christopher Ré, Ce Zhang, Stefano Massaroli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The development of deep learning architectures is a resource-demanding
process, due to a vast design space, long prototyping times, and high compute
costs associated with at-scale model training and evaluation. We set out to
simplify this process by grounding it in an end-to-end mechanistic architecture
design (MAD) pipeline, encompassing small-scale capability unit tests
predictive of scaling laws. Through a suite of synthetic token manipulation
tasks such as compression and recall, designed to probe capabilities, we
identify and test new hybrid architectures constructed from a variety of
computational primitives. We experimentally validate the resulting
architectures via an extensive compute-optimal and a new state-optimal scaling
law analysis, training over 500 language models between 70M to 7B parameters.
Surprisingly, we find MAD synthetics to correlate with compute-optimal
perplexity, enabling accurate evaluation of new architectures via isolated
proxy tasks. The new architectures found via MAD, based on simple ideas such as
hybridization and sparsity, outperform state-of-the-art Transformer,
convolutional, and recurrent architectures (Transformer++, Hyena, Mamba) in
scaling, both at compute-optimal budgets and in overtrained regimes. Overall,
these results provide evidence that performance on curated synthetic tasks can
be predictive of scaling laws, and that an optimal architecture should leverage
specialized layers via a hybrid topology.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GTA-HDR: A Large-Scale Synthetic <span class="highlight-title">Dataset</span> for HDR Image Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17837v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17837v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hrishav Bakul Barua, Kalin Stefanov, KokSheik Wong, Abhinav Dhall, Ganesh Krishnasamy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High Dynamic Range (HDR) content (i.e., images and videos) has a broad range
of applications. However, capturing HDR content from real-world scenes is
expensive and time- consuming. Therefore, the challenging task of
reconstructing visually accurate HDR images from their Low Dynamic Range (LDR)
counterparts is gaining attention in the vision research community. A major
challenge in this research problem is the lack of datasets, which capture
diverse scene conditions (e.g., lighting, shadows, weather, locations,
landscapes, objects, humans, buildings) and various image features (e.g.,
color, contrast, saturation, hue, luminance, brightness, radiance). To address
this gap, in this paper, we introduce GTA-HDR, a large-scale synthetic dataset
of photo-realistic HDR images sampled from the GTA-V video game. We perform
thorough evaluation of the proposed dataset, which demonstrates significant
qualitative and quantitative improvements of the state-of-the-art HDR image
reconstruction methods. Furthermore, we demonstrate the effectiveness of the
proposed dataset and its impact on additional computer vision tasks including
3D human pose estimation, human body part segmentation, and holistic scene
segmentation. The dataset, data collection pipeline, and evaluation code are
available at: https://github.com/HrishavBakulBarua/GTA-HDR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GPFL: A Gradient Projection-Based Client Selection Framework for
  Efficient Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17833v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17833v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shijie Na, Yuzhi Liang, Siu-Ming Yiu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning client selection is crucial for determining participant
clients while balancing model accuracy and communication efficiency. Existing
methods have limitations in handling data heterogeneity, computational burdens,
and independent client treatment. To address these challenges, we propose GPFL,
which measures client value by comparing local and global descent directions.
We also employ an Exploit-Explore mechanism to enhance performance.
Experimental results on FEMINST and CIFAR-10 datasets demonstrate that GPFL
outperforms baselines in Non-IID scenarios, achieving over 9\% improvement in
FEMINST test accuracy. Moreover, GPFL exhibits shorter computation times
through pre-selection and parameter reuse in federated learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning the Optimal Power Flow: Environment Design Matters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17831v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17831v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Wolgast, Astrid Nieße
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To solve the optimal power flow (OPF) problem, reinforcement learning (RL)
emerges as a promising new approach. However, the RL-OPF literature is strongly
divided regarding the exact formulation of the OPF problem as an RL
environment. In this work, we collect and implement diverse environment design
decisions from the literature regarding training data, observation space,
episode definition, and reward function choice. In an experimental analysis, we
show the significant impact of these environment design options on RL-OPF
training performance. Further, we derive some first recommendations regarding
the choice of these design decisions. The created environment framework is
fully open-source and can serve as a benchmark for future research in the
RL-OPF field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DiffH2O: Diffusion-Based Synthesis of Hand-Object Interactions from
  Textual Descriptions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17827v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17827v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sammy Christen, Shreyas Hampali, Fadime Sener, Edoardo Remelli, Tomas Hodan, Eric Sauser, Shugao Ma, Bugra Tekin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating natural hand-object interactions in 3D is challenging as the
resulting hand and object motions are expected to be physically plausible and
semantically meaningful. Furthermore, generalization to unseen objects is
hindered by the limited scale of available hand-object interaction datasets. We
propose DiffH2O, a novel method to synthesize realistic, one or two-handed
object interactions from provided text prompts and geometry of the object. The
method introduces three techniques that enable effective learning from limited
data. First, we decompose the task into a grasping stage and a text-based
interaction stage and use separate diffusion models for each. In the grasping
stage, the model only generates hand motions, whereas in the interaction phase
both hand and object poses are synthesized. Second, we propose a compact
representation that tightly couples hand and object poses. Third, we propose
two different guidance schemes to allow more control of the generated motions:
grasp guidance and detailed textual guidance. Grasp guidance takes a single
target grasping pose and guides the diffusion model to reach this grasp at the
end of the grasping stage, which provides control over the grasping pose. Given
a grasping motion from this stage, multiple different actions can be prompted
in the interaction phase. For textual guidance, we contribute comprehensive
text descriptions to the GRAB dataset and show that they enable our method to
have more fine-grained control over hand-object interactions. Our quantitative
and qualitative evaluation demonstrates that the proposed method outperforms
baseline methods and leads to natural hand-object motions. Moreover, we
demonstrate the practicality of our framework by utilizing a hand pose estimate
from an off-the-shelf pose estimator for guidance, and then sampling multiple
different actions in the interaction stage.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://diffh2o.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Are Compressed Language Models Less Subgroup Robust? <span class="chip">EMNLP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17811v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17811v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leonidas Gee, Andrea Zugarini, Novi Quadrianto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To reduce the inference cost of large language models, model compression is
increasingly used to create smaller scalable models. However, little is known
about their robustness to minority subgroups defined by the labels and
attributes of a dataset. In this paper, we investigate the effects of 18
different compression methods and settings on the subgroup robustness of BERT
language models. We show that worst-group performance does not depend on model
size alone, but also on the compression method used. Additionally, we find that
model compression does not always worsen the performance on minority subgroups.
Altogether, our analysis serves to further research into the subgroup
robustness of model compression.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The 2023 Conference on Empirical Methods in Natural Language
  Processing (EMNLP 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Annotated Biomedical Video Generation using Denoising Diffusion
  Probabilistic Models and Flow Fields 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17808v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17808v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rüveyda Yilmaz, Dennis Eschweiler, Johannes Stegmaier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The segmentation and tracking of living cells play a vital role within the
biomedical domain, particularly in cancer research, drug development, and
developmental biology. These are usually tedious and time-consuming tasks that
are traditionally done by biomedical experts. Recently, to automatize these
processes, deep learning based segmentation and tracking methods have been
proposed. These methods require large-scale datasets and their full potential
is constrained by the scarcity of annotated data in the biomedical imaging
domain. To address this limitation, we propose Biomedical Video Diffusion Model
(BVDM), capable of generating realistic-looking synthetic microscopy videos.
Trained only on a single real video, BVDM can generate videos of arbitrary
length with pixel-level annotations that can be used for training data-hungry
models. It is composed of a denoising diffusion probabilistic model (DDPM)
generating high-fidelity synthetic cell microscopy images and a flow prediction
model (FPM) predicting the non-rigid transformation between consecutive video
frames. During inference, initially, the DDPM imposes realistic cell textures
on synthetic cell masks which are generated based on real data statistics. The
flow prediction model predicts the flow field between consecutive masks and
applies that to the DDPM output from the previous time frame to create the next
one while keeping temporal consistency. BVDM outperforms state-of-the-art
synthetic live cell microscopy video generation models. Furthermore, we
demonstrate that a sufficiently large synthetic dataset enhances the
performance of cell segmentation and tracking models compared to using a
limited amount of available real data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Have Faith in Faithfulness: Going Beyond Circuit Overlap When Finding
  Model Mechanisms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17806v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17806v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Hanna, Sandro Pezzelle, Yonatan Belinkov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many recent language model (LM) interpretability studies have adopted the
circuits framework, which aims to find the minimal computational subgraph, or
circuit, that explains LM behavior on a given task. Most studies determine
which edges belong in a LM's circuit by performing causal interventions on each
edge independently, but this scales poorly with model size. Edge attribution
patching (EAP), gradient-based approximation to interventions, has emerged as a
scalable but imperfect solution to this problem. In this paper, we introduce a
new method - EAP with integrated gradients (EAP-IG) - that aims to better
maintain a core property of circuits: faithfulness. A circuit is faithful if
all model edges outside the circuit can be ablated without changing the model's
performance on the task; faithfulness is what justifies studying circuits,
rather than the full model. Our experiments demonstrate that circuits found
using EAP are less faithful than those found using EAP-IG, even though both
have high node overlap with circuits found previously using causal
interventions. We conclude more generally that when using circuits to compare
the mechanisms models use to solve tasks, faithfulness, not overlap, is what
should be measured.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scenario-Based Curriculum Generation for Multi-Agent Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17805v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17805v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Axel Brunnbauer, Luigi Berducci, Peter Priller, Dejan Nickovic, Radu Grosu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The automated generation of diverse and complex training scenarios has been
an important ingredient in many complex learning tasks. Especially in
real-world application domains, such as autonomous driving, auto-curriculum
generation is considered vital for obtaining robust and general policies.
However, crafting traffic scenarios with multiple, heterogeneous agents is
typically considered as a tedious and time-consuming task, especially in more
complex simulation environments. In our work, we introduce MATS-Gym, a
Multi-Agent Traffic Scenario framework to train agents in CARLA, a
high-fidelity driving simulator. MATS-Gym is a multi-agent training framework
for autonomous driving that uses partial scenario specifications to generate
traffic scenarios with variable numbers of agents. This paper unifies various
existing approaches to traffic scenario description into a single training
framework and demonstrates how it can be integrated with techniques from
unsupervised environment design to automate the generation of adaptive
auto-curricula. The code is available at
https://github.com/AutonomousDrivingExaminer/mats-gym.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 Pages, Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Secure Aggregation is Not Private Against Membership Inference Attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17775v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17775v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Khac-Hoang Ngo, Johan Östman, Giuseppe Durisi, Alexandre Graell i Amat
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Secure aggregation (SecAgg) is a commonly-used privacy-enhancing mechanism in
federated learning, affording the server access only to the aggregate of model
updates while safeguarding the confidentiality of individual updates. Despite
widespread claims regarding SecAgg's privacy-preserving capabilities, a formal
analysis of its privacy is lacking, making such presumptions unjustified. In
this paper, we delve into the privacy implications of SecAgg by treating it as
a local differential privacy (LDP) mechanism for each local update. We design a
simple attack wherein an adversarial server seeks to discern which update
vector a client submitted, out of two possible ones, in a single training round
of federated learning under SecAgg. By conducting privacy auditing, we assess
the success probability of this attack and quantify the LDP guarantees provided
by SecAgg. Our numerical results unveil that, contrary to prevailing claims,
SecAgg offers weak privacy against membership inference attacks even in a
single training round. Indeed, it is difficult to hide a local update by adding
other independent local updates when the updates are of high dimension. Our
findings underscore the imperative for additional privacy-enhancing mechanisms,
such as noise injection, in federated learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SciNews: From Scholarly Complexities to Public Narratives -- A <span class="highlight-title">Dataset</span>
  for Scientific News Report Generation <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17768v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17768v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongqi Pu, Yifan Wang, Jia Loy, Vera Demberg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scientific news reports serve as a bridge, adeptly translating complex
research articles into reports that resonate with the broader public. The
automated generation of such narratives enhances the accessibility of scholarly
insights. In this paper, we present a new corpus to facilitate this paradigm
development. Our corpus comprises a parallel compilation of academic
publications and their corresponding scientific news reports across nine
disciplines. To demonstrate the utility and reliability of our dataset, we
conduct an extensive analysis, highlighting the divergences in readability and
brevity between scientific news narratives and academic manuscripts. We
benchmark our dataset employing state-of-the-art text generation models. The
evaluation process involves both automatic and human evaluation, which lays the
groundwork for future explorations into the automated generation of scientific
news reports. The dataset and code related to this work are available at
https://dongqi.me/projects/SciNews.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LREC-COLING 2024 Main Conference Paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Asymptotic Bayes risk of semi-supervised learning with uncertain
  labeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17767v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17767v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Victor Leger, Romain Couillet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This article considers a semi-supervised classification setting on a Gaussian
mixture model, where the data is not labeled strictly as usual, but instead
with uncertain labels. Our main aim is to compute the Bayes risk for this
model. We compare the behavior of the Bayes risk and the best known algorithm
for this model. This comparison eventually gives new insights over the
algorithm.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Noise2Noise Denoising of CRISM Hyperspectral Data <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17757v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17757v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Robert Platt, Rossella Arcucci, Cédric John
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hyperspectral data acquired by the Compact Reconnaissance Imaging
Spectrometer for Mars (CRISM) have allowed for unparalleled mapping of the
surface mineralogy of Mars. Due to sensor degradation over time, a significant
portion of the recently acquired data is considered unusable. Here a new
data-driven model architecture, Noise2Noise4Mars (N2N4M), is introduced to
remove noise from CRISM images. Our model is self-supervised and does not
require zero-noise target data, making it well suited for use in Planetary
Science applications where high quality labelled data is scarce. We demonstrate
its strong performance on synthetic-noise data and CRISM images, and its impact
on downstream classification performance, outperforming benchmark methods on
most metrics. This allows for detailed analysis for critical sites of interest
on the Martian surface, including proposed lander sites.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 3 figures. Accepted as a conference paper at the ICLR 2024
  ML4RS Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CCDSReFormer: Traffic Flow Prediction with a Criss-Crossed Dual-Stream
  Enhanced Rectified <span class="highlight-title">Transformer</span> Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17753v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17753v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiqi Shao, Michael G. H. Bell, Ze Wang, D. Glenn Geers, Xusheng Yao, Junbin Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate, and effective traffic forecasting is vital for smart traffic
systems, crucial in urban traffic planning and management. Current
Spatio-Temporal Transformer models, despite their prediction capabilities,
struggle with balancing computational efficiency and accuracy, favoring global
over local information, and handling spatial and temporal data separately,
limiting insight into complex interactions. We introduce the Criss-Crossed
Dual-Stream Enhanced Rectified Transformer model (CCDSReFormer), which includes
three innovative modules: Enhanced Rectified Spatial Self-attention (ReSSA),
Enhanced Rectified Delay Aware Self-attention (ReDASA), and Enhanced Rectified
Temporal Self-attention (ReTSA). These modules aim to lower computational needs
via sparse attention, focus on local information for better traffic dynamics
understanding, and merge spatial and temporal insights through a unique
learning method. Extensive tests on six real-world datasets highlight
CCDSReFormer's superior performance. An ablation study also confirms the
significant impact of each component on the model's predictive accuracy,
showcasing our model's ability to forecast traffic flow effectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leave No Patient Behind: Enhancing Medication Recommendation for Rare
  Disease Patients 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17745v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17745v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihao Zhao, Yi Jing, Fuli Feng, Jiancan Wu, Chongming Gao, Xiangnan He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medication recommendation systems have gained significant attention in
healthcare as a means of providing tailored and effective drug combinations
based on patients' clinical information. However, existing approaches often
suffer from fairness issues, as recommendations tend to be more accurate for
patients with common diseases compared to those with rare conditions. In this
paper, we propose a novel model called Robust and Accurate REcommendations for
Medication (RAREMed), which leverages the pretrain-finetune learning paradigm
to enhance accuracy for rare diseases. RAREMed employs a transformer encoder
with a unified input sequence approach to capture complex relationships among
disease and procedure codes. Additionally, it introduces two self-supervised
pre-training tasks, namely Sequence Matching Prediction (SMP) and Self
Reconstruction (SR), to learn specialized medication needs and interrelations
among clinical codes. Experimental results on two real-world datasets
demonstrate that RAREMed provides accurate drug sets for both rare and common
disease patients, thereby mitigating unfairness in medication recommendation
systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EulerFormer: Sequential User Behavior Modeling with Complex Vector
  Attention <span class="chip">SIGIR'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17729v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17729v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhen Tian, Wayne Xin Zhao, Changwang Zhang, Xin Zhao, Zhongrui Ma, Ji-Rong Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To capture user preference, transformer models have been widely applied to
model sequential user behavior data. The core of transformer architecture lies
in the self-attention mechanism, which computes the pairwise attention scores
in a sequence. Due to the permutation-equivariant nature, positional encoding
is used to enhance the attention between token representations. In this
setting, the pairwise attention scores can be derived by both semantic
difference and positional difference. However, prior studies often model the
two kinds of difference measurements in different ways, which potentially
limits the expressive capacity of sequence modeling. To address this issue,
this paper proposes a novel transformer variant with complex vector attention,
named EulerFormer, which provides a unified theoretical framework to formulate
both semantic difference and positional difference. The EulerFormer involves
two key technical improvements. First, it employs a new transformation function
for efficiently transforming the sequence tokens into polar-form complex
vectors using Euler's formula, enabling the unified modeling of both semantic
and positional information in a complex rotation form.Secondly, it develops a
differential rotation mechanism, where the semantic rotation angles can be
controlled by an adaptation function, enabling the adaptive integration of the
semantic and positional information according to the semantic
contexts.Furthermore, a phase contrastive learning task is proposed to improve
the anisotropy of contextual representations in EulerFormer. Our theoretical
framework possesses a high degree of completeness and generality. It is more
robust to semantic variations and possesses moresuperior theoretical properties
in principle. Extensive experiments conducted on four public datasets
demonstrate the effectiveness and efficiency of our approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in SIGIR'24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Masked Autoencoders are PDE Learners 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17728v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17728v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anthony Zhou, Amir Barati Farimani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural solvers for partial differential equations (PDEs) have great
potential, yet their practicality is currently limited by their
generalizability. PDEs evolve over broad scales and exhibit diverse behaviors;
predicting these phenomena will require learning representations across a wide
variety of inputs, which may encompass different coefficients, geometries, or
equations. As a step towards generalizable PDE modeling, we adapt masked
pretraining for PDEs. Through self-supervised learning across PDEs, masked
autoencoders can learn useful latent representations for downstream tasks. In
particular, masked pretraining can improve coefficient regression and
timestepping performance of neural solvers on unseen equations. We hope that
masked pretraining can emerge as a unifying method across large, unlabeled, and
heterogeneous datasets to learn latent physics at scale.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rotate to Scan: UNet-like Mamba with Triplet SSM Module for Medical
  Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17701v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17701v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Tang, Lianglun Cheng, Guoheng Huang, Zhengguang Tan, Junhao Lu, Kaihong Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image segmentation holds a vital position in the realms of diagnosis and
treatment within the medical domain. Traditional convolutional neural networks
(CNNs) and Transformer models have made significant advancements in this realm,
but they still encounter challenges because of limited receptive field or high
computing complexity. Recently, State Space Models (SSMs), particularly Mamba
and its variants, have demonstrated notable performance in the field of vision.
However, their feature extraction methods may not be sufficiently effective and
retain some redundant structures, leaving room for parameter reduction.
Motivated by previous spatial and channel attention methods, we propose Triplet
Mamba-UNet. The method leverages residual VSS Blocks to extract intensive
contextual features, while Triplet SSM is employed to fuse features across
spatial and channel dimensions. We conducted experiments on ISIC17, ISIC18,
CVC-300, CVC-ClinicDB, Kvasir-SEG, CVC-ColonDB, and Kvasir-Instrument datasets,
demonstrating the superior segmentation performance of our proposed TM-UNet.
Additionally, compared to the previous VM-UNet, our model achieves a one-third
reduction in parameters.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MEP: Multiple Kernel Learning Enhancing Relative Positional Encoding
  Length Extrapolation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17698v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17698v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weiguo Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When the predicted sequence length exceeds the length seen during training,
the transformer's inference accuracy diminishes. Existing relative position
encoding methods, such as those based on the ALiBi technique, address the
length extrapolation challenge exclusively through the implementation of a
single kernel function, which introduces a constant bias to every post-softmax
attention scores according to their distance. These approaches do not
investigate or employ multiple kernel functions to address the extrapolation
challenge. Drawing on the ALiBi approach, this study proposes a novel relative
positional encoding method, called MEP, which employs a weighted average to
combine distinct kernel functions(such as the exponential kernel and the
Gaussian kernel) to generate a bias that is applied to post-softmax attention
scores. Initially, the framework utilizes various kernel functions to construct
multiple kernel functions. Each kernel function adheres to a consistent mean
weight coefficient, harnessing the synergistic advantages of different kernels
to formulate an innovative bias function. Subsequently, specific slopes are
tailored for each kernel function, applying penalties at varying rates, to
enhance the model's extrapolation capabilities. Finally, this bias is
seamlessly incorporated as a penalty to the post-softmax scores. We present two
distinct versions of our method: a parameter-free variant that requires no new
learnable parameters, which enhances length extrapolation capabilities without
compromising training efficiency, and a parameterized variant capable of
integrating state-of-the-art techniques. Empirical evaluations across diverse
datasets have demonstrated that both variants of our method achieve
state-of-the-art performance, outperforming traditional parameter-free and
parameterized approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PlainMamba: Improving Non-Hierarchical Mamba in Visual Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17695v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17695v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenhongyi Yang, Zehui Chen, Miguel Espinosa, Linus Ericsson, Zhenyu Wang, Jiaming Liu, Elliot J. Crowley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present PlainMamba: a simple non-hierarchical state space model (SSM)
designed for general visual recognition. The recent Mamba model has shown how
SSMs can be highly competitive with other architectures on sequential data and
initial attempts have been made to apply it to images. In this paper, we
further adapt the selective scanning process of Mamba to the visual domain,
enhancing its ability to learn features from two-dimensional images by (i) a
continuous 2D scanning process that improves spatial continuity by ensuring
adjacency of tokens in the scanning sequence, and (ii) direction-aware updating
which enables the model to discern the spatial relations of tokens by encoding
directional information. Our architecture is designed to be easy to use and
easy to scale, formed by stacking identical PlainMamba blocks, resulting in a
model with constant width throughout all layers. The architecture is further
simplified by removing the need for special tokens. We evaluate PlainMamba on a
variety of visual recognition tasks including image classification, semantic
segmentation, object detection, and instance segmentation. Our method achieves
performance gains over previous non-hierarchical models and is competitive with
hierarchical alternatives. For tasks requiring high-resolution inputs, in
particular, PlainMamba requires much less computing while maintaining high
performance. Code and models are available at
https://github.com/ChenhongyiYang/PlainMamba
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Manifold-Guided Lyapunov Control with Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17692v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17692v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amartya Mukherjee, Thanin Quartz, Jun Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a novel approach to generating stabilizing controllers
for a large class of dynamical systems using diffusion models. The core
objective is to develop stabilizing control functions by identifying the
closest asymptotically stable vector field relative to a predetermined manifold
and adjusting the control function based on this finding. To achieve this, we
employ a diffusion model trained on pairs consisting of asymptotically stable
vector fields and their corresponding Lyapunov functions. Our numerical results
demonstrate that this pre-trained model can achieve stabilization over
previously unseen systems efficiently and rapidly, showcasing the potential of
our approach in fast zero-shot control and generalizability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How Private is DP-SGD? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17673v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17673v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lynn Chua, Badih Ghazi, Pritish Kamath, Ravi Kumar, Pasin Manurangsi, Amer Sinha, Chiyuan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We demonstrate a substantial gap between the privacy guarantees of the
Adaptive Batch Linear Queries (ABLQ) mechanism under different types of batch
sampling: (i) Shuffling, and (ii) Poisson subsampling; the typical analysis of
Differentially Private Stochastic Gradient Descent (DP-SGD) follows by
interpreting it as a post-processing of ABLQ. While shuffling based DP-SGD is
more commonly used in practical implementations, it is neither analytically nor
numerically amenable to easy privacy analysis. On the other hand, Poisson
subsampling based DP-SGD is challenging to scalably implement, but has a
well-understood privacy analysis, with multiple open-source numerically tight
privacy accountants available. This has led to a common practice of using
shuffling based DP-SGD in practice, but using the privacy analysis for the
corresponding Poisson subsampling version. Our result shows that there can be a
substantial gap between the privacy analysis when using the two types of batch
sampling, and thus advises caution in reporting privacy parameters for DP-SGD.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CANOS: A Fast and Scalable Neural AC-OPF Solver Robust To N-1
  Perturbations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17660v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17660v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luis Piloto, Sofia Liguori, Sephora Madjiheurem, Miha Zgubic, Sean Lovett, Hamish Tomlinson, Sophie Elster, Chris Apps, Sims Witherspoon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Optimal Power Flow (OPF) refers to a wide range of related optimization
problems with the goal of operating power systems efficiently and securely. In
the simplest setting, OPF determines how much power to generate in order to
minimize costs while meeting demand for power and satisfying physical and
operational constraints. In even the simplest case, power grid operators use
approximations of the AC-OPF problem because solving the exact problem is
prohibitively slow with state-of-the-art solvers. These approximations
sacrifice accuracy and operational feasibility in favor of speed. This
trade-off leads to costly "uplift payments" and increased carbon emissions,
especially for large power grids. In the present work, we train a deep learning
system (CANOS) to predict near-optimal solutions (within 1% of the true AC-OPF
cost) without compromising speed (running in as little as 33--65 ms).
Importantly, CANOS scales to realistic grid sizes with promising empirical
results on grids containing as many as 10,000 buses. Finally, because CANOS is
a Graph Neural Network, it is robust to changes in topology. We show that CANOS
is accurate across N-1 topological perturbations of a base grid typically used
in security-constrained analysis. This paves the way for more efficient
optimization of more complex OPF problems which alter grid connectivity such as
unit commitment, topology optimization and security-constrained OPF.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SGHormer: An Energy-Saving Graph <span class="highlight-title">Transformer</span> Driven by Spikes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17656v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17656v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huizhe Zhang, Jintang Li, Liang Chen, Zibin Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Transformers (GTs) with powerful representation learning ability make a
huge success in wide range of graph tasks. However, the costs behind
outstanding performances of GTs are higher energy consumption and computational
overhead. The complex structure and quadratic complexity during attention
calculation in vanilla transformer seriously hinder its scalability on the
large-scale graph data. Though existing methods have made strides in
simplifying combinations among blocks or attention-learning paradigm to improve
GTs' efficiency, a series of energy-saving solutions originated from
biologically plausible structures are rarely taken into consideration when
constructing GT framework. To this end, we propose a new spiking-based graph
transformer (SGHormer). It turns full-precision embeddings into sparse and
binarized spikes to reduce memory and computational costs. The spiking graph
self-attention and spiking rectify blocks in SGHormer explicitly capture global
structure information and recover the expressive power of spiking embeddings,
respectively. In experiments, SGHormer achieves comparable performances to
other full-precision GTs with extremely low computational energy consumption.
The results show that SGHomer makes a remarkable progress in the field of
low-energy GTs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uncertainty-aware Distributional Offline Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17646v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17646v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaocong Chen, Siyu Wang, Tong Yu, Lina Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Offline reinforcement learning (RL) presents distinct challenges as it relies
solely on observational data. A central concern in this context is ensuring the
safety of the learned policy by quantifying uncertainties associated with
various actions and environmental stochasticity. Traditional approaches
primarily emphasize mitigating epistemic uncertainty by learning risk-averse
policies, often overlooking environmental stochasticity. In this study, we
propose an uncertainty-aware distributional offline RL method to simultaneously
address both epistemic uncertainty and environmental stochasticity. We propose
a model-free offline RL algorithm capable of learning risk-averse policies and
characterizing the entire distribution of discounted cumulative rewards, as
opposed to merely maximizing the expected value of accumulated discounted
returns. Our method is rigorously evaluated through comprehensive experiments
in both risk-sensitive and risk-neutral benchmarks, demonstrating its superior
performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PeersimGym: An Environment for Solving the Task Offloading Problem with
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17637v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17637v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Frederico Metelo, Stevo Racković, Pedro Ákos, Cláudia Soares
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Task offloading, crucial for balancing computational loads across devices in
networks such as the Internet of Things, poses significant optimization
challenges, including minimizing latency and energy usage under strict
communication and storage constraints. While traditional optimization falls
short in scalability; and heuristic approaches lack in achieving optimal
outcomes, Reinforcement Learning (RL) offers a promising avenue by enabling the
learning of optimal offloading strategies through iterative interactions.
However, the efficacy of RL hinges on access to rich datasets and
custom-tailored, realistic training environments. To address this, we introduce
PeersimGym, an open-source, customizable simulation environment tailored for
developing and optimizing task offloading strategies within computational
networks. PeersimGym supports a wide range of network topologies and
computational constraints and integrates a \textit{PettingZoo}-based interface
for RL agent deployment in both solo and multi-agent setups. Furthermore, we
demonstrate the utility of the environment through experiments with Deep
Reinforcement Learning agents, showcasing the potential of RL-based approaches
to significantly enhance offloading strategies in distributed computing
settings. PeersimGym thus bridges the gap between theoretical RL models and
their practical applications, paving the way for advancements in efficient task
offloading methodologies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Retentive Decision <span class="highlight-title">Transformer</span> with Adaptive Masking for Reinforcement
  Learning based Recommendation Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17634v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17634v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siyu Wang, Xiaocong Chen, Lina Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning-based Recommender Systems (RLRS) have shown promise
across a spectrum of applications, from e-commerce platforms to streaming
services. Yet, they grapple with challenges, notably in crafting reward
functions and harnessing large pre-existing datasets within the RL framework.
Recent advancements in offline RLRS provide a solution for how to address these
two challenges. However, existing methods mainly rely on the transformer
architecture, which, as sequence lengths increase, can introduce challenges
associated with computational resources and training costs. Additionally, the
prevalent methods employ fixed-length input trajectories, restricting their
capacity to capture evolving user preferences. In this study, we introduce a
new offline RLRS method to deal with the above problems. We reinterpret the
RLRS challenge by modeling sequential decision-making as an inference task,
leveraging adaptive masking configurations. This adaptive approach selectively
masks input tokens, transforming the recommendation task into an inference
challenge based on varying token subsets, thereby enhancing the agent's ability
to infer across diverse trajectory lengths. Furthermore, we incorporate a
multi-scale segmented retention mechanism that facilitates efficient modeling
of long sequences, significantly enhancing computational efficiency. Our
experimental analysis, conducted on both online simulator and offline datasets,
clearly demonstrates the advantages of our proposed method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Data-driven Energy Consumption Modelling for Electric Micromobility
  using an Open <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17632v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17632v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Ding, Sen Yan, Maqsood Hussain Shah, Hongyuan Fang, Ji Li, Mingming Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The escalating challenges of traffic congestion and environmental degradation
underscore the critical importance of embracing E-Mobility solutions in urban
spaces. In particular, micro E-Mobility tools such as E-scooters and E-bikes,
play a pivotal role in this transition, offering sustainable alternatives for
urban commuters. However, the energy consumption patterns for these tools are a
critical aspect that impacts their effectiveness in real-world scenarios and is
essential for trip planning and boosting user confidence in using these. To
this effect, recent studies have utilised physical models customised for
specific mobility tools and conditions, but these models struggle with
generalization and effectiveness in real-world scenarios due to a notable
absence of open datasets for thorough model evaluation and verification. To
fill this gap, our work presents an open dataset, collected in Dublin, Ireland,
specifically designed for energy modelling research related to E-Scooters and
E-Bikes. Furthermore, we provide a comprehensive analysis of energy consumption
modelling based on the dataset using a set of representative machine learning
algorithms and compare their performance against the contemporary mathematical
models as a baseline. Our results demonstrate a notable advantage for
data-driven models in comparison to the corresponding mathematical models for
estimating energy consumption. Specifically, data-driven models outperform
physical models in accuracy by up to 83.83% for E-Bikes and 82.16% for
E-Scooters based on an in-depth analysis of the dataset under certain
assumptions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 5 figures, 4 tables. This manuscript has been accepted by
  the IEEE ITEC 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fake or JPEG? Revealing Common Biases in Generated Image Detection
  <span class="highlight-title">Dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17608v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17608v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Patrick Grommelt, Louis Weiss, Franz-Josef Pfreundt, Janis Keuper
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The widespread adoption of generative image models has highlighted the urgent
need to detect artificial content, which is a crucial step in combating
widespread manipulation and misinformation. Consequently, numerous detectors
and associated datasets have emerged. However, many of these datasets
inadvertently introduce undesirable biases, thereby impacting the effectiveness
and evaluation of detectors. In this paper, we emphasize that many datasets for
AI-generated image detection contain biases related to JPEG compression and
image size. Using the GenImage dataset, we demonstrate that detectors indeed
learn from these undesired factors. Furthermore, we show that removing the
named biases substantially increases robustness to JPEG compression and
significantly alters the cross-generator performance of evaluated detectors.
Specifically, it leads to more than 11 percentage points increase in
cross-generator performance for ResNet50 and Swin-T detectors on the GenImage
dataset, achieving state-of-the-art results.
  We provide the dataset and source codes of this paper on the anonymous
website: https://www.unbiased-genimage.org
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LASIL: Learner-Aware Supervised Imitation Learning For Long-term
  Microscopic Traffic Simulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17601v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17601v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ke Guo, Zhenwei Miao, Wei Jing, Weiwei Liu, Weizi Li, Dayang Hao, Jia Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Microscopic traffic simulation plays a crucial role in transportation
engineering by providing insights into individual vehicle behavior and overall
traffic flow. However, creating a realistic simulator that accurately
replicates human driving behaviors in various traffic conditions presents
significant challenges. Traditional simulators relying on heuristic models
often fail to deliver accurate simulations due to the complexity of real-world
traffic environments. Due to the covariate shift issue, existing imitation
learning-based simulators often fail to generate stable long-term simulations.
In this paper, we propose a novel approach called learner-aware supervised
imitation learning to address the covariate shift problem in multi-agent
imitation learning. By leveraging a variational autoencoder simultaneously
modeling the expert and learner state distribution, our approach augments
expert states such that the augmented state is aware of learner state
distribution. Our method, applied to urban traffic simulation, demonstrates
significant improvements over existing state-of-the-art baselines in both
short-term microscopic and long-term macroscopic realism when evaluated on the
real-world dataset pNEUMA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by cvpr 2024. arXiv admin note: text overlap with
  arXiv:2306.06401</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Benefits of Over-parameterization for Out-of-Distribution
  Generalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17592v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17592v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Hao, Yong Lin, Difan Zou, Tong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, machine learning models have achieved success based on the
independently and identically distributed assumption. However, this assumption
can be easily violated in real-world applications, leading to the
Out-of-Distribution (OOD) problem. Understanding how modern over-parameterized
DNNs behave under non-trivial natural distributional shifts is essential, as
current theoretical understanding is insufficient. Existing theoretical works
often provide meaningless results for over-parameterized models in OOD
scenarios or even contradict empirical findings. To this end, we are
investigating the performance of the over-parameterized model in terms of OOD
generalization under the general benign overfitting conditions. Our analysis
focuses on a random feature model and examines non-trivial natural
distributional shifts, where the benign overfitting estimators demonstrate a
constant excess OOD loss, despite achieving zero excess in-distribution (ID)
loss. We demonstrate that in this scenario, further increasing the model's
parameterization can significantly reduce the OOD loss. Intuitively, the
variance term of ID loss remains low due to orthogonality of long-tail
features, meaning overfitting noise during training generally doesn't raise
testing loss. However, in OOD cases, distributional shift increases the
variance term. Thankfully, the inherent shift is unrelated to individual x,
maintaining the orthogonality of long-tail features. Expanding the hidden
dimension can additionally improve this orthogonality by mapping the features
into higher-dimensional spaces, thereby reducing the variance term. We further
show that model ensembles also improve OOD loss, akin to increasing model
capacity. These insights explain the empirical phenomenon of enhanced OOD
generalization through model ensembles, supported by consistent simulations
with theoretical results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Forest-ORE: Mining Optimal Rule Ensemble to interpret Random Forest
  models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17588v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17588v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haddouchi Maissae, Berrado Abdelaziz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Random Forest (RF) is well-known as an efficient ensemble learning method in
terms of predictive performance. It is also considered a Black Box because of
its hundreds of deep decision trees. This lack of interpretability can be a
real drawback for acceptance of RF models in several real-world applications,
especially those affecting one's lives, such as in healthcare, security, and
law. In this work, we present Forest-ORE, a method that makes RF interpretable
via an optimized rule ensemble (ORE) for local and global interpretation.
Unlike other rule-based approaches aiming at interpreting the RF model, this
method simultaneously considers several parameters that influence the choice of
an interpretable rule ensemble. Existing methods often prioritize predictive
performance over interpretability coverage and do not provide information about
existing overlaps or interactions between rules. Forest-ORE uses a
mixed-integer optimization program to build an ORE that considers the trade-off
between predictive performance, interpretability coverage, and model size (size
of the rule ensemble, rule lengths, and rule overlaps). In addition to
providing an ORE competitive in predictive performance with RF, this method
enriches the ORE through other rules that afford complementary information. It
also enables monitoring of the rule selection process and delivers various
metrics that can be used to generate a graphical representation of the final
model. This framework is illustrated through an example, and its robustness is
assessed through 36 benchmark datasets. A comparative analysis of well-known
methods shows that Forest-ORE provides an excellent trade-off between
predictive performance, interpretability coverage, and model size.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>48 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dual Memory Networks: A Versatile Adaptation Approach for
  Vision-Language Models <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17589v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17589v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yabin Zhang, Wenjie Zhu, Hui Tang, Zhiyuan Ma, Kaiyang Zhou, Lei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the emergence of pre-trained vision-language models like CLIP, how to
adapt them to various downstream classification tasks has garnered significant
attention in recent research. The adaptation strategies can be typically
categorized into three paradigms: zero-shot adaptation, few-shot adaptation,
and the recently-proposed training-free few-shot adaptation. Most existing
approaches are tailored for a specific setting and can only cater to one or two
of these paradigms. In this paper, we introduce a versatile adaptation approach
that can effectively work under all three settings. Specifically, we propose
the dual memory networks that comprise dynamic and static memory components.
The static memory caches training data knowledge, enabling training-free
few-shot adaptation, while the dynamic memory preserves historical test
features online during the testing process, allowing for the exploration of
additional data insights beyond the training set. This novel capability
enhances model performance in the few-shot setting and enables model usability
in the absence of training data. The two memory networks employ the same
flexible memory interactive strategy, which can operate in a training-free mode
and can be further enhanced by incorporating learnable projection layers. Our
approach is tested across 11 datasets under the three task settings.
Remarkably, in the zero-shot scenario, it outperforms existing methods by over
3\% and even shows superior results against methods utilizing external training
data. Additionally, our method exhibits robust performance against natural
distribution shifts. Codes are available at \url{https://github.com/YBZh/DMN}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR2024; Codes are available at \url{https://github.com/YBZh/DMN}</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards a Zero-Data, Controllable, Adaptive Dialog System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17582v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17582v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dirk Väth, Lindsey Vanderlyn, Ngoc Thang Vu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conversational Tree Search (V\"ath et al., 2023) is a recent approach to
controllable dialog systems, where domain experts shape the behavior of a
Reinforcement Learning agent through a dialog tree. The agent learns to
efficiently navigate this tree, while adapting to information needs, e.g.,
domain familiarity, of different users. However, the need for additional
training data hinders deployment in new domains. To address this, we explore
approaches to generate this data directly from dialog trees. We improve the
original approach, and show that agents trained on synthetic data can achieve
comparable dialog success to models trained on human data, both when using a
commercial Large Language Model for generation, or when using a smaller
open-source model, running on a single GPU. We further demonstrate the
scalability of our approach by collecting and testing on two new datasets:
ONBOARD, a new domain helping foreign residents moving to a new city, and the
medical domain DIAGNOSE, a subset of Wikipedia articles related to scalp and
head symptoms. Finally, we perform human testing, where no statistically
significant differences were found in either objective or subjective measures
between models trained on human and generated data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Privacy in Federated Learning through Local Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17572v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17572v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicola Bastianello, Changxin Liu, Karl H. Johansson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we propose the federated private local training algorithm
(Fed-PLT) for federated learning, to overcome the challenges of (i) expensive
communications and (ii) privacy preservation. We address (i) by allowing for
both partial participation and local training, which significantly reduce the
number of communication rounds between the central coordinator and computing
agents. The algorithm matches the state of the art in the sense that the use of
local training demonstrably does not impact accuracy. Additionally, agents have
the flexibility to choose from various local training solvers, such as
(stochastic) gradient descent and accelerated gradient descent. Further, we
investigate how employing local training can enhance privacy, addressing point
(ii). In particular, we derive differential privacy bounds and highlight their
dependence on the number of local training epochs. We assess the effectiveness
of the proposed algorithm by comparing it to alternative techniques,
considering both theoretical analysis and numerical results from a
classification task.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Survey</span> on Deep Learning and State-of-the-arts Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17561v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17561v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohd Halim Mohd Noor, Ayokunle Olalekan Ige
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning, a branch of artificial intelligence, is a computational model
that uses multiple layers of interconnected units (neurons) to learn intricate
patterns and representations directly from raw input data. Empowered by this
learning capability, it has become a powerful tool for solving complex problems
and is the core driver of many groundbreaking technologies and innovations.
Building a deep learning model is a challenging task due to the algorithm`s
complexity and the dynamic nature of real-world problems. Several studies have
reviewed deep learning concepts and applications. However, the studies mostly
focused on the types of deep learning models and convolutional neural network
architectures, offering limited coverage of the state-of-the-art of deep
learning models and their applications in solving complex problems across
different domains. Therefore, motivated by the limitations, this study aims to
comprehensively review the state-of-the-art deep learning models in computer
vision, natural language processing, time series analysis and pervasive
computing. We highlight the key features of the models and their effectiveness
in solving the problems within each domain. Furthermore, this study presents
the fundamentals of deep learning, various deep learning model types and
prominent convolutional neural network architectures. Finally, challenges and
future directions in deep learning research are discussed to offer a broader
perspective for future researchers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DeepMIF: Deep Monotonic Implicit Fields for Large-Scale LiDAR 3D Mapping 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17550v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17550v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kutay Yılmaz, Matthias Nießner, Anastasiia Kornilova, Alexey Artemov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, significant progress has been achieved in sensing real large-scale
outdoor 3D environments, particularly by using modern acquisition equipment
such as LiDAR sensors. Unfortunately, they are fundamentally limited in their
ability to produce dense, complete 3D scenes. To address this issue, recent
learning-based methods integrate neural implicit representations and
optimizable feature grids to approximate surfaces of 3D scenes. However,
naively fitting samples along raw LiDAR rays leads to noisy 3D mapping results
due to the nature of sparse, conflicting LiDAR measurements. Instead, in this
work we depart from fitting LiDAR data exactly, instead letting the network
optimize a non-metric monotonic implicit field defined in 3D space. To fit our
field, we design a learning system integrating a monotonicity loss that enables
optimizing neural monotonic fields and leverages recent progress in large-scale
3D mapping. Our algorithm achieves high-quality dense 3D mapping performance as
captured by multiple quantitative and perceptual measures and visual results
obtained for Mai City, Newer College, and KITTI benchmarks. The code of our
approach will be made publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VDSC: Enhancing Exploration Timing with Value Discrepancy and State
  Counts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17542v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17542v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marius Captari, Remo Sasso, Matthia Sabatelli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the considerable attention given to the questions of \textit{how
much} and \textit{how to} explore in deep reinforcement learning, the
investigation into \textit{when} to explore remains relatively less researched.
While more sophisticated exploration strategies can excel in specific, often
sparse reward environments, existing simpler approaches, such as
$\epsilon$-greedy, persist in outperforming them across a broader spectrum of
domains. The appeal of these simpler strategies lies in their ease of
implementation and generality across a wide range of domains. The downside is
that these methods are essentially a blind switching mechanism, which
completely disregards the agent's internal state. In this paper, we propose to
leverage the agent's internal state to decide \textit{when} to explore,
addressing the shortcomings of blind switching mechanisms. We present Value
Discrepancy and State Counts through homeostasis (VDSC), a novel approach for
efficient exploration timing. Experimental results on the Atari suite
demonstrate the superiority of our strategy over traditional methods such as
$\epsilon$-greedy and Boltzmann, as well as more sophisticated techniques like
Noisy Nets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BVR Gym: A Reinforcement Learning Environment for Beyond-Visual-Range
  Air Combat 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17533v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17533v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Edvards Scukins, Markus Klein, Lars Kroon, Petter Ögren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Creating new air combat tactics and discovering novel maneuvers can require
numerous hours of expert pilots' time. Additionally, for each different combat
scenario, the same strategies may not work since small changes in equipment
performance may drastically change the air combat outcome. For this reason, we
created a reinforcement learning environment to help investigate potential air
combat tactics in the field of beyond-visual-range (BVR) air combat: the BVR
Gym. This type of air combat is important since long-range missiles are often
the first weapon to be used in aerial combat. Some existing environments
provide high-fidelity simulations but are either not open source or are not
adapted to the BVR air combat domain. Other environments are open source but
use less accurate simulation models. Our work provides a high-fidelity
environment based on the open-source flight dynamics simulator JSBSim and is
adapted to the BVR air combat domain. This article describes the building
blocks of the environment and some use cases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Boosting Adversarial Training via Fisher-Rao Norm-based Regularization <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17520v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17520v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiangyu Yin, Wenjie Ruan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adversarial training is extensively utilized to improve the adversarial
robustness of deep neural networks. Yet, mitigating the degradation of standard
generalization performance in adversarial-trained models remains an open
problem. This paper attempts to resolve this issue through the lens of model
complexity. First, We leverage the Fisher-Rao norm, a geometrically invariant
metric for model complexity, to establish the non-trivial bounds of the
Cross-Entropy Loss-based Rademacher complexity for a ReLU-activated Multi-Layer
Perceptron. Then we generalize a complexity-related variable, which is
sensitive to the changes in model width and the trade-off factors in
adversarial training. Moreover, intensive empirical evidence validates that
this variable highly correlates with the generalization gap of Cross-Entropy
loss between adversarial-trained and standard-trained models, especially during
the initial and final phases of the training process. Building upon this
observation, we propose a novel regularization framework, called Logit-Oriented
Adversarial Training (LOAT), which can mitigate the trade-off between
robustness and accuracy while imposing only a negligible increase in
computational overhead. Our extensive experiments demonstrate that the proposed
regularization strategy can boost the performance of the prevalent adversarial
training algorithms, including PGD-AT, TRADES, TRADES (LSE), MART, and DM-AT,
across various network architectures. Our code will be available at
https://github.com/TrustAI/LOAT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted to CVPR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Prediction-sharing During Training and Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17515v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17515v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yotam Gafni, Ronen Gradwohl, Moshe Tennenholtz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Two firms are engaged in a competitive prediction task. Each firm has two
sources of data -- labeled historical data and unlabeled inference-time data --
and uses the former to derive a prediction model, and the latter to make
predictions on new instances. We study data-sharing contracts between the
firms. The novelty of our study is to introduce and highlight the differences
between contracts that share prediction models only, contracts to share
inference-time predictions only, and contracts to share both. Our analysis
proceeds on three levels. First, we develop a general Bayesian framework that
facilitates our study. Second, we narrow our focus to two natural settings
within this framework: (i) a setting in which the accuracy of each firm's
prediction model is common knowledge, but the correlation between the
respective models is unknown; and (ii) a setting in which two hypotheses exist
regarding the optimal predictor, and one of the firms has a structural
advantage in deducing it. Within these two settings we study optimal contract
choice. More specifically, we find the individually rational and Pareto-optimal
contracts for some notable cases, and describe specific settings where each of
the different sharing contracts emerge as optimal. Finally, in the third level
of our analysis we demonstrate the applicability of our concepts in a synthetic
simulation using real loan data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EL-MLFFs: Ensemble Learning of Machine Leaning Force Fields 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17507v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17507v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bangchen Yin, Yue Yin, Yuda W. Tang, Hai Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning force fields (MLFFs) have emerged as a promising approach to
bridge the accuracy of quantum mechanical methods and the efficiency of
classical force fields. However, the abundance of MLFF models and the challenge
of accurately predicting atomic forces pose significant obstacles in their
practical application. In this paper, we propose a novel ensemble learning
framework, EL-MLFFs, which leverages the stacking method to integrate
predictions from diverse MLFFs and enhance force prediction accuracy. By
constructing a graph representation of molecular structures and employing a
graph neural network (GNN) as the meta-model, EL-MLFFs effectively captures
atomic interactions and refines force predictions. We evaluate our approach on
two distinct datasets: methane molecules and methanol adsorbed on a Cu(100)
surface. The results demonstrate that EL-MLFFs significantly improves force
prediction accuracy compared to individual MLFFs, with the ensemble of all
eight models yielding the best performance. Moreover, our ablation study
highlights the crucial roles of the residual network and graph attention layers
in the model's architecture. The EL-MLFFs framework offers a promising solution
to the challenges of model selection and force prediction accuracy in MLFFs,
paving the way for more reliable and efficient molecular simulations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DS-AL: A Dual-Stream Analytic Learning for Exemplar-Free
  Class-Incremental Learning <span class="chip">AAAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17503v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17503v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huiping Zhuang, Run He, Kai Tong, Ziqian Zeng, Cen Chen, Zhiping Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Class-incremental learning (CIL) under an exemplar-free constraint has
presented a significant challenge. Existing methods adhering to this constraint
are prone to catastrophic forgetting, far more so than replay-based techniques
that retain access to past samples. In this paper, to solve the exemplar-free
CIL problem, we propose a Dual-Stream Analytic Learning (DS-AL) approach. The
DS-AL contains a main stream offering an analytical (i.e., closed-form) linear
solution, and a compensation stream improving the inherent under-fitting
limitation due to adopting linear mapping. The main stream redefines the CIL
problem into a Concatenated Recursive Least Squares (C-RLS) task, allowing an
equivalence between the CIL and its joint-learning counterpart. The
compensation stream is governed by a Dual-Activation Compensation (DAC) module.
This module re-activates the embedding with a different activation function
from the main stream one, and seeks fitting compensation by projecting the
embedding to the null space of the main stream's linear mapping. Empirical
results demonstrate that the DS-AL, despite being an exemplar-free technique,
delivers performance comparable with or better than that of replay-based
methods across various datasets, including CIFAR-100, ImageNet-100 and
ImageNet-Full. Additionally, the C-RLS' equivalent property allows the DS-AL to
execute CIL in a phase-invariant manner. This is evidenced by a
never-before-seen 500-phase CIL ImageNet task, which performs on a level
identical to a 5-phase one. Our codes are available at
https://github.com/ZHUANGHP/Analytic-continual-learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in AAAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Variational Graph Auto-Encoder Based Inductive Learning Method for
  Semi-Supervised Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17500v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17500v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanxuan Yang, Zhaoxin Yu, Qingchao Kong, Wei Liu, Wenji Mao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph representation learning is a fundamental research issue in various
domains of applications, of which the inductive learning problem is
particularly challenging as it requires models to generalize to unseen graph
structures during inference. In recent years, graph neural networks (GNNs) have
emerged as powerful graph models for inductive learning tasks such as node
classification, whereas they typically heavily rely on the annotated nodes
under a fully supervised training setting. Compared with the GNN-based methods,
variational graph auto-encoders (VGAEs) are known to be more generalizable to
capture the internal structural information of graphs independent of node
labels and have achieved prominent performance on multiple unsupervised
learning tasks. However, so far there is still a lack of work focusing on
leveraging the VGAE framework for inductive learning, due to the difficulties
in training the model in a supervised manner and avoiding over-fitting the
proximity information of graphs. To solve these problems and improve the model
performance of VGAEs for inductive graph representation learning, in this work,
we propose the Self-Label Augmented VGAE model. To leverage the label
information for training, our model takes node labels as one-hot encoded inputs
and then performs label reconstruction in model training. To overcome the
scarcity problem of node labels for semi-supervised settings, we further
propose the Self-Label Augmentation Method (SLAM), which uses pseudo labels
generated by our model with a node-wise masking approach to enhance the label
information. Experiments on benchmark inductive learning graph datasets verify
that our proposed model archives promising results on node classification with
particular superiority under semi-supervised learning settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Capacity Provisioning Motivated Online Non-Convex Optimization Problem
  with Memory and Switching Cost 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17480v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17480v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rahul Vaze, Jayakrishnan Nair
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  An online non-convex optimization problem is considered where the goal is to
minimize the flow time (total delay) of a set of jobs by modulating the number
of active servers, but with a switching cost associated with changing the
number of active servers over time. Each job can be processed by at most one
fixed speed server at any time. Compared to the usual online convex
optimization (OCO) problem with switching cost, the objective function
considered is non-convex and more importantly, at each time, it depends on all
past decisions and not just the present one. Both worst-case and stochastic
inputs are considered; for both cases, competitive algorithms are derived.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Natural Language Requirements Testability Measurement Based on
  Requirement Smells 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17479v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17479v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Morteza Zakeri-Nasrabadi, Saeed Parsa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Requirements form the basis for defining software systems' obligations and
tasks. Testable requirements help prevent failures, reduce maintenance costs,
and make it easier to perform acceptance tests. However, despite the importance
of measuring and quantifying requirements testability, no automatic approach
for measuring requirements testability has been proposed based on the
requirements smells, which are at odds with the requirements testability. This
paper presents a mathematical model to evaluate and rank the natural language
requirements testability based on an extensive set of nine requirements smells,
detected automatically, and acceptance test efforts determined by requirement
length and its application domain. Most of the smells stem from uncountable
adjectives, context-sensitive, and ambiguous words. A comprehensive dictionary
is required to detect such words. We offer a neural word-embedding technique to
generate such a dictionary automatically. Using the dictionary, we could
automatically detect Polysemy smell (domain-specific ambiguity) for the first
time in 10 application domains. Our empirical study on nearly 1000 software
requirements from six well-known industrial and academic projects demonstrates
that the proposed smell detection approach outperforms Smella, a
state-of-the-art tool, in detecting requirements smells. The precision and
recall of smell detection are improved with an average of 0.03 and 0.33,
respectively, compared to the state-of-the-art. The proposed requirement
testability model measures the testability of 985 requirements with a mean
absolute error of 0.12 and a mean squared error of 0.03, demonstrating the
model's potential for practical use.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>45 pages, 16 figures, and 13 tables; submitted as a journal paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Unified Kernel for Neural Network Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17467v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17467v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shao-Qun Zhang, Zong-Yi Chen, Yong-Ming Tian, Xun Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Past decades have witnessed a great interest in the distinction and
connection between neural network learning and kernel learning. Recent
advancements have made theoretical progress in connecting infinite-wide neural
networks and Gaussian processes. Two predominant approaches have emerged: the
Neural Network Gaussian Process (NNGP) and the Neural Tangent Kernel (NTK). The
former, rooted in Bayesian inference, represents a zero-order kernel, while the
latter, grounded in the tangent space of gradient descents, is a first-order
kernel. In this paper, we present the Unified Neural Kernel (UNK), which
characterizes the learning dynamics of neural networks with gradient descents
and parameter initialization. The proposed UNK kernel maintains the limiting
properties of both NNGP and NTK, exhibiting behaviors akin to NTK with a finite
learning step and converging to NNGP as the learning step approaches infinity.
Besides, we also theoretically characterize the uniform tightness and learning
convergence of the UNK kernel, providing comprehensive insights into this
unified kernel. Experimental results underscore the effectiveness of our
proposed method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Expectations Versus Reality: Evaluating Intrusion Detection Systems in
  Practice 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17458v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17458v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jake Hesford, Daniel Cheng, Alan Wan, Larry Huynh, Seungho Kim, Hyoungshick Kim, Jin B. Hong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Our paper provides empirical comparisons between recent IDSs to provide an
objective comparison between them to help users choose the most appropriate
solution based on their requirements. Our results show that no one solution is
the best, but is dependent on external variables such as the types of attacks,
complexity, and network environment in the dataset. For example, BoT_IoT and
Stratosphere IoT datasets both capture IoT-related attacks, but the deep neural
network performed the best when tested using the BoT_IoT dataset while HELAD
performed the best when tested using the Stratosphere IoT dataset. So although
we found that a deep neural network solution had the highest average F1 scores
on tested datasets, it is not always the best-performing one. We further
discuss difficulties in using IDS from literature and project repositories,
which complicated drawing definitive conclusions regarding IDS selection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Imitating Cost-Constrained Behaviors in Reinforcement Learning <span class="chip">ICAPS-24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17456v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17456v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qian Shao, Pradeep Varakantham, Shih-Fen Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Complex planning and scheduling problems have long been solved using various
optimization or heuristic approaches. In recent years, imitation learning that
aims to learn from expert demonstrations has been proposed as a viable
alternative to solving these problems. Generally speaking, imitation learning
is designed to learn either the reward (or preference) model or directly the
behavioral policy by observing the behavior of an expert. Existing work in
imitation learning and inverse reinforcement learning has focused on imitation
primarily in unconstrained settings (e.g., no limit on fuel consumed by the
vehicle). However, in many real-world domains, the behavior of an expert is
governed not only by reward (or preference) but also by constraints. For
instance, decisions on self-driving delivery vehicles are dependent not only on
the route preferences/rewards (depending on past demand data) but also on the
fuel in the vehicle and the time available. In such problems, imitation
learning is challenging as decisions are not only dictated by the reward model
but are also dependent on a cost-constrained model. In this paper, we provide
multiple methods that match expert distributions in the presence of trajectory
cost constraints through (a) Lagrangian-based method; (b) Meta-gradients to
find a good trade-off between expected return and minimizing constraint
violation; and (c) Cost-violation-based alternating gradient. We empirically
show that leading imitation learning approaches imitate cost-constrained
behaviors poorly and our meta-gradient-based approach achieves the best
performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the 34th International Conference on Automated Planning
  and Scheduling (ICAPS-24)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Chain of Compression: A Systematic Approach to Combinationally Compress
  Convolutional Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17447v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17447v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingtao Shen, Minqing Sun, Jie Zhao, An Zou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Convolutional neural networks (CNNs) have achieved significant popularity,
but their computational and memory intensity poses challenges for
resource-constrained computing systems, particularly with the prerequisite of
real-time performance. To release this burden, model compression has become an
important research focus. Many approaches like quantization, pruning, early
exit, and knowledge distillation have demonstrated the effect of reducing
redundancy in neural networks. Upon closer examination, it becomes apparent
that each approach capitalizes on its unique features to compress the neural
network, and they can also exhibit complementary behavior when combined. To
explore the interactions and reap the benefits from the complementary features,
we propose the Chain of Compression, which works on the combinational sequence
to apply these common techniques to compress the neural network. Validated on
the image-based regression and classification networks across different data
sets, our proposed Chain of Compression can significantly compress the
computation cost by 100-1000 times with ignorable accuracy loss compared with
the baseline model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 15 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Incorporating Exponential Smoothing into MLP: A Simple but Effective
  Sequence Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17445v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17445v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiqun Chu, Zuoquan Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modeling long-range dependencies in sequential data is a crucial step in
sequence learning. A recently developed model, the Structured State Space (S4),
demonstrated significant effectiveness in modeling long-range sequences.
However, It is unclear whether the success of S4 can be attributed to its
intricate parameterization and HiPPO initialization or simply due to State
Space Models (SSMs). To further investigate the potential of the deep SSMs, we
start with exponential smoothing (ETS), a simple SSM, and propose a stacked
architecture by directly incorporating it into an element-wise MLP. We augment
simple ETS with additional parameters and complex field to reduce the inductive
bias. Despite increasing less than 1\% of parameters of element-wise MLP, our
models achieve comparable results to S4 on the LRA benchmark.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 5 tables, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Particle identification with machine learning from incomplete data in
  the ALICE experiment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17436v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17436v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maja Karwowska, Łukasz Graczykowski, Kamil Deja, Miłosz Kasak, Małgorzata Janik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ALICE experiment at the LHC measures properties of the strongly
interacting matter formed in ultrarelativistic heavy-ion collisions. Such
studies require accurate particle identification (PID). ALICE provides PID
information via several detectors for particles with momentum from about 100
MeV/c up to 20 GeV/c. Traditionally, particles are selected with rectangular
cuts. Acmuch better performance can be achieved with machine learning (ML)
methods. Our solution uses multiple neural networks (NN) serving as binary
classifiers. Moreover, we extended our particle classifier with Feature Set
Embedding and attention in order to train on data with incomplete samples. We
also present the integration of the ML project with the ALICE analysis
software, and we discuss domain adaptation, the ML technique needed to transfer
the knowledge between simulated and real experimental data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Proceedings of 3rd Artificial Intelligence for the Electron Ion
  Collider workshop -- AI4EIC2023, 28.11-1.12.2023. Prepared for submission to
  JINST</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust and Scalable Model Editing for Large Language Models <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17431v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17431v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingfa Chen, Zhengyan Zhang, Xu Han, Chaojun Xiao, Zhiyuan Liu, Chen Chen, Kuai Li, Tao Yang, Maosong Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) can make predictions using parametric
knowledge--knowledge encoded in the model weights--or contextual
knowledge--knowledge presented in the context. In many scenarios, a desirable
behavior is that LLMs give precedence to contextual knowledge when it conflicts
with the parametric knowledge, and fall back to using their parametric
knowledge when the context is irrelevant. This enables updating and correcting
the model's knowledge by in-context editing instead of retraining. Previous
works have shown that LLMs are inclined to ignore contextual knowledge and fail
to reliably fall back to parametric knowledge when presented with irrelevant
context. In this work, we discover that, with proper prompting methods,
instruction-finetuned LLMs can be highly controllable by contextual knowledge
and robust to irrelevant context. Utilizing this feature, we propose EREN (Edit
models by REading Notes) to improve the scalability and robustness of LLM
editing. To better evaluate the robustness of model editors, we collect a new
dataset, that contains irrelevant questions that are more challenging than the
ones in existing datasets. Empirical results show that our method outperforms
current state-of-the-art methods by a large margin. Unlike existing techniques,
it can integrate knowledge from multiple edits, and correctly respond to
syntactically similar but semantically unrelated inputs (and vice versa). The
source code can be found at https://github.com/thunlp/EREN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LREC-COLING 2024 paper, 16 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Masked Multi-Domain Network: Multi-Type and Multi-Scenario Conversion
  Rate Prediction with a Single Model <span class="chip">CIKM 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17425v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17425v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wentao Ouyang, Xiuwu Zhang, Chaofeng Guo, Shukui Ren, Yupei Sui, Kun Zhang, Jinmei Luo, Yunfeng Chen, Dongbo Xu, Xiangzheng Liu, Yanlong Du
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In real-world advertising systems, conversions have different types in nature
and ads can be shown in different display scenarios, both of which highly
impact the actual conversion rate (CVR). This results in the multi-type and
multi-scenario CVR prediction problem. A desired model for this problem should
satisfy the following requirements: 1) Accuracy: the model should achieve
fine-grained accuracy with respect to any conversion type in any display
scenario. 2) Scalability: the model parameter size should be affordable. 3)
Convenience: the model should not require a large amount of effort in data
partitioning, subset processing and separate storage. Existing approaches
cannot simultaneously satisfy these requirements. For example, building a
separate model for each (conversion type, display scenario) pair is neither
scalable nor convenient. Building a unified model trained on all the data with
conversion type and display scenario included as two features is not accurate
enough. In this paper, we propose the Masked Multi-domain Network (MMN) to
solve this problem. To achieve the accuracy requirement, we model
domain-specific parameters and propose a dynamically weighted loss to account
for the loss scale imbalance issue within each mini-batch. To achieve the
scalability requirement, we propose a parameter sharing and composition
strategy to reduce model parameters from a product space to a sum space. To
achieve the convenience requirement, we propose an auto-masking strategy which
can take mixed data from all the domains as input. It avoids the overhead
caused by data partitioning, individual processing and separate storage. Both
offline and online experimental results validate the superiority of MMN for
multi-type and multi-scenario CVR prediction. MMN is now the serving model for
real-time CVR prediction in UC Toutiao.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CIKM 2023 (larger figures)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On permutation-invariant neural networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17410v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17410v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Masanari Kimura, Ryotaro Shimizu, Yuki Hirakawa, Ryosuke Goto, Yuki Saito
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conventional machine learning algorithms have traditionally been designed
under the assumption that input data follows a vector-based format, with an
emphasis on vector-centric paradigms. However, as the demand for tasks
involving set-based inputs has grown, there has been a paradigm shift in the
research community towards addressing these challenges. In recent years, the
emergence of neural network architectures such as Deep Sets and Transformers
has presented a significant advancement in the treatment of set-based data.
These architectures are specifically engineered to naturally accommodate sets
as input, enabling more effective representation and processing of set
structures. Consequently, there has been a surge of research endeavors
dedicated to exploring and harnessing the capabilities of these architectures
for various tasks involving the approximation of set functions. This
comprehensive survey aims to provide an overview of the diverse problem
settings and ongoing research efforts pertaining to neural networks that
approximate set functions. By delving into the intricacies of these approaches
and elucidating the associated challenges, the survey aims to equip readers
with a comprehensive understanding of the field. Through this comprehensive
perspective, we hope that researchers can gain valuable insights into the
potential applications, inherent limitations, and future directions of
set-based neural networks. Indeed, from this survey we gain two insights: i)
Deep Sets and its variants can be generalized by differences in the aggregation
function, and ii) the behavior of Deep Sets is sensitive to the choice of the
aggregation function. From these observations, we show that Deep Sets, one of
the well-known permutation-invariant neural networks, can be generalized in the
sense of a quasi-arithmetic mean.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Transcribing Bengali Text with Regional Dialects to IPA using District
  Guided Tokens 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17407v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17407v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        S M Jishanul Islam, Sadia Ahmmed, Sahid Hossain Mustakim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate transcription of Bengali text to the International Phonetic Alphabet
(IPA) is a challenging task due to the complex phonology of the language and
context-dependent sound changes. This challenge is even more for regional
Bengali dialects due to unavailability of standardized spelling conventions for
these dialects, presence of local and foreign words popular in those regions
and phonological diversity across different regions. This paper presents an
approach to this sequence-to-sequence problem by introducing the District
Guided Tokens (DGT) technique on a new dataset spanning six districts of
Bangladesh. The key idea is to provide the model with explicit information
about the regional dialect or "district" of the input text before generating
the IPA transcription. This is achieved by prepending a district token to the
input sequence, effectively guiding the model to understand the unique phonetic
patterns associated with each district. The DGT technique is applied to
fine-tune several transformer-based models, on this new dataset. Experimental
results demonstrate the effectiveness of DGT, with the ByT5 model achieving
superior performance over word-based models like mT5, BanglaT5, and umT5. This
is attributed to ByT5's ability to handle a high percentage of
out-of-vocabulary words in the test set. The proposed approach highlights the
importance of incorporating regional dialect information into ubiquitous
natural language processing systems for languages with diverse phonological
variations. The following work was a result of the "Bhashamul" challenge, which
is dedicated to solving the problem of Bengali text with regional dialects to
IPA transcription https://www.kaggle.com/competitions/regipa/. The training and
inference notebooks are available through the competition link.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work became the champion of the Bhashamul challenge</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generalization Error Analysis for Sparse Mixture-of-Experts: A
  Preliminary Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17404v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17404v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinze Zhao, Peihao Wang, Zhangyang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mixture-of-Experts (MoE) represents an ensemble methodology that amalgamates
predictions from several specialized sub-models (referred to as experts). This
fusion is accomplished through a router mechanism, dynamically assigning
weights to each expert's contribution based on the input data. Conventional MoE
mechanisms select all available experts, incurring substantial computational
costs. In contrast, Sparse Mixture-of-Experts (Sparse MoE) selectively engages
only a limited number, or even just one expert, significantly reducing
computation overhead while empirically preserving, and sometimes even
enhancing, performance. Despite its wide-ranging applications and these
advantageous characteristics, MoE's theoretical underpinnings have remained
elusive. In this paper, we embark on an exploration of Sparse MoE's
generalization error concerning various critical factors. Specifically, we
investigate the impact of the number of data samples, the total number of
experts, the sparsity in expert selection, the complexity of the routing
mechanism, and the complexity of individual experts. Our analysis sheds light
on \textit{how \textbf{sparsity} contributes to the MoE's generalization},
offering insights from the perspective of classical learning theory.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Application-Driven Innovation in Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17381v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17381v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Rolnick, Alan Aspuru-Guzik, Sara Beery, Bistra Dilkina, Priya L. Donti, Marzyeh Ghassemi, Hannah Kerner, Claire Monteleoni, Esther Rolf, Milind Tambe, Adam White
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As applications of machine learning proliferate, innovative algorithms
inspired by specific real-world challenges have become increasingly important.
Such work offers the potential for significant impact not merely in domains of
application but also in machine learning itself. In this paper, we describe the
paradigm of application-driven research in machine learning, contrasting it
with the more standard paradigm of methods-driven research. We illustrate the
benefits of application-driven machine learning and how this approach can
productively synergize with methods-driven work. Despite these benefits, we
find that reviewing, hiring, and teaching practices in machine learning often
hold back application-driven innovation. We outline how these processes may be
improved.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self-Rectifying Diffusion Sampling with Perturbed-Attention Guidance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17377v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17377v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Donghoon Ahn, Hyoungwon Cho, Jaewon Min, Wooseok Jang, Jungwoo Kim, SeonHwa Kim, Hyun Hee Park, Kyong Hwan Jin, Seungryong Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies have demonstrated that diffusion models are capable of
generating high-quality samples, but their quality heavily depends on sampling
guidance techniques, such as classifier guidance (CG) and classifier-free
guidance (CFG). These techniques are often not applicable in unconditional
generation or in various downstream tasks such as image restoration. In this
paper, we propose a novel sampling guidance, called Perturbed-Attention
Guidance (PAG), which improves diffusion sample quality across both
unconditional and conditional settings, achieving this without requiring
additional training or the integration of external modules. PAG is designed to
progressively enhance the structure of samples throughout the denoising
process. It involves generating intermediate samples with degraded structure by
substituting selected self-attention maps in diffusion U-Net with an identity
matrix, by considering the self-attention mechanisms' ability to capture
structural information, and guiding the denoising process away from these
degraded samples. In both ADM and Stable Diffusion, PAG surprisingly improves
sample quality in conditional and even unconditional scenarios. Moreover, PAG
significantly improves the baseline performance in various downstream tasks
where existing guidances such as CG or CFG cannot be fully utilized, including
ControlNet with empty prompts and image restoration such as inpainting and
deblurring.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page is available at
  https://ku-cvlab.github.io/Perturbed-Attention-Guidance</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AIDE: An Automatic Data Engine for Object Detection in Autonomous
  Driving <span class="chip">CVPR-2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17373v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17373v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingfu Liang, Jong-Chyi Su, Samuel Schulter, Sparsh Garg, Shiyu Zhao, Ying Wu, Manmohan Chandraker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous vehicle (AV) systems rely on robust perception models as a
cornerstone of safety assurance. However, objects encountered on the road
exhibit a long-tailed distribution, with rare or unseen categories posing
challenges to a deployed perception model. This necessitates an expensive
process of continuously curating and annotating data with significant human
effort. We propose to leverage recent advances in vision-language and large
language models to design an Automatic Data Engine (AIDE) that automatically
identifies issues, efficiently curates data, improves the model through
auto-labeling, and verifies the model through generation of diverse scenarios.
This process operates iteratively, allowing for continuous self-improvement of
the model. We further establish a benchmark for open-world detection on AV
datasets to comprehensively evaluate various learning paradigms, demonstrating
our method's superior performance at a reduced cost.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR-2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Moreau Envelope Approach for LQR Meta-Policy Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17364v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17364v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ashwin Aravind, Mohammad Taha Toghani, César A. Uribe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the problem of policy estimation for the Linear Quadratic Regulator
(LQR) in discrete-time linear time-invariant uncertain dynamical systems. We
propose a Moreau Envelope-based surrogate LQR cost, built from a finite set of
realizations of the uncertain system, to define a meta-policy efficiently
adjustable to new realizations. Moreover, we design an algorithm to find an
approximate first-order stationary point of the meta-LQR cost function.
Numerical results show that the proposed approach outperforms naive averaging
of controllers on new realizations of the linear system. We also provide
empirical evidence that our method has better sample complexity than
Model-Agnostic Meta-Learning (MAML) approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Objective Trajectory Planning with Dual-Encoder 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17353v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17353v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Beibei Zhang, Tian Xiang, Chentao Mao, Yuhua Zheng, Shuai Li, Haoyi Niu, Xiangming Xi, Wenyuan Bai, Feng Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Time-jerk optimal trajectory planning is crucial in advancing robotic arms'
performance in dynamic tasks. Traditional methods rely on solving complex
nonlinear programming problems, bringing significant delays in generating
optimized trajectories. In this paper, we propose a two-stage approach to
accelerate time-jerk optimal trajectory planning. Firstly, we introduce a
dual-encoder based transformer model to establish a good preliminary
trajectory. This trajectory is subsequently refined through sequential
quadratic programming to improve its optimality and robustness. Our approach
outperforms the state-of-the-art by up to 79.72\% in reducing trajectory
planning time. Compared with existing methods, our method shrinks the
optimality gap with the objective function value decreasing by up to 29.9\%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 7 figures, conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learn from Heterophily: Heterophilous Information-enhanced Graph Neural
  Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17351v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17351v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yilun Zheng, Jiahao Xu, Lihui Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Under circumstances of heterophily, where nodes with different labels tend to
be connected based on semantic meanings, Graph Neural Networks (GNNs) often
exhibit suboptimal performance. Current studies on graph heterophily mainly
focus on aggregation calibration or neighbor extension and address the
heterophily issue by utilizing node features or structural information to
improve GNN representations. In this paper, we propose and demonstrate that the
valuable semantic information inherent in heterophily can be utilized
effectively in graph learning by investigating the distribution of neighbors
for each individual node within the graph. The theoretical analysis is carried
out to demonstrate the efficacy of the idea in enhancing graph learning. Based
on this analysis, we propose HiGNN, an innovative approach that constructs an
additional new graph structure, that integrates heterophilous information by
leveraging node distribution to enhance connectivity between nodes that share
similar semantic characteristics. We conduct empirical assessments on node
classification tasks using both homophilous and heterophilous benchmark
datasets and compare HiGNN to popular GNN baselines and SoTA methods,
confirming the effectiveness in improving graph representations. In addition,
by incorporating heterophilous information, we demonstrate a notable
enhancement in existing GNN-based approaches, and the homophily degree across
real-world datasets, thus affirming the efficacy of our approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Language Models are Free Boosters for Biomedical Imaging Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17343v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17343v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhixin Lai, Jing Wu, Suiyao Chen, Yucheng Zhou, Anna Hovakimyan, Naira Hovakimyan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we uncover the unexpected efficacy of residual-based large
language models (LLMs) as part of encoders for biomedical imaging tasks, a
domain traditionally devoid of language or textual data. The approach diverges
from established methodologies by utilizing a frozen transformer block,
extracted from pre-trained LLMs, as an innovative encoder layer for the direct
processing of visual tokens. This strategy represents a significant departure
from the standard multi-modal vision-language frameworks, which typically hinge
on language-driven prompts and inputs. We found that these LLMs could boost
performance across a spectrum of biomedical imaging applications, including
both 2D and 3D visual classification tasks, serving as plug-and-play boosters.
More interestingly, as a byproduct, we found that the proposed framework
achieved superior performance, setting new state-of-the-art results on
extensive, standardized datasets in MedMNIST-2D and 3D. Through this work, we
aim to open new avenues for employing LLMs in biomedical imaging and enriching
the understanding of their potential in this specialized domain.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Pursuit of Fairness in Artificial Intelligence Models: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17333v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17333v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tahsin Alamgir Kheya, Mohamed Reda Bouadjenek, Sunil Aryal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial Intelligence (AI) models are now being utilized in all facets of
our lives such as healthcare, education and employment. Since they are used in
numerous sensitive environments and make decisions that can be life altering,
potential biased outcomes are a pressing matter. Developers should ensure that
such models don't manifest any unexpected discriminatory practices like
partiality for certain genders, ethnicities or disabled people. With the
ubiquitous dissemination of AI systems, researchers and practitioners are
becoming more aware of unfair models and are bound to mitigate bias in them.
Significant research has been conducted in addressing such issues to ensure
models don't intentionally or unintentionally perpetuate bias. This survey
offers a synopsis of the different ways researchers have promoted fairness in
AI systems. We explore the different definitions of fairness existing in the
current literature. We create a comprehensive taxonomy by categorizing
different types of bias and investigate cases of biased AI in different
application domains. A thorough study is conducted of the approaches and
techniques employed by researchers to mitigate bias in AI models. Moreover, we
also delve into the impact of biased models on user experience and the ethical
considerations to contemplate when developing and deploying such models. We
hope this survey helps researchers and practitioners understand the intricate
details of fairness and bias in AI systems. By sharing this thorough survey, we
aim to promote additional discourse in the domain of equitable and responsible
AI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>37 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Support Vectors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17329v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17329v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junhoo Lee, Hyunho Lee, Kyomin Hwang, Nojun Kwak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While the success of deep learning is commonly attributed to its theoretical
equivalence with Support Vector Machines (SVM), the practical implications of
this relationship have not been thoroughly explored. This paper pioneers an
exploration in this domain, specifically focusing on the identification of Deep
Support Vectors (DSVs) within deep learning models. We introduce the concept of
DeepKKT conditions, an adaptation of the traditional Karush-Kuhn-Tucker (KKT)
conditions tailored for deep learning. Through empirical investigations, we
illustrate that DSVs exhibit similarities to support vectors in SVM, offering a
tangible method to interpret the decision-making criteria of models.
Additionally, our findings demonstrate that models can be effectively
reconstructed using DSVs, resembling the process in SVM. The code will be
available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LocalTweets to LocalHealth: A Mental Health Surveillance Framework Based
  on Twitter Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.13452v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.13452v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vijeta Deshpande, Minhwa Lee, Zonghai Yao, Zihao Zhang, Jason Brian Gibbons, Hong Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prior research on Twitter (now X) data has provided positive evidence of its
utility in developing supplementary health surveillance systems. In this study,
we present a new framework to surveil public health, focusing on mental health
(MH) outcomes. We hypothesize that locally posted tweets are indicative of
local MH outcomes and collect tweets posted from 765 neighborhoods (census
block groups) in the USA. We pair these tweets from each neighborhood with the
corresponding MH outcome reported by the Center for Disease Control (CDC) to
create a benchmark dataset, LocalTweets. With LocalTweets, we present the first
population-level evaluation task for Twitter-based MH surveillance systems. We
then develop an efficient and effective method, LocalHealth, for predicting MH
outcomes based on LocalTweets. When used with GPT3.5, LocalHealth achieves the
highest F1-score and accuracy of 0.7429 and 79.78\%, respectively, a 59\%
improvement in F1-score over the GPT3.5 in zero-shot setting. We also utilize
LocalHealth to extrapolate CDC's estimates to proxy unreported neighborhoods,
achieving an F1-score of 0.7291. Our work suggests that Twitter data can be
effectively leveraged to simulate neighborhood-level MH outcomes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Simple and Scalable Strategies to Continually <span class="highlight-title">Pre-train</span> Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08763v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08763v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adam Ibrahim, Benjamin Thérien, Kshitij Gupta, Mats L. Richter, Quentin Anthony, Timothée Lesort, Eugene Belilovsky, Irina Rish
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are routinely pre-trained on billions of tokens,
only to start the process over again once new data becomes available. A much
more efficient solution is to continually pre-train these models, saving
significant compute compared to re-training. However, the distribution shift
induced by new data typically results in degraded performance on previous data
or poor adaptation to the new data. In this work, we show that a simple and
scalable combination of learning rate (LR) re-warming, LR re-decaying, and
replay of previous data is sufficient to match the performance of fully
re-training from scratch on all available data, as measured by the final loss
and the average score on several language model (LM) evaluation benchmarks.
Specifically, we show this for a weak but realistic distribution shift between
two commonly used LLM pre-training datasets (English$\rightarrow$English) and a
stronger distribution shift (English$\rightarrow$German) at the $405$M
parameter model scale with large dataset sizes (hundreds of billions of
tokens). Selecting the weak but realistic shift for larger-scale experiments,
we also find that our continual learning strategies match the re-training
baseline for a 10B parameter LLM. Our results demonstrate that LLMs can be
successfully updated via simple and scalable continual learning strategies,
matching the re-training baseline using only a fraction of the compute.
Finally, inspired by previous work, we propose alternatives to the cosine
learning rate schedule that help circumvent forgetting induced by LR re-warming
and that are not bound to a fixed token budget.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An optimal control perspective on diffusion-based generative modeling <span class="chip">NeurIPS 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.01364v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.01364v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julius Berner, Lorenz Richter, Karen Ullrich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We establish a connection between stochastic optimal control and generative
models based on stochastic differential equations (SDEs), such as recently
developed diffusion probabilistic models. In particular, we derive a
Hamilton-Jacobi-Bellman equation that governs the evolution of the
log-densities of the underlying SDE marginals. This perspective allows to
transfer methods from optimal control theory to generative modeling. First, we
show that the evidence lower bound is a direct consequence of the well-known
verification theorem from control theory. Further, we can formulate
diffusion-based generative modeling as a minimization of the Kullback-Leibler
divergence between suitable measures in path space. Finally, we develop a novel
diffusion-based method for sampling from unnormalized densities -- a problem
frequently occurring in statistics and computational sciences. We demonstrate
that our time-reversed diffusion sampler (DIS) can outperform other
diffusion-based sampling approaches on multiple numerical examples.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for oral presentation at NeurIPS 2022 Workshop on
  Score-Based Methods</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fully Independent Communication in Multi-Agent Reinforcement Learning <span class="chip">AAMAS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.15059v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.15059v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rafael Pina, Varuna De Silva, Corentin Artaud, Xiaolan Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-Agent Reinforcement Learning (MARL) comprises a broad area of research
within the field of multi-agent systems. Several recent works have focused
specifically on the study of communication approaches in MARL. While multiple
communication methods have been proposed, these might still be too complex and
not easily transferable to more practical contexts. One of the reasons for that
is due to the use of the famous parameter sharing trick. In this paper, we
investigate how independent learners in MARL that do not share parameters can
communicate. We demonstrate that this setting might incur into some problems,
to which we propose a new learning scheme as a solution. Our results show that,
despite the challenges, independent agents can still learn communication
strategies following our method. Additionally, we use this method to
investigate how communication in MARL is affected by different network
capacities, both for sharing and not sharing parameters. We observe that
communication may not always be needed and that the chosen agent network sizes
need to be considered when used together with communication in order to achieve
efficient learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Extended version of the paper appearing on AAMAS 2024 with the same
  title. 11 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A randomized algorithm for nonconvex minimization with inexact
  evaluations and complexity guarantees 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.18841v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.18841v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuyao Li, Stephen J. Wright
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider minimization of a smooth nonconvex function with inexact oracle
access to gradient and Hessian (without assuming access to the function value)
to achieve approximate second-order optimality. A novel feature of our method
is that if an approximate direction of negative curvature is chosen as the
step, we choose its sense to be positive or negative with equal probability. We
allow gradients to be inexact in a relative sense and relax the coupling
between inexactness thresholds for the first- and second-order optimality
conditions. Our convergence analysis includes both an expectation bound based
on martingale analysis and a high-probability bound based on concentration
inequalities. We apply our algorithm to empirical risk minimization problems
and obtain improved gradient sample complexity over existing works.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Borrowing Treasures from Neighbors: In-Context Learning for Multimodal
  Learning with Missing Modalities and Data Scarcity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09428v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09428v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuo Zhi, Ziquan Liu, Moe Elbadawi, Adam Daneshmend, Mine Orlu, Abdul Basit, Andreas Demosthenous, Miguel Rodrigues
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal machine learning with missing modalities is an increasingly
relevant challenge arising in various applications such as healthcare. This
paper extends the current research into missing modalities to the low-data
regime, i.e., a downstream task has both missing modalities and limited sample
size issues. This problem setting is particularly challenging and also
practical as it is often expensive to get full-modality data and sufficient
annotated training samples. We propose to use retrieval-augmented in-context
learning to address these two crucial issues by unleashing the potential of a
transformer's in-context learning ability. Diverging from existing methods,
which primarily belong to the parametric paradigm and often require sufficient
training samples, our work exploits the value of the available full-modality
data, offering a novel perspective on resolving the challenge. The proposed
data-dependent framework exhibits a higher degree of sample efficiency and is
empirically demonstrated to enhance the classification model's performance on
both full- and missing-modality data in the low-data regime across various
multimodal learning tasks. When only 1% of the training data are available, our
proposed method demonstrates an average improvement of 6.1% over a recent
strong baseline across various datasets and missing states. Notably, our method
also reduces the performance gap between full-modality and missing-modality
data compared with the baseline.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Probabilistically Rewired Message-Passing Neural Networks <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02156v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02156v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chendi Qian, Andrei Manolache, Kareem Ahmed, Zhe Zeng, Guy Van den Broeck, Mathias Niepert, Christopher Morris
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Message-passing graph neural networks (MPNNs) emerged as powerful tools for
processing graph-structured input. However, they operate on a fixed input graph
structure, ignoring potential noise and missing information. Furthermore, their
local aggregation mechanism can lead to problems such as over-squashing and
limited expressive power in capturing relevant graph structures. Existing
solutions to these challenges have primarily relied on heuristic methods, often
disregarding the underlying data distribution. Hence, devising principled
approaches for learning to infer graph structures relevant to the given
prediction task remains an open challenge. In this work, leveraging recent
progress in exact and differentiable $k$-subset sampling, we devise
probabilistically rewired MPNNs (PR-MPNNs), which learn to add relevant edges
while omitting less beneficial ones. For the first time, our theoretical
analysis explores how PR-MPNNs enhance expressive power, and we identify
precise conditions under which they outperform purely randomized approaches.
Empirically, we demonstrate that our approach effectively mitigates issues like
over-squashing and under-reaching. In addition, on established real-world
datasets, our method exhibits competitive or superior predictive performance
compared to traditional MPNN models and recent graph transformer architectures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimal Data Splitting in Distributed Optimization for Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.07809v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.07809v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniil Medyakov, Gleb Molodtsov, Aleksandr Beznosikov, Alexander Gasnikov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The distributed optimization problem has become increasingly relevant
recently. It has a lot of advantages such as processing a large amount of data
in less time compared to non-distributed methods. However, most distributed
approaches suffer from a significant bottleneck - the cost of communications.
Therefore, a large amount of research has recently been directed at solving
this problem. One such approach uses local data similarity. In particular,
there exists an algorithm provably optimally exploiting the similarity
property. But this result, as well as results from other works solve the
communication bottleneck by focusing only on the fact that communication is
significantly more expensive than local computing and does not take into
account the various capacities of network devices and the different
relationship between communication time and local computing expenses. We
consider this setup and the objective of this study is to achieve an optimal
ratio of distributed data between the server and local machines for any costs
of communications and local computations. The running times of the network are
compared between uniform and optimal distributions. The superior theoretical
performance of our solutions is experimentally validated.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dynamics of Moral Behavior in Heterogeneous Populations of Learning
  Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.04202v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.04202v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elizaveta Tennant, Stephen Hailes, Mirco Musolesi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Growing concerns about safety and alignment of AI systems highlight the
importance of embedding moral capabilities in artificial agents. A promising
solution is the use of learning from experience, i.e., Reinforcement Learning.
In multi-agent (social) environments, complex population-level phenomena may
emerge from interactions between individual learning agents. Many of the
existing studies rely on simulated social dilemma environments to study the
interactions of independent learning agents. However, they tend to ignore the
moral heterogeneity that is likely to be present in societies of agents in
practice. For example, at different points in time a single learning agent may
face opponents who are consequentialist (i.e., caring about maximizing some
outcome over time) or norm-based (i.e., focusing on conforming to a specific
norm here and now). The extent to which agents' co-development may be impacted
by such moral heterogeneity in populations is not well understood. In this
paper, we present a study of the learning dynamics of morally heterogeneous
populations interacting in a social dilemma setting. Using a Prisoner's Dilemma
environment with a partner selection mechanism, we investigate the extent to
which the prevalence of diverse moral agents in populations affects individual
agents' learning behaviors and emergent population-level outcomes. We observe
several types of non-trivial interactions between pro-social and anti-social
agents, and find that certain classes of moral agents are able to steer selfish
agents towards more cooperative behavior.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Room Transfer Function Reconstruction Using Complex-valued Neural
  Networks and Irregularly Distributed Microphones 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.04866v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.04866v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francesca Ronchini, Luca Comanducci, Mirco Pezzoli, Fabio Antonacci, Augusto Sarti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reconstructing the room transfer functions needed to calculate the complex
sound field in a room has several impor- tant real-world applications. However,
an unpractical number of microphones is often required. Recently, in addition
to classical signal processing methods, deep learning techniques have been
applied to reconstruct the room transfer function starting from a very limited
set of measurements at scattered points in the room. In this paper, we employ
complex-valued neural networks to estimate room transfer functions in the
frequency range of the first room resonances, using a few irregularly
distributed microphones. To the best of our knowledge, this is the first time
that complex-valued neural networks are used to estimate room transfer
functions. To analyze the benefits of applying complex- valued optimization to
the considered task, we compare the proposed technique with a state-of-the-art
kernel-based signal processing approach for sound field reconstruction, showing
that the proposed technique exhibits relevant advantages in terms of phase
accuracy and overall quality of the reconstructed sound field. For informative
purposes, we also compare the model with a similarly-structured data-driven
approach that, however, applies a real-valued neural network to reconstruct
only the magnitude of the sound field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to EUSIPCO 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Activations and Gradients Compression for Model-Parallel Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.07788v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.07788v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mikhail Rudakov, Aleksandr Beznosikov, Yaroslav Kholodov, Alexander Gasnikov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large neural networks require enormous computational clusters of machines.
Model-parallel training, when the model architecture is partitioned
sequentially between workers, is a popular approach for training modern models.
Information compression can be applied to decrease workers communication time,
as it is often a bottleneck in such systems. This work explores how
simultaneous compression of activations and gradients in model-parallel
distributed training setup affects convergence. We analyze compression methods
such as quantization and TopK compression, and also experiment with error
compensation techniques. Moreover, we employ TopK with AQ-SGD per-batch error
feedback approach. We conduct experiments on image classification and language
model fine-tuning tasks. Our findings demonstrate that gradients require milder
compression rates than activations. We observe that $K=10\%$ is the lowest TopK
compression level, which does not harm model convergence severely. Experiments
also show that models trained with TopK perform well only when compression is
also applied during inference. We find that error feedback techniques do not
improve model-parallel training compared to plain compression, but allow model
inference without compression with almost no quality drop. Finally, when
applied with the AQ-SGD approach, TopK stronger than with $ K=30\%$ worsens
model performance significantly.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 6 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Differentially private multivariate medians 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.06459v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.06459v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kelly Ramsay, Aukosh Jagannath, Shoja'eddin Chenouri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Statistical tools which satisfy rigorous privacy guarantees are necessary for
modern data analysis. It is well-known that robustness against contamination is
linked to differential privacy. Despite this fact, using multivariate medians
for differentially private and robust multivariate location estimation has not
been systematically studied. We develop novel finite-sample performance
guarantees for differentially private multivariate depth-based medians, which
are essentially sharp. Our results cover commonly used depth functions, such as
the halfspace (or Tukey) depth, spatial depth, and the integrated dual depth.
We show that under Cauchy marginals, the cost of heavy-tailed location
estimation outweighs the cost of privacy. We demonstrate our results
numerically using a Gaussian contamination model in dimensions up to d = 100,
and compare them to a state-of-the-art private mean estimation algorithm. As a
by-product of our investigation, we prove concentration inequalities for the
output of the exponential mechanism about the maximizer of the population
objective function. This bound applies to objective functions that satisfy a
mild regularity condition.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>42 pages, 3 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AI and Generative AI for Research Discovery and Summarization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.06795v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.06795v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mark Glickman, Yi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  AI and generative AI tools, including chatbots like ChatGPT that rely on
large language models (LLMs), have burst onto the scene this year, creating
incredible opportunities to increase work productivity and improve our lives.
Statisticians and data scientists have begun experiencing the benefits from the
availability of these tools in numerous ways, such as the generation of
programming code from text prompts to analyze data or fit statistical models.
One area that these tools can make a substantial impact is in research
discovery and summarization. Standalone tools and plugins to chatbots are being
developed that allow researchers to more quickly find relevant literature than
pre-2023 search tools. Furthermore, generative AI tools have improved to the
point where they can summarize and extract the key points from research
articles in succinct language. Finally, chatbots based on highly parameterized
LLMs can be used to simulate abductive reasoning, which provides researchers
the ability to make connections among related technical topics, which can also
be used for research discovery. We review the developments in AI and generative
AI for research discovery and summarization, and propose directions where these
types of tools are likely to head in the future that may be of interest to
statistician and data scientists.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PINN surrogate of Li-ion battery models for parameter inference. Part
  II: Regularization and application of the pseudo-2D model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.17336v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.17336v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Malik Hassanaly, Peter J. Weddle, Ryan N. King, Subhayan De, Alireza Doostan, Corey R. Randall, Eric J. Dufek, Andrew M. Colclasure, Kandler Smith
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bayesian parameter inference is useful to improve Li-ion battery diagnostics
and can help formulate battery aging models. However, it is computationally
intensive and cannot be easily repeated for multiple cycles, multiple operating
conditions, or multiple replicate cells. To reduce the computational cost of
Bayesian calibration, numerical solvers for physics-based models can be
replaced with faster surrogates. A physics-informed neural network (PINN) is
developed as a surrogate for the pseudo-2D (P2D) battery model calibration. For
the P2D surrogate, additional training regularization was needed as compared to
the PINN single-particle model (SPM) developed in Part I. Both the PINN SPM and
P2D surrogate models are exercised for parameter inference and compared to data
obtained from a direct numerical solution of the governing equations. A
parameter inference study highlights the ability to use these PINNs to
calibrate scaling parameters for the cathode Li diffusion and the anode
exchange current density. By realizing computational speed-ups of 2250x for the
P2D model, as compared to using standard integrating methods, the PINN
surrogates enable rapid state-of-health diagnostics. In the low-data
availability scenario, the testing error was estimated to 2mV for the SPM
surrogate and 10mV for the P2D surrogate which could be mitigated with
additional data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Chat<span class="highlight-title">GPT</span> Needs SPADE (Sustainability, PrivAcy, Digital divide, and
  Ethics) Evaluation: A <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.03123v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.03123v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sunder Ali Khowaja, Parus Khuwaja, Kapal Dev, Weizheng Wang, Lewis Nkenyereye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  ChatGPT is another large language model (LLM) vastly available for the
consumers on their devices but due to its performance and ability to converse
effectively, it has gained a huge popularity amongst research as well as
industrial community. Recently, many studies have been published to show the
effectiveness, efficiency, integration, and sentiments of chatGPT and other
LLMs. In contrast, this study focuses on the important aspects that are mostly
overlooked, i.e. sustainability, privacy, digital divide, and ethics and
suggests that not only chatGPT but every subsequent entry in the category of
conversational bots should undergo Sustainability, PrivAcy, Digital divide, and
Ethics (SPADE) evaluation. This paper discusses in detail the issues and
concerns raised over chatGPT in line with aforementioned characteristics. We
also discuss the recent EU AI Act briefly in accordance with the SPADE
evaluation. We support our hypothesis by some preliminary data collection and
visualizations along with hypothesized facts. We also suggest mitigations and
recommendations for each of the concerns. Furthermore, we also suggest some
policies and recommendations for AI policy act, if designed by the governments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 8 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PINN surrogate of Li-ion battery models for parameter inference. Part I:
  Implementation and multi-fidelity hierarchies for the single-particle model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.17329v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.17329v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Malik Hassanaly, Peter J. Weddle, Ryan N. King, Subhayan De, Alireza Doostan, Corey R. Randall, Eric J. Dufek, Andrew M. Colclasure, Kandler Smith
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To plan and optimize energy storage demands that account for Li-ion battery
aging dynamics, techniques need to be developed to diagnose battery internal
states accurately and rapidly. This study seeks to reduce the computational
resources needed to determine a battery's internal states by replacing
physics-based Li-ion battery models -- such as the single-particle model (SPM)
and the pseudo-2D (P2D) model -- with a physics-informed neural network (PINN)
surrogate. The surrogate model makes high-throughput techniques, such as
Bayesian calibration, tractable to determine battery internal parameters from
voltage responses. This manuscript is the first of a two-part series that
introduces PINN surrogates of Li-ion battery models for parameter inference
(i.e., state-of-health diagnostics). In this first part, a method is presented
for constructing a PINN surrogate of the SPM. A multi-fidelity hierarchical
training, where several neural nets are trained with multiple physics-loss
fidelities is shown to significantly improve the surrogate accuracy when only
training on the governing equation residuals. The implementation is made
available in a companion repository (https://github.com/NREL/pinnstripes). The
techniques used to develop a PINN surrogate of the SPM are extended in Part II
for the PINN surrogate for the P2D battery model, and explore the Bayesian
calibration capabilities of both surrogates.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Disentangling the Spectral Properties of the Hodge Laplacian: Not All
  Small Eigenvalues Are Equal 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.14427v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.14427v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vincent P. Grande, Michael T. Schaub
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rich spectral information of the graph Laplacian has been instrumental in
graph theory, machine learning, and graph signal processing for applications
such as graph classification, clustering, or eigenmode analysis. Recently, the
Hodge Laplacian has come into focus as a generalisation of the ordinary
Laplacian for higher-order graph models such as simplicial and cellular
complexes. Akin to the traditional analysis of graph Laplacians, many authors
analyse the smallest eigenvalues of the Hodge Laplacian, which are connected to
important topological properties such as homology. However, small eigenvalues
of the Hodge Laplacian can carry different information depending on whether
they are related to curl or gradient eigenmodes, and thus may not be
comparable. We therefore introduce the notion of persistent eigenvector
similarity and provide a method to track individual harmonic, curl, and
gradient eigenvectors/-values through the so-called persistence filtration,
leveraging the full information contained in the Hodge-Laplacian spectrum
across all possible scales of a point cloud. Finally, we use our insights (a)
to introduce a novel form of Hodge spectral clustering and (b) to classify
edges and higher-order simplices based on their relationship to the smallest
harmonic, curl, and gradient eigenvectors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 4 figures, comments welcome</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Semi-Supervised Crowd Counting from Unlabeled Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2108.13969v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2108.13969v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoran Duan, Fan Wan, Rui Sun, Zeyu Wang, Varun Ojha, Yu Guan, Hubert P. H. Shum, Bingzhang Hu, Yang Long
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic Crowd behavior analysis can be applied to effectively help the
daily transportation statistics and planning, which helps the smart city
construction. As one of the most important keys, crowd counting has drawn
increasing attention. Recent works achieved promising performance but relied on
the supervised paradigm with expensive crowd annotations. To alleviate the
annotation cost in real-world transportation scenarios, in this work we
proposed a semi-supervised learning framework $S^{4}\textit{Crowd}$, which can
leverage both unlabeled/labeled data for robust crowd counting. In the
unsupervised pathway, two \textit{self-supervised losses} were proposed to
simulate the crowd variations such as scale, illumination, based on which
supervised information pseudo labels were generated and gradually refined. We
also proposed a crowd-driven recurrent unit \textit{Gated-Crowd-Recurrent-Unit
(GCRU)}, which can preserve discriminant crowd information by extracting
second-order statistics, yielding pseudo labels with improved quality. A joint
loss including both unsupervised/supervised information was proposed, and a
dynamic weighting strategy was employed to balance the importance of the
unsupervised loss and supervised loss at different training stages. We
conducted extensive experiments on four popular crowd counting datasets in
semi-supervised settings. Experimental results supported the effectiveness of
each proposed component in our $S^{4}$Crowd framework. Our method achieved
competitive performance in semi-supervised learning approaches on these crowd
counting datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient <span class="highlight-title">Pre-train</span>ing for Localized Instruction Generation of Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.15964v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.15964v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anil Batra, Davide Moltisanti, Laura Sevilla-Lara, Marcus Rohrbach, Frank Keller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Procedural videos show step-by-step demonstrations of tasks like recipe
preparation. Understanding such videos is challenging, involving the precise
localization of steps and the generation of textual instructions. Manually
annotating steps and writing instructions is costly, which limits the size of
current datasets and hinders effective learning. Leveraging large but noisy
video-transcript datasets for pre-training can boost performance, but demands
significant computational resources. Furthermore, transcripts contain
irrelevant content and exhibit style variation compared to instructions written
by human annotators. To mitigate both issues, we propose a technique,
Sieve-&-Swap, to automatically curate a smaller dataset: (i) Sieve filters
irrelevant transcripts and (ii) Swap enhances the quality of the text
instruction by automatically replacing the transcripts with human-written
instructions from a text-only recipe dataset. The curated dataset, three orders
of magnitude smaller than current web-scale datasets, enables efficient
training of large-scale models with competitive performance. We complement our
Sieve-\&-Swap approach with a Procedure Transformer (ProcX) for end-to-end step
localization and instruction generation for procedural videos. When this model
is pre-trained on our curated dataset, it achieves state-of-the-art performance
in zero-shot and finetuning settings on YouCook2 and Tasty, while using a
fraction of the computational resources.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This version has some missing experiments and elaborative technical
  details</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Toward a Theory of Causation for Interpreting Neural Code Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.03788v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.03788v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David N. Palacio, Alejandro Velasco, Nathan Cooper, Alvaro Rodriguez, Kevin Moran, Denys Poshyvanyk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Language Models of Code, or Neural Code Models (NCMs), are rapidly
progressing from research prototypes to commercial developer tools. As such,
understanding the capabilities and limitations of such models is becoming
critical. However, the abilities of these models are typically measured using
automated metrics that often only reveal a portion of their real-world
performance. While, in general, the performance of NCMs appears promising,
currently much is unknown about how such models arrive at decisions. To this
end, this paper introduces $do_{code}$, a post hoc interpretability method
specific to NCMs that is capable of explaining model predictions. $do_{code}$
is based upon causal inference to enable programming language-oriented
explanations. While the theoretical underpinnings of $do_{code}$ are extensible
to exploring different model properties, we provide a concrete instantiation
that aims to mitigate the impact of spurious correlations by grounding
explanations of model behavior in properties of programming languages. To
demonstrate the practical benefit of $do_{code}$, we illustrate the insights
that our framework can provide by performing a case study on two popular deep
learning architectures and ten NCMs. The results of this case study illustrate
that our studied NCMs are sensitive to changes in code syntax. All our NCMs,
except for the BERT-like model, statistically learn to predict tokens related
to blocks of code (\eg brackets, parenthesis, semicolon) with less confounding
bias as compared to other programming language constructs. These insights
demonstrate the potential of $do_{code}$ as a useful method to detect and
facilitate the elimination of confounding bias in NCMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to appear in IEEE Transactions on Software Engineering</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RL$^3$: Boosting Meta Reinforcement Learning via RL inside RL$^2$ 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.15909v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.15909v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhinav Bhatia, Samer B. Nashed, Shlomo Zilberstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Meta reinforcement learning (meta-RL) methods such as RL$^2$ have emerged as
promising approaches for learning data-efficient RL algorithms tailored to a
given task distribution. However, they show poor asymptotic performance and
struggle with out-of-distribution tasks because they rely on sequence models,
such as recurrent neural networks or transformers, to process experiences
rather than summarize them using general-purpose RL components such as value
functions. In contrast, traditional RL algorithms are data-inefficient as they
do not use domain knowledge, but they do converge to an optimal policy in the
limit. We propose RL$^3$, a principled hybrid approach that incorporates
action-values, learned per task through traditional RL, in the inputs to
meta-RL. We show that RL$^3$ earns greater cumulative reward in the long term,
compared to RL$^2$, while maintaining data-efficiency in the short term, and
generalizes better to out-of-distribution tasks. Experiments are conducted on
both custom and benchmark discrete domains from the meta-RL literature that
exhibit a range of short-term, long-term, and complex dependencies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-Objective Optimization for Sparse Deep Multi-Task Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.12243v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.12243v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        S. S. Hotegni, M. Berkemeier, S. Peitz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Different conflicting optimization criteria arise naturally in various Deep
Learning scenarios. These can address different main tasks (i.e., in the
setting of Multi-Task Learning), but also main and secondary tasks such as loss
minimization versus sparsity. The usual approach is a simple weighting of the
criteria, which formally only works in the convex setting. In this paper, we
present a Multi-Objective Optimization algorithm using a modified Weighted
Chebyshev scalarization for training Deep Neural Networks (DNNs) with respect
to several tasks. By employing this scalarization technique, the algorithm can
identify all optimal solutions of the original problem while reducing its
complexity to a sequence of single-objective problems. The simplified problems
are then solved using an Augmented Lagrangian method, enabling the use of
popular optimization techniques such as Adam and Stochastic Gradient Descent,
while efficaciously handling constraints. Our work aims to address the
(economical and also ecological) sustainability issue of DNN models, with a
particular focus on Deep Multi-Task models, which are typically designed with a
very large number of weights to perform equally well on multiple tasks. Through
experiments conducted on two Machine Learning datasets, we demonstrate the
possibility of adaptively sparsifying the model during training without
significantly impacting its performance, if we are willing to apply
task-specific adaptations to the network weights. Code is available at
https://github.com/salomonhotegni/MDMTN
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Accelerating Scientific Discovery with Generative Knowledge Extraction,
  Graph-Based Representation, and Multimodal Intelligent Graph Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11996v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11996v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Markus J. Buehler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Leveraging generative Artificial Intelligence (AI), we have transformed a
dataset comprising 1,000 scientific papers into an ontological knowledge graph.
Through an in-depth structural analysis, we have calculated node degrees,
identified communities and connectivities, and evaluated clustering
coefficients and betweenness centrality of pivotal nodes, uncovering
fascinating knowledge architectures. The graph has an inherently scale-free
nature, is highly connected, and can be used for graph reasoning by taking
advantage of transitive and isomorphic properties that reveal unprecedented
interdisciplinary relationships that can be used to answer queries, identify
gaps in knowledge, propose never-before-seen material designs, and predict
material behaviors. We compute deep node embeddings for combinatorial node
similarity ranking for use in a path sampling strategy links dissimilar
concepts that have previously not been related. One comparison revealed
structural parallels between biological materials and Beethoven's 9th Symphony,
highlighting shared patterns of complexity through isomorphic mapping. In
another example, the algorithm proposed a hierarchical mycelium-based composite
based on integrating path sampling with principles extracted from Kandinsky's
'Composition VII' painting. The resulting material integrates an innovative set
of concepts that include a balance of chaos/order, adjustable porosity,
mechanical strength, and complex patterned chemical functionalization. We
uncover other isomorphisms across science, technology and art, revealing a
nuanced ontology of immanence that reveal a context-dependent heterarchical
interplay of constituents. Graph-based generative AI achieves a far higher
degree of novelty, explorative capacity, and technical detail, than
conventional approaches and establishes a widely useful framework for
innovation by revealing hidden connections.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unveiling the Pitfalls of Knowledge Editing for Large Language Models <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02129v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02129v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhoubo Li, Ningyu Zhang, Yunzhi Yao, Mengru Wang, Xi Chen, Huajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the cost associated with fine-tuning Large Language Models (LLMs)
continues to rise, recent research efforts have pivoted towards developing
methodologies to edit implicit knowledge embedded within LLMs. Yet, there's
still a dark cloud lingering overhead -- will knowledge editing trigger
butterfly effect? since it is still unclear whether knowledge editing might
introduce side effects that pose potential risks or not. This paper pioneers
the investigation into the potential pitfalls associated with knowledge editing
for LLMs. To achieve this, we introduce new benchmark datasets and propose
innovative evaluation metrics. Our results underline two pivotal concerns: (1)
Knowledge Conflict: Editing groups of facts that logically clash can magnify
the inherent inconsistencies in LLMs-a facet neglected by previous methods. (2)
Knowledge Distortion: Altering parameters with the aim of editing factual
knowledge can irrevocably warp the innate knowledge structure of LLMs.
Experimental results vividly demonstrate that knowledge editing might
inadvertently cast a shadow of unintended consequences on LLMs, which warrant
attention and efforts for future works. Code and data are available at
https://github.com/zjunlp/PitfallsKnowledgeEditing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Artificial Neural Nets and the Representation of Human Concepts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.05337v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.05337v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Timo Freiesleben
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  What do artificial neural networks (ANNs) learn? The machine learning (ML)
community shares the narrative that ANNs must develop abstract human concepts
to perform complex tasks. Some go even further and believe that these concepts
are stored in individual units of the network. Based on current research, I
systematically investigate the assumptions underlying this narrative. I
conclude that ANNs are indeed capable of performing complex prediction tasks,
and that they may learn human and non-human concepts to do so. However,
evidence indicates that ANNs do not represent these concepts in individual
units.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>For: Philosophy of Science for Machine Learning: Core Issues and New
  Perspectives, edited by Juan Duran and Giorgia Pozzi</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dual Conic Proxies for AC Optimal Power Flow <span class="chip">SC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02969v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02969v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guancheng Qiu, Mathieu Tanneau, Pascal Van Hentenryck
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, there has been significant interest in the development of
machine learning-based optimization proxies for AC Optimal Power Flow (AC-OPF).
Although significant progress has been achieved in predicting high-quality
primal solutions, no existing learning-based approach can provide valid dual
bounds for AC-OPF. This paper addresses this gap by training optimization
proxies for a convex relaxation of AC-OPF. Namely, the paper considers a
second-order cone (SOC) relaxation of AC-OPF, and proposes \revision{a novel
architecture} that embeds a fast, differentiable (dual) feasibility recovery,
thus providing valid dual bounds. The paper combines this new architecture with
a self-supervised learning scheme, which alleviates the need for costly
training data generation. Extensive numerical experiments on medium- and
large-scale power grids demonstrate the efficiency and scalability of the
proposed methodology.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted to PSCC 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Harmonic Control Lyapunov Barrier Functions for Constrained Optimal
  Control with Reach-Avoid Specifications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02869v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02869v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amartya Mukherjee, Ruikun Zhou, Haocheng Chang, Jun Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces harmonic control Lyapunov barrier functions (harmonic
CLBF) that aid in constrained control problems such as reach-avoid problems.
Harmonic CLBFs exploit the maximum principle that harmonic functions satisfy to
encode the properties of control Lyapunov barrier functions (CLBFs). As a
result, they can be initiated at the start of an experiment rather than trained
based on sample trajectories. The control inputs are selected to maximize the
inner product of the system dynamics with the steepest descent direction of the
harmonic CLBF. Numerical results are presented with four different systems
under different reach-avoid environments. Harmonic CLBFs show a significantly
low risk of entering unsafe regions and a high probability of entering the goal
region.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Investigating Feature and Model Importance in Android Malware Detection:
  An Implemented <span class="highlight-title">Survey</span> and Experimental Comparison of ML-Based Methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.12778v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.12778v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Muzaffar, Hani Ragab Hassen, Hind Zantout, Michael A Lones
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The popularity of Android means it is a common target for malware. Over the
years, various studies have found that machine learning models can effectively
discriminate malware from benign applications. However, as the operating system
evolves, so does malware, bringing into question the findings of these previous
studies, many of which report very high accuracies using small, outdated, and
often imbalanced datasets. In this paper, we reimplement 18 representative past
works and reevaluate them using a balanced, relevant, and up-to-date dataset
comprising 124,000 applications. We also carry out new experiments designed to
fill holes in existing knowledge, and use our findings to identify the most
effective features and models to use for Android malware detection within a
contemporary environment. We show that high detection accuracies (up to 96.8%)
can be achieved using features extracted through static analysis alone,
yielding a modest benefit (1%) from using far more expensive dynamic analysis.
API calls and opcodes are the most productive static and TCP network traffic
provide the most predictive dynamic features. Random forests are generally the
most effective model, outperforming more complex deep learning approaches.
Whilst directly combining static and dynamic features is generally ineffective,
ensembling models separately leads to performances comparable to the best
models but using less brittle features.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ In Search of a Data Transformation That Accelerates Neural Field
  Training <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.17094v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.17094v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junwon Seo, Sangyoon Lee, Kwang In Kim, Jaeho Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural field is an emerging paradigm in data representation that trains a
neural network to approximate the given signal. A key obstacle that prevents
its widespread adoption is the encoding speed-generating neural fields requires
an overfitting of a neural network, which can take a significant number of SGD
steps to reach the desired fidelity level. In this paper, we delve into the
impacts of data transformations on the speed of neural field training,
specifically focusing on how permuting pixel locations affect the convergence
speed of SGD. Counterintuitively, we find that randomly permuting the pixel
locations can considerably accelerate the training. To explain this phenomenon,
we examine the neural field training through the lens of PSNR curves, loss
landscapes, and error patterns. Our analyses suggest that the random pixel
permutations remove the easy-to-fit patterns, which facilitate easy
optimization in the early stage but hinder capturing fine details of the
signal.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Coarse-Tuning for Ad-hoc Document Retrieval Using <span class="highlight-title">Pre-train</span>ed Language
  Models <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16915v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16915v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Atsushi Keyaki, Ribeka Keyaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning in information retrieval systems using pre-trained language
models (PLM-based IR) requires learning query representations and
query-document relations, in addition to downstream task-specific learning.
This study introduces coarse-tuning as an intermediate learning stage that
bridges pre-training and fine-tuning. By learning query representations and
query-document relations in coarse-tuning, we aim to reduce the load of
fine-tuning and improve the learning effect of downstream IR tasks. We propose
Query-Document Pair Prediction (QDPP) for coarse-tuning, which predicts the
appropriateness of query-document pairs. Evaluation experiments show that the
proposed method significantly improves MRR and/or nDCG@5 in four ad-hoc
document retrieval datasets. Furthermore, the results of the query prediction
task suggested that coarse-tuning facilitated learning of query representation
and query-document relations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Domain Randomization via Entropy Maximization <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.01885v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.01885v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gabriele Tiboni, Pascal Klink, Jan Peters, Tatiana Tommasi, Carlo D'Eramo, Georgia Chalvatzaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Varying dynamics parameters in simulation is a popular Domain Randomization
(DR) approach for overcoming the reality gap in Reinforcement Learning (RL).
Nevertheless, DR heavily hinges on the choice of the sampling distribution of
the dynamics parameters, since high variability is crucial to regularize the
agent's behavior but notoriously leads to overly conservative policies when
randomizing excessively. In this paper, we propose a novel approach to address
sim-to-real transfer, which automatically shapes dynamics distributions during
training in simulation without requiring real-world data. We introduce DOmain
RAndomization via Entropy MaximizatiON (DORAEMON), a constrained optimization
problem that directly maximizes the entropy of the training distribution while
retaining generalization capabilities. In achieving this, DORAEMON gradually
increases the diversity of sampled dynamics parameters as long as the
probability of success of the current policy is sufficiently high. We
empirically validate the consistent benefits of DORAEMON in obtaining highly
adaptive and generalizable policies, i.e. solving the task at hand across the
widest range of dynamics parameters, as opposed to representative baselines
from the DR literature. Notably, we also demonstrate the Sim2Real applicability
of DORAEMON through its successful zero-shot transfer in a robotic manipulation
setup under unknown real-world parameters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a conference paper at ICLR 2024. Project website at
  https://gabrieletiboni.github.io/doraemon/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Byzantine-resilient Federated Learning With Adaptivity to Data
  Heterogeneity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13374v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13374v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiyuan Zuo, Xingrun Yan, Rongfei Fan, Han Hu, Hangguan Shan, Tony Q. S. Quek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper deals with federated learning (FL) in the presence of malicious
Byzantine attacks and data heterogeneity. A novel Robust Average Gradient
Algorithm (RAGA) is proposed, which leverages the geometric median for
aggregation and can freely select the round number for local updating.
Different from most existing resilient approaches, which perform convergence
analysis based on strongly-convex loss function or homogeneously distributed
dataset, we conduct convergence analysis for not only strongly-convex but also
non-convex loss function over heterogeneous dataset. According to our
theoretical analysis, as long as the fraction of dataset from malicious users
is less than half, RAGA can achieve convergence at rate
$\mathcal{O}({1}/{T^{2/3- \delta}})$ where $T$ is the iteration number and
$\delta \in (0, 2/3)$ for non-convex loss function, and at linear rate for
strongly-convex loss function. Moreover, stationary point or global optimal
solution is proved to obtainable as data heterogeneity vanishes. Experimental
results corroborate the robustness of RAGA to Byzantine attacks and verifies
the advantage of RAGA over baselines on convergence performance under various
intensity of Byzantine attacks, for heterogeneous dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Discretized Distributed Optimization over Dynamic Digraphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.07939v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.07939v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammadreza Doostmohammadian, Wei Jiang, Muwahida Liaquat, Alireza Aghasi, Houman Zarrabi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider a discrete-time model of continuous-time distributed optimization
over dynamic directed-graphs (digraphs) with applications to distributed
learning. Our optimization algorithm works over general strongly connected
dynamic networks under switching topologies, e.g., in mobile multi-agent
systems and volatile networks due to link failures. Compared to many existing
lines of work, there is no need for bi-stochastic weight designs on the links.
The existing literature mostly needs the link weights to be stochastic using
specific weight-design algorithms needed both at the initialization and at all
times when the topology of the network changes. This paper eliminates the need
for such algorithms and paves the way for distributed optimization over
time-varying digraphs. We derive the bound on the gradient-tracking step-size
and discrete time-step for convergence and prove dynamic stability using
arguments from consensus algorithms, matrix perturbation theory, and Lyapunov
theory. This work, particularly, is an improvement over existing
stochastic-weight undirected networks in case of link removal or packet drops.
This is because the existing literature may need to rerun time-consuming and
computationally complex algorithms for stochastic design, while the proposed
strategy works as long as the underlying network is weight-symmetric and
balanced. The proposed optimization framework finds applications to distributed
classification and learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ COPR: Continual Learning Human Preference through Optimal Policy
  Regularization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.15694v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.15694v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Zhang, Lin Gui, Yuanzhao Zhai, Hui Wang, Yu Lei, Ruifeng Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The technique of Reinforcement Learning from Human Feedback (RLHF) is a
commonly employed method to improve pre-trained Language Models (LM), enhancing
their ability to conform to human preferences. Nevertheless, the current
RLHF-based LMs necessitate full retraining each time novel queries or feedback
are introduced, which becomes a challenging task because human preferences can
vary between different domains or tasks. Retraining LMs poses practical
difficulties in many real-world situations due to the significant time and
computational resources required, along with concerns related to data privacy.
To address this limitation, we propose a new method called Continual Optimal
Policy Regularization (COPR), in which we compute the distribution of optimal
policy bypassing the partition function and then regularize the current policy
based on the historically optimal distribution to mitigate Catastrophic
Forgetting (CF). COPR involves a single learning phase and doesn't necessitate
complex reinforcement learning. Importantly, it shares the capability with RLHF
to learn from unlabeled data by maintaining a scoring module, similar to reward
model, making it flexible for continually learning without human feedback. Our
experimental results show that COPR outperforms strong Continuous Learning (CL)
baselines when it comes to consistently aligning with human preferences on
incremental tasks and domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SD4Match: Learning to <span class="highlight-title">Prompt</span> Stable Diffusion Model for Semantic
  Matching <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.17569v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.17569v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinghui Li, Jingyi Lu, Kai Han, Victor Prisacariu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we address the challenge of matching semantically similar
keypoints across image pairs. Existing research indicates that the intermediate
output of the UNet within the Stable Diffusion (SD) can serve as robust image
feature maps for such a matching task. We demonstrate that by employing a basic
prompt tuning technique, the inherent potential of Stable Diffusion can be
harnessed, resulting in a significant enhancement in accuracy over previous
approaches. We further introduce a novel conditional prompting module that
conditions the prompt on the local details of the input image pairs, leading to
a further improvement in performance. We designate our approach as SD4Match,
short for Stable Diffusion for Semantic Matching. Comprehensive evaluations of
SD4Match on the PF-Pascal, PF-Willow, and SPair-71k datasets show that it sets
new benchmarks in accuracy across all these datasets. Particularly, SD4Match
outperforms the previous state-of-the-art by a margin of 12 percentage points
on the challenging SPair-71k dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2024. Project website:
  https://sd4match.active.vision/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Stable Linear Subspace Identification: A Machine Learning Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.03197v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.03197v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Loris Di Natale, Muhammad Zakwan, Bratislav Svetozarevic, Philipp Heer, Giancarlo Ferrari-Trecate, Colin N. Jones
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine Learning (ML) and linear System Identification (SI) have been
historically developed independently. In this paper, we leverage
well-established ML tools - especially the automatic differentiation framework
- to introduce SIMBa, a family of discrete linear multi-step-ahead state-space
SI methods using backpropagation. SIMBa relies on a novel
Linear-Matrix-Inequality-based free parametrization of Schur matrices to ensure
the stability of the identified model.
  We show how SIMBa generally outperforms traditional linear state-space SI
methods, and sometimes significantly, although at the price of a higher
computational burden. This performance gap is particularly remarkable compared
to other SI methods with stability guarantees, where the gain is frequently
above 25% in our investigations, hinting at SIMBa's ability to simultaneously
achieve state-of-the-art fitting performance and enforce stability.
Interestingly, these observations hold for a wide variety of input-output
systems and on both simulated and real-world data, showcasing the flexibility
of the proposed approach. We postulate that this new SI paradigm presents a
great extension potential to identify structured nonlinear models from data,
and we hence open-source SIMBa on https://github.com/Cemempamoi/simba.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ECC 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DeepMachining: Online Prediction of Machining Errors of Lathe Machines 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16451v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16451v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang-Li Lu, Hwai-Jung Hsu, Che-Wei Chou, H. T. Kung, Chen-Hsin Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We describe DeepMachining, a deep learning-based AI system for online
prediction of machining errors of lathe machine operations. We have built and
evaluated DeepMachining based on manufacturing data from factories.
Specifically, we first pretrain a deep learning model for a given lathe
machine's operations to learn the salient features of machining states. Then,
we fine-tune the pretrained model to adapt to specific machining tasks. We
demonstrate that DeepMachining achieves high prediction accuracy for multiple
tasks that involve different workpieces and cutting tools. To the best of our
knowledge, this work is one of the first factory experiments using pre-trained
deep-learning models to predict machining errors of lathe machines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RetroBridge: Modeling Retrosynthesis with Markov Bridges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.16212v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.16212v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ilia Igashov, Arne Schneuing, Marwin Segler, Michael Bronstein, Bruno Correia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrosynthesis planning is a fundamental challenge in chemistry which aims at
designing reaction pathways from commercially available starting materials to a
target molecule. Each step in multi-step retrosynthesis planning requires
accurate prediction of possible precursor molecules given the target molecule
and confidence estimates to guide heuristic search algorithms. We model
single-step retrosynthesis planning as a distribution learning problem in a
discrete state space. First, we introduce the Markov Bridge Model, a generative
framework aimed to approximate the dependency between two intractable discrete
distributions accessible via a finite sample of coupled data points. Our
framework is based on the concept of a Markov bridge, a Markov process pinned
at its endpoints. Unlike diffusion-based methods, our Markov Bridge Model does
not need a tractable noise distribution as a sampling proxy and directly
operates on the input product molecules as samples from the intractable prior
distribution. We then address the retrosynthesis planning problem with our
novel framework and introduce RetroBridge, a template-free retrosynthesis
modeling approach that achieves state-of-the-art results on standard evaluation
benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ChIRAAG: Chat<span class="highlight-title">GPT</span> Informed Rapid and Automated Assertion Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.00093v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.00093v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bhabesh Mali, Karthik Maddala, Sweeya Reddy, Vatsal Gupta, Chandan Karfa, Ramesh Karri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  System Verilog Assertion (SVA) formulation- a critical yet complex task is a
prerequisite in the Formal Property Verification (FPV) process. Traditionally,
SVA formulation involves expert-driven interpretation of specifications, which
is timeconsuming and prone to human error. However, LLM-informed automatic
assertion generation is gaining interest. We designeda novel framework called
ChIRAAG, based on OpenAI GPT4, to generate SVA assertions from natural language
specifications. ChIRAAG constitutes the systematic breakdown of design
specifications into a standardized format, further generating assertions from
formatted specifications using LLM. Furthermore, we developed testbenches to
verify/validate the LLM-generated assertions. Automatic feedback of log files
from the simulation tool to the LLM ensures that the framework can generate
correc SVAs automatically. Only 33% of LLM-generated raw assertions had errors.
Our results on OpenTitan designs shows that LLMs can streamline and assist
engineers in the assertion generation process, reshaping verification
workflows.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 5 figures and 2 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Low-Energy Adaptive Personalization for Resource-Constrained
  Devices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15905v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15905v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yushan Huang, Josh Millar, Yuxuan Long, Yuchen Zhao, Hamed Hadaddi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The personalization of machine learning (ML) models to address data drift is
a significant challenge in the context of Internet of Things (IoT)
applications. Presently, most approaches focus on fine-tuning either the full
base model or its last few layers to adapt to new data, while often neglecting
energy costs. However, various types of data drift exist, and fine-tuning the
full base model or the last few layers may not result in optimal performance in
certain scenarios. We propose Target Block Fine-Tuning (TBFT), a low-energy
adaptive personalization framework designed for resource-constrained devices.
We categorize data drift and personalization into three types: input-level,
feature-level, and output-level. For each type, we fine-tune different blocks
of the model to achieve optimal performance with reduced energy costs.
Specifically, input-, feature-, and output-level correspond to fine-tuning the
front, middle, and rear blocks of the model. We evaluate TBFT on a ResNet
model, three datasets, three different training sizes, and a Raspberry Pi.
Compared with the $Block Avg$, where each block is fine-tuned individually and
their performance improvements are averaged, TBFT exhibits an improvement in
model accuracy by an average of 15.30% whilst saving 41.57% energy consumption
on average compared with full fine-tuning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepetd to The 4th Workshop on Machine Learning and Systems
  (EuroMLSys '24)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FedCSD: A Federated Learning Based Approach for Code-Smell Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.00038v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.00038v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sadi Alawadi, Khalid Alkharabsheh, Fahed Alkhabbas, Victor Kebande, Feras M. Awaysheh, Fabio Palomba, Mohammed Awad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a Federated Learning Code Smell Detection (FedCSD)
approach that allows organizations to collaboratively train federated ML models
while preserving their data privacy. These assertions have been supported by
three experiments that have significantly leveraged three manually validated
datasets aimed at detecting and examining different code smell scenarios. In
experiment 1, which was concerned with a centralized training experiment,
dataset two achieved the lowest accuracy (92.30%) with fewer smells, while
datasets one and three achieved the highest accuracy with a slight difference
(98.90% and 99.5%, respectively). This was followed by experiment 2, which was
concerned with cross-evaluation, where each ML model was trained using one
dataset, which was then evaluated over the other two datasets. Results from
this experiment show a significant drop in the model's accuracy (lowest
accuracy: 63.80\%) where fewer smells exist in the training dataset, which has
a noticeable reflection (technical debt) on the model's performance. Finally,
the last and third experiments evaluate our approach by splitting the dataset
into 10 companies. The ML model was trained on the company's site, then all
model-updated weights were transferred to the server. Ultimately, an accuracy
of 98.34% was achieved by the global model that has been trained using 10
companies for 100 training rounds. The results reveal a slight difference in
the global model's accuracy compared to the highest accuracy of the centralized
model, which can be ignored in favour of the global model's comprehensive
knowledge, lower training cost, preservation of data privacy, and avoidance of
the technical debt problem.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 7 figures, Journal paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Multimodal Approach to Device-Directed Speech Detection with Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14438v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14438v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dominik Wagner, Alexander Churchill, Siddharth Sigtia, Panayiotis Georgiou, Matt Mirsamadi, Aarshee Mishra, Erik Marchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Interactions with virtual assistants typically start with a predefined
trigger phrase followed by the user command. To make interactions with the
assistant more intuitive, we explore whether it is feasible to drop the
requirement that users must begin each command with a trigger phrase. We
explore this task in three ways: First, we train classifiers using only
acoustic information obtained from the audio waveform. Second, we take the
decoder outputs of an automatic speech recognition (ASR) system, such as 1-best
hypotheses, as input features to a large language model (LLM). Finally, we
explore a multimodal system that combines acoustic and lexical features, as
well as ASR decoder signals in an LLM. Using multimodal information yields
relative equal-error-rate improvements over text-only and audio-only models of
up to 39% and 61%. Increasing the size of the LLM and training with low-rank
adaption leads to further relative EER reductions of up to 18% on our dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2312.03632</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FedCau: A Proactive Stop Policy for Communication and Computation
  Efficient Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.07773v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.07773v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Afsaneh Mahmoudi, Hossein S. Ghadikolaei, José Mairton Barros Da Silva Júnior, Carlo Fischione
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates efficient distributed training of a Federated
Learning~(FL) model over a wireless network of wireless devices. The
communication iterations of the distributed training algorithm may be
substantially deteriorated or even blocked by the effects of the devices'
background traffic, packet losses, congestion, or latency. We abstract the
communication-computation impacts as an `iteration cost' and propose a
cost-aware causal FL algorithm~(FedCau) to tackle this problem. We propose an
iteration-termination method that trade-offs the training performance and
networking costs. We apply our approach when clients use the slotted-ALOHA, the
carrier-sense multiple access with collision avoidance~(CSMA/CA), and the
orthogonal frequency-division multiple access~(OFDMA) protocols. We show that,
given a total cost budget, the training performance degrades as either the
background communication traffic or the dimension of the training problem
increases. Our results demonstrate the importance of proactively designing
optimal cost-efficient stopping criteria to avoid unnecessary
communication-computation costs to achieve only a marginal FL training
improvement. We validate our method by training and testing FL over the MNIST
dataset. Finally, we apply our approach to existing communication efficient FL
methods from the literature, achieving further efficiency. We conclude that
cost-efficient stopping criteria are essential for the success of practical FL
over wireless networks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DreamComposer: Controllable 3D Object Generation via Multi-View
  Conditions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.03611v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.03611v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunhan Yang, Yukun Huang, Xiaoyang Wu, Yuan-Chen Guo, Song-Hai Zhang, Hengshuang Zhao, Tong He, Xihui Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Utilizing pre-trained 2D large-scale generative models, recent works are
capable of generating high-quality novel views from a single in-the-wild image.
However, due to the lack of information from multiple views, these works
encounter difficulties in generating controllable novel views. In this paper,
we present DreamComposer, a flexible and scalable framework that can enhance
existing view-aware diffusion models by injecting multi-view conditions.
Specifically, DreamComposer first uses a view-aware 3D lifting module to obtain
3D representations of an object from multiple views. Then, it renders the
latent features of the target view from 3D representations with the multi-view
feature fusion module. Finally the target view features extracted from
multi-view inputs are injected into a pre-trained diffusion model. Experiments
show that DreamComposer is compatible with state-of-the-art diffusion models
for zero-shot novel view synthesis, further enhancing them to generate
high-fidelity novel view images with multi-view conditions, ready for
controllable 3D object reconstruction and various other applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://yhyang-myron.github.io/DreamComposer/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Transport meets Variational Inference: Controlled Monte Carlo Diffusions <span class="chip">ICML</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.01050v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.01050v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francisco Vargas, Shreyas Padhy, Denis Blessing, Nikolas Nüsken
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Connecting optimal transport and variational inference, we present a
principled and systematic framework for sampling and generative modelling
centred around divergences on path space. Our work culminates in the
development of the \emph{Controlled Monte Carlo Diffusion} sampler (CMCD) for
Bayesian computation, a score-based annealing technique that crucially adapts
both forward and backward dynamics in a diffusion model. On the way, we clarify
the relationship between the EM-algorithm and iterative proportional fitting
(IPF) for Schr{\"o}dinger bridges, deriving as well a regularised objective
that bypasses the iterative bottleneck of standard IPF-updates. Finally, we
show that CMCD has a strong foundation in the Jarzinsky and Crooks identities
from statistical physics, and that it convincingly outperforms competing
approaches across a wide array of experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Workshop on New Frontiers in Learning, Control, and Dynamical Systems
  at the International Conference on Machine Learning (ICML), Honolulu, Hawaii,
  USA, 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ P2ANet: A <span class="highlight-title">Dataset</span> and Benchmark for Dense Action Detection from Table
  Tennis Match Broadcasting Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.12730v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.12730v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiang Bian, Xuhong Li, Tao Wang, Qingzhong Wang, Jun Huang, Chen Liu, Jun Zhao, Feixiang Lu, Dejing Dou, Haoyi Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While deep learning has been widely used for video analytics, such as video
classification and action detection, dense action detection with fast-moving
subjects from sports videos is still challenging. In this work, we release yet
another sports video benchmark \TheName{} for \emph{\underline{P}}ing
\emph{\underline{P}}ong-\emph{\underline{A}}ction detection, which consists of
2,721 video clips collected from the broadcasting videos of professional table
tennis matches in World Table Tennis Championships and Olympiads. We work with
a crew of table tennis professionals and referees on a specially designed
annotation toolbox to obtain fine-grained action labels (in 14 classes) for
every ping-pong action that appeared in the dataset, and formulate two sets of
action detection problems -- \emph{action localization} and \emph{action
recognition}. We evaluate a number of commonly-seen action recognition (e.g.,
TSM, TSN, Video SwinTransformer, and Slowfast) and action localization models
(e.g., BSN, BSN++, BMN, TCANet), using \TheName{} for both problems, under
various settings. These models can only achieve 48\% area under the AR-AN curve
for localization and 82\% top-one accuracy for recognition since the ping-pong
actions are dense with fast-moving subjects but broadcasting videos are with
only 25 FPS. The results confirm that \TheName{} is still a challenging task
and can be used as a special benchmark for dense action detection from videos.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Implicit GNN Solver for Poisson-like problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.10891v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.10891v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthieu Nastorg, Michele Alessandro Bucci, Thibault Faney, Jean-Marc Gratien, Guillaume Charpiat, Marc Schoenauer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents $\Psi$-GNN, a novel Graph Neural Network (GNN) approach
for solving the ubiquitous Poisson PDE problems with mixed boundary conditions.
By leveraging the Implicit Layer Theory, $\Psi$-GNN models an "infinitely" deep
network, thus avoiding the empirical tuning of the number of required Message
Passing layers to attain the solution. Its original architecture explicitly
takes into account the boundary conditions, a critical prerequisite for
physical applications, and is able to adapt to any initially provided solution.
$\Psi$-GNN is trained using a "physics-informed" loss, and the training process
is stable by design, and insensitive to its initialization. Furthermore, the
consistency of the approach is theoretically proven, and its flexibility and
generalization efficiency are experimentally demonstrated: the same learned
model can accurately handle unstructured meshes of various sizes, as well as
different boundary conditions. To the best of our knowledge, $\Psi$-GNN is the
first physics-informed GNN-based method that can handle various unstructured
domains, boundary conditions and initial solutions while also providing
convergence guarantees.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Ensemble learning for Physics Informed Neural Networks: a Gradient
  Boosting approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.13143v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.13143v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiwei Fang, Sifan Wang, Paris Perdikaris
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While the popularity of physics-informed neural networks (PINNs) is steadily
rising, to this date, PINNs have not been successful in simulating multi-scale
and singular perturbation problems. In this work, we present a new training
paradigm referred to as "gradient boosting" (GB), which significantly enhances
the performance of physics informed neural networks (PINNs). Rather than
learning the solution of a given PDE using a single neural network directly,
our algorithm employs a sequence of neural networks to achieve a superior
outcome. This approach allows us to solve problems presenting great challenges
for traditional PINNs. Our numerical experiments demonstrate the effectiveness
of our algorithm through various benchmarks, including comparisons with finite
element methods and PINNs. Furthermore, this work also unlocks the door to
employing ensemble learning techniques in PINNs, providing opportunities for
further improvement in solving PDEs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bayesian data-driven discovery of partial differential equations with
  variable coefficients 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2102.01432v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2102.01432v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aoxue Chen, Yifan Du, Liyao Mars Gao, Guang Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The discovery of Partial Differential Equations (PDEs) is an essential task
for applied science and engineering. However, data-driven discovery of PDEs is
generally challenging, primarily stemming from the sensitivity of the
discovered equation to noise and the complexities of model selection. In this
work, we propose an advanced Bayesian sparse learning algorithm for PDE
discovery with variable coefficients, predominantly when the coefficients are
spatially or temporally dependent. Specifically, we apply threshold Bayesian
group Lasso regression with a spike-and-slab prior (tBGL-SS) and leverage a
Gibbs sampler for Bayesian posterior estimation of PDE coefficients. This
approach not only enhances the robustness of point estimation with valid
uncertainty quantification but also relaxes the computational burden from
Bayesian inference through the integration of coefficient thresholds as an
approximate MCMC method. Moreover, from the quantified uncertainties, we
propose a Bayesian total error bar criteria for model selection, which
outperforms classic metrics including the root mean square and the Akaike
information criterion. The capability of this method is illustrated by the
discovery of several classical benchmark PDEs with spatially or temporally
varying coefficients from solution data obtained from the reference
simulations. In the experiments, we show that the tBGL-SS method is more robust
than the baseline methods under noisy environments and provides better model
selection criteria along the regularization path.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Graph Signal Diffusion Model for Collaborative Filtering <span class="chip">SIGIR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.08744v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.08744v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunqin Zhu, Chao Wang, Qi Zhang, Hui Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Collaborative filtering is a critical technique in recommender systems. Among
various methods, an increasingly popular paradigm is to reconstruct user-item
interactions based on the historical observations. This can be viewed as a
conditional generative task, where recently developed diffusion model
demonstrates great potential. However, existing studies on diffusion models
lack effective solutions for modeling implicit feedback data. Particularly, the
isotropic nature of the standard diffusion process fails to account for the
heterogeneous dependencies among items, leading to a misalignment with the
graphical structure of the interaction space. Meanwhile, random noise
destroying personalized information in interaction vectors, causing difficulty
in reverse reconstruction. In this paper, we make novel adaptions of diffusion
model and propose Graph Signal Diffusion Model for Collaborative Filtering
(named GiffCF). To better represent the high-dimensional and sparse
distribution of implicit feedback, we define a generalized form of denoising
diffusion using heat equation on the item-item similarity graph. Our forward
process smooths interaction signals with an advanced family of graph filters.
Hence, instead of losing information, it involves item-item similarities as
beneficial prior knowledge for recommendation. To reconstruct high-quality
interactions, our reverse process iteratively refines and sharpens preference
signals in a deterministic manner, where the update direction is conditioned on
the user history and computed from a carefully designed two-stage denoiser.
Finally, through extensive experiments, we show that GiffCF effectively
leverages the advantages of both diffusion model and graph signal processing,
and achieves state-of-the-art performance on three benchmark datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 8 figures, Accepted by SIGIR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Riemannian Laplace Approximation with the Fisher Metric <span class="chip">AISTATS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.02766v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.02766v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanlin Yu, Marcelo Hartmann, Bernardo Williams, Mark Girolami, Arto Klami
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Laplace's method approximates a target density with a Gaussian distribution
at its mode. It is computationally efficient and asymptotically exact for
Bayesian inference due to the Bernstein-von Mises theorem, but for complex
targets and finite-data posteriors it is often too crude an approximation. A
recent generalization of the Laplace Approximation transforms the Gaussian
approximation according to a chosen Riemannian geometry providing a richer
approximation family, while still retaining computational efficiency. However,
as shown here, its properties depend heavily on the chosen metric, indeed the
metric adopted in previous work results in approximations that are overly
narrow as well as being biased even at the limit of infinite data. We correct
this shortcoming by developing the approximation family further, deriving two
alternative variants that are exact at the limit of infinite data, extending
the theoretical analysis of the method, and demonstrating practical
improvements in a range of experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AISTATS 2024, with additional fixes</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DISL: Fueling Research with A Large <span class="highlight-title">Dataset</span> of Solidity Smart Contracts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16861v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16861v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gabriele Morello, Mojtaba Eshghie, Sofia Bobadilla, Martin Monperrus
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The DISL dataset features a collection of $514,506$ unique Solidity files
that have been deployed to Ethereum mainnet. It caters to the need for a large
and diverse dataset of real-world smart contracts. DISL serves as a resource
for developing machine learning systems and for benchmarking software
engineering tools designed for smart contracts. By aggregating every verified
smart contract from Etherscan up to January 15, 2024, DISL surpasses existing
datasets in size and recency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Masked Autoencoders Are Robust Neural Architecture Search Learners 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12086v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12086v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiming Hu, Xiangxiang Chu, Bo Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Architecture Search (NAS) currently relies heavily on labeled data,
which is both expensive and time-consuming to acquire. In this paper, we
propose a novel NAS framework based on Masked Autoencoders (MAE) that
eliminates the need for labeled data during the search process. By replacing
the supervised learning objective with an image reconstruction task, our
approach enables the robust discovery of network architectures without
compromising performance and generalization ability. Additionally, we address
the problem of performance collapse encountered in the widely-used
Differentiable Architecture Search (DARTS) method in the unsupervised paradigm
by introducing a multi-scale decoder. Through extensive experiments conducted
on various search spaces and datasets, we demonstrate the effectiveness and
robustness of the proposed method, providing empirical evidence of its
superiority over baseline approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Domain-Aware Fine-Tuning: Enhancing Neural Network Adaptability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.07728v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.07728v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seokhyeon Ha, Sunbeom Jung, Jungwoo Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning pre-trained neural network models has become a widely adopted
approach across various domains. However, it can lead to the distortion of
pre-trained feature extractors that already possess strong generalization
capabilities. Mitigating feature distortion during adaptation to new target
domains is crucial. Recent studies have shown promising results in handling
feature distortion by aligning the head layer on in-distribution datasets
before performing fine-tuning. Nonetheless, a significant limitation arises
from the treatment of batch normalization layers during fine-tuning, leading to
suboptimal performance. In this paper, we propose Domain-Aware Fine-Tuning
(DAFT), a novel approach that incorporates batch normalization conversion and
the integration of linear probing and fine-tuning. Our batch normalization
conversion method effectively mitigates feature distortion by reducing
modifications to the neural network during fine-tuning. Additionally, we
introduce the integration of linear probing and fine-tuning to optimize the
head layer with gradual adaptation of the feature extractor. By leveraging
batch normalization layers and integrating linear probing and fine-tuning, our
DAFT significantly mitigates feature distortion and achieves improved model
performance on both in-distribution and out-of-distribution datasets. Extensive
experiments demonstrate that our method outperforms other baseline methods,
demonstrating its effectiveness in not only improving performance but also
mitigating feature distortion.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Motion Planning Diffusion: Learning and Planning of Robot Motions with
  Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.01557v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.01557v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joao Carvalho, An T. Le, Mark Baierl, Dorothea Koert, Jan Peters
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning priors on trajectory distributions can help accelerate robot motion
planning optimization. Given previously successful plans, learning trajectory
generative models as priors for a new planning problem is highly desirable.
Prior works propose several ways on utilizing this prior to bootstrapping the
motion planning problem. Either sampling the prior for initializations or using
the prior distribution in a maximum-a-posterior formulation for trajectory
optimization. In this work, we propose learning diffusion models as priors. We
then can sample directly from the posterior trajectory distribution conditioned
on task goals, by leveraging the inverse denoising process of diffusion models.
Furthermore, diffusion has been recently shown to effectively encode data
multimodality in high-dimensional settings, which is particularly well-suited
for large trajectory dataset. To demonstrate our method efficacy, we compare
our proposed method - Motion Planning Diffusion - against several baselines in
simulated planar robot and 7-dof robot arm manipulator environments. To assess
the generalization capabilities of our method, we test it in environments with
previously unseen obstacles. Our experiments show that diffusion models are
strong priors to encode high-dimensional trajectory distributions of robot
motions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Lightweight and Gradient-Stable Neural Layer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2106.04088v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2106.04088v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yueyao Yu, Yin Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To enhance resource efficiency and model deployability of neural networks, we
propose a neural-layer architecture based on Householder weighting and
absolute-value activating, called Householder-absolute neural layer or simply
Han-layer. Compared to a fully connected layer with $d$-neurons and $d$
outputs, a Han-layer reduces the number of parameters and the corresponding
computational complexity from $O(d^2)$ to $O(d)$. {The Han-layer structure
guarantees that the Jacobian of the layer function is always orthogonal, thus
ensuring gradient stability (i.e., free of gradient vanishing or exploding
issues) for any Han-layer sub-networks.} Extensive numerical experiments show
that one can strategically use Han-layers to replace fully connected (FC)
layers, reducing the number of model parameters while maintaining or even
improving the generalization performance. We will also showcase the
capabilities of the Han-layer architecture on a few small stylized models, and
discuss its current limitations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Brain Networks and Intelligence: A Graph Neural Network Based Approach
  to Resting State fMRI Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.03520v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.03520v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bishal Thapaliya, Esra Akbas, Jiayu Chen, Raam Sapkota, Bhaskar Ray, Pranav Suresh, Vince Calhoun, Jingyu Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Resting-state functional magnetic resonance imaging (rsfMRI) is a powerful
tool for investigating the relationship between brain function and cognitive
processes as it allows for the functional organization of the brain to be
captured without relying on a specific task or stimuli. In this paper, we
present a novel modeling architecture called BrainRGIN for predicting
intelligence (fluid, crystallized, and total intelligence) using graph neural
networks on rsfMRI derived static functional network connectivity matrices.
Extending from the existing graph convolution networks, our approach
incorporates a clustering-based embedding and graph isomorphism network in the
graph convolutional layer to reflect the nature of the brain sub-network
organization and efficient network expression, in combination with TopK pooling
and attention-based readout functions. We evaluated our proposed architecture
on a large dataset, specifically the Adolescent Brain Cognitive Development
Dataset, and demonstrated its effectiveness in predicting individual
differences in intelligence. Our model achieved lower mean squared errors and
higher correlation scores than existing relevant graph architectures and other
traditional machine learning models for all of the intelligence prediction
tasks. The middle frontal gyrus exhibited a significant contribution to both
fluid and crystallized intelligence, suggesting their pivotal role in these
cognitive processes. Total composite scores identified a diverse set of brain
regions to be relevant which underscores the complex nature of total
intelligence.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ All Rivers Run to the Sea: Private Learning with Asymmetric Flows <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.05264v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.05264v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Niu, Ramy E. Ali, Saurav Prakash, Salman Avestimehr
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data privacy is of great concern in cloud machine-learning service platforms,
when sensitive data are exposed to service providers. While private computing
environments (e.g., secure enclaves), and cryptographic approaches (e.g.,
homomorphic encryption) provide strong privacy protection, their computing
performance still falls short compared to cloud GPUs. To achieve privacy
protection with high computing performance, we propose Delta, a new private
training and inference framework, with comparable model performance as
non-private centralized training. Delta features two asymmetric data flows: the
main information-sensitive flow and the residual flow. The main part flows into
a small model while the residuals are offloaded to a large model. Specifically,
Delta embeds the information-sensitive representations into a low-dimensional
space while pushing the information-insensitive part into high-dimension
residuals. To ensure privacy protection, the low-dimensional
information-sensitive part is secured and fed to a small model in a private
environment. On the other hand, the residual part is sent to fast cloud GPUs,
and processed by a large model. To further enhance privacy and reduce the
communication cost, Delta applies a random binary quantization technique along
with a DP-based technique to the residuals before sharing them with the public
platform. We theoretically show that Delta guarantees differential privacy in
the public environment and greatly reduces the complexity in the private
environment. We conduct empirical analyses on CIFAR-10, CIFAR-100 and ImageNet
datasets and ResNet-18 and ResNet-34, showing that Delta achieves strong
privacy protection, fast training, and inference without significantly
compromising the model utility.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Camera-ready for CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NaNa and MiGu: Semantic Data Augmentation Techniques to Enhance Protein
  Classification in Graph Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14736v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14736v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi-Shan Lan, Pin-Yu Chen, Tsung-Yi Ho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Protein classification tasks are essential in drug discovery. Real-world
protein structures are dynamic, which will determine the properties of
proteins. However, the existing machine learning methods, like ProNet (Wang et
al., 2022a), only access limited conformational characteristics and protein
side-chain features, leading to impractical protein structure and inaccuracy of
protein classes in their predictions. In this paper, we propose novel semantic
data augmentation methods, Novel Augmentation of New Node Attributes (NaNa),
and Molecular Interactions and Geometric Upgrading (MiGu) to incorporate
backbone chemical and side-chain biophysical information into protein
classification tasks and a co-embedding residual learning framework.
Specifically, we leverage molecular biophysical, secondary structure, chemical
bonds, and ionic features of proteins to facilitate protein classification
tasks. Furthermore, our semantic augmentation methods and the co-embedding
residual learning framework can improve the performance of GIN (Xu et al.,
2019) on EC and Fold datasets (Bairoch, 2000; Andreeva et al., 2007) by 16.41%
and 11.33% respectively. Our code is available at
https://github.com/r08b46009/Code_for_MIGU_NANA/tree/main.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Graph Generation with $K^2$-trees <span class="chip">ICLR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.19125v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.19125v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunhui Jang, Dongwoo Kim, Sungsoo Ahn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating graphs from a target distribution is a significant challenge
across many domains, including drug discovery and social network analysis. In
this work, we introduce a novel graph generation method leveraging $K^2$-tree
representation, originally designed for lossless graph compression. The
$K^2$-tree representation {encompasses inherent hierarchy while enabling
compact graph generation}. In addition, we make contributions by (1) presenting
a sequential $K^2$-treerepresentation that incorporates pruning, flattening,
and tokenization processes and (2) introducing a Transformer-based architecture
designed to generate the sequence by incorporating a specialized tree
positional encoding scheme. Finally, we extensively evaluate our algorithm on
four general and two molecular graph datasets to confirm its superiority for
graph generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>International Conference on Learning Representations (ICLR) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Simple and Scalable Representation for Graph Generation <span class="chip">ICLR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.02230v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.02230v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunhui Jang, Seul Lee, Sungsoo Ahn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, there has been a surge of interest in employing neural networks for
graph generation, a fundamental statistical learning problem with critical
applications like molecule design and community analysis. However, most
approaches encounter significant limitations when generating large-scale
graphs. This is due to their requirement to output the full adjacency matrices
whose size grows quadratically with the number of nodes. In response to this
challenge, we introduce a new, simple, and scalable graph representation named
gap encoded edge list (GEEL) that has a small representation size that aligns
with the number of edges. In addition, GEEL significantly reduces the
vocabulary size by incorporating the gap encoding and bandwidth restriction
schemes. GEEL can be autoregressively generated with the incorporation of node
positional encoding, and we further extend GEEL to deal with attributed graphs
by designing a new grammar. Our findings reveal that the adoption of this
compact representation not only enhances scalability but also bolsters
performance by simplifying the graph generation process. We conduct a
comprehensive evaluation across ten non-attributed and two molecular graph
generation tasks, demonstrating the effectiveness of GEEL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>International Conference on Learning Representations (ICLR) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Identification of Craving Maps among Marijuana Users via the Analysis of
  Functional Brain Networks with High-Order Attention Graph Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.00033v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.00033v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun-En Ding, Shihao Yang, Anna Zilverstand, Feng Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The excessive consumption of marijuana can induce substantial psychological
and social consequences. In this investigation, we propose an elucidative
framework termed high-order graph attention neural networks (HOGANN) for the
classification of Marijuana addiction, coupled with an analysis of localized
brain network communities exhibiting abnormal activities among chronic
marijuana users. HOGANN integrates dynamic intrinsic functional brain networks,
estimated from resting-state functional magnetic resonance imaging (rs-fMRI),
using long short-term memory (LSTM) to capture temporal network dynamics. We
employ a high-order attention module for information fusion and message passing
among neighboring nodes, enhancing the network community analysis. Our model is
validated across two distinct data cohorts, yielding substantially higher
classification accuracy than benchmark algorithms. Furthermore, we discern the
most pertinent subnetworks and cognitive regions affected by persistent
marijuana consumption, indicating adverse effects on functional brain networks,
particularly within the dorsal attention and frontoparietal networks.
Intriguingly, our model demonstrates superior performance in cohorts exhibiting
prolonged dependence, implying that prolonged marijuana usage induces more
pronounced alterations in brain networks. The model proficiently identifies
craving brain maps, thereby delineating critical brain regions for analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Troika: Multi-Path Cross-Modal Traction for Compositional Zero-Shot
  Learning <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.15230v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.15230v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siteng Huang, Biao Gong, Yutong Feng, Min Zhang, Yiliang Lv, Donglin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent compositional zero-shot learning (CZSL) methods adapt pre-trained
vision-language models (VLMs) by constructing trainable prompts only for
composed state-object pairs. Relying on learning the joint representation of
seen compositions, these methods ignore the explicit modeling of the state and
object, thus limiting the exploitation of pre-trained knowledge and
generalization to unseen compositions. With a particular focus on the
universality of the solution, in this work, we propose a novel paradigm for
CZSL models that establishes three identification branches (i.e., Multi-Path)
to jointly model the state, object, and composition. The presented Troika is
our implementation that aligns the branch-specific prompt representations with
decomposed visual features. To calibrate the bias between semantically similar
multi-modal representations, we further devise a Cross-Modal Traction module
into Troika that shifts the prompt representation towards the current visual
content. We conduct extensive experiments on three popular benchmarks, where
our method significantly outperforms existing methods in both closed-world and
open-world settings. The code will be available at
https://github.com/bighuang624/Troika.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Prediction Error Estimation in Random Forests 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.00736v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.00736v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ian Krupkin, Johanna Hardin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, error estimates of classification Random Forests are
quantitatively assessed. Based on the initial theoretical framework built by
Bates et al. (2023), the true error rate and expected error rate are
theoretically and empirically investigated in the context of a variety of error
estimation methods common to Random Forests. We show that in the classification
case, Random Forests' estimates of prediction error is closer on average to the
true error rate instead of the average prediction error. This is opposite the
findings of Bates et al. (2023) which are given for logistic regression. We
further show that our result holds across different error estimation strategies
such as cross-validation, bagging, and data splitting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2104.00673 by other authors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Aligning with Human Judgement: The Role of Pairwise Preference in Large
  Language Model Evaluators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16950v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16950v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinhong Liu, Han Zhou, Zhijiang Guo, Ehsan Shareghi, Ivan Vulić, Anna Korhonen, Nigel Collier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated promising capabilities as
automatic evaluators in assessing the quality of generated natural language.
However, LLMs still exhibit biases in evaluation and often struggle to generate
coherent evaluations that align with human assessments. In this work, we first
conduct a systematic study of the misalignment between LLM evaluators and human
judgement, revealing that existing calibration methods aimed at mitigating
biases are insufficient for effectively aligning LLM evaluators. Inspired by
the use of preference data in RLHF, we formulate the evaluation as a ranking
problem and introduce Pairwise-preference Search (PairS), an uncertainty-guided
search method that employs LLMs to conduct pairwise comparisons and efficiently
ranks candidate texts. PairS achieves state-of-the-art performance on
representative evaluation tasks and demonstrates significant improvements over
direct scoring. Furthermore, we provide insights into the role of pairwise
preference in quantifying the transitivity of LLMs and demonstrate how PairS
benefits from calibration.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Zero-Shot Chain-of-Thought Reasoning in Large Language Models
  through Logic <span class="chip">COLING 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.13339v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.13339v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xufeng Zhao, Mengdi Li, Wenhao Lu, Cornelius Weber, Jae Hee Lee, Kun Chu, Stefan Wermter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in large language models have showcased their remarkable
generalizability across various domains. However, their reasoning abilities
still have significant room for improvement, especially when confronted with
scenarios requiring multi-step reasoning. Although large language models
possess extensive knowledge, their reasoning often fails to effectively utilize
this knowledge to establish a coherent thinking paradigm. These models
sometimes show hallucinations as their reasoning procedures are unconstrained
by logical principles. Aiming at improving the zero-shot chain-of-thought
reasoning ability of large language models, we propose LoT (Logical Thoughts),
a self-improvement prompting framework that leverages principles rooted in
symbolic logic, particularly Reductio ad Absurdum, to systematically verify and
rectify the reasoning processes step by step. Experimental evaluations
conducted on language tasks in diverse domains, including arithmetic,
commonsense, symbolic, causal inference, and social problems, demonstrate the
efficacy of enhanced reasoning by logic. The implementation code for LoT can be
accessed at: https://github.com/xf-zhao/LoT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in COLING 2024. Code see https://github.com/xf-zhao/LoT</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Flexible Body Collision Dynamics with Hierarchical Contact Mesh
  <span class="highlight-title">Transformer</span> <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.12467v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.12467v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Youn-Yeol Yu, Jeongwhan Choi, Woojin Cho, Kookjin Lee, Nayong Kim, Kiseok Chang, Chang-Seung Woo, Ilho Kim, Seok-Woo Lee, Joon-Young Yang, Sooyoung Yoon, Noseong Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, many mesh-based graph neural network (GNN) models have been
proposed for modeling complex high-dimensional physical systems. Remarkable
achievements have been made in significantly reducing the solving time compared
to traditional numerical solvers. These methods are typically designed to i)
reduce the computational cost in solving physical dynamics and/or ii) propose
techniques to enhance the solution accuracy in fluid and rigid body dynamics.
However, it remains under-explored whether they are effective in addressing the
challenges of flexible body dynamics, where instantaneous collisions occur
within a very short timeframe. In this paper, we present Hierarchical Contact
Mesh Transformer (HCMT), which uses hierarchical mesh structures and can learn
long-range dependencies (occurred by collisions) among spatially distant
positions of a body -- two close positions in a higher-level mesh correspond to
two distant positions in a lower-level mesh. HCMT enables long-range
interactions, and the hierarchical mesh structure quickly propagates collision
effects to faraway positions. To this end, it consists of a contact mesh
Transformer and a hierarchical mesh Transformer (CMT and HMT, respectively).
Lastly, we propose a flexible body dynamics dataset, consisting of trajectories
that reflect experimental settings frequently used in the display industry for
product designs. We also compare the performance of several baselines using
well-known benchmark datasets. Our results show that HCMT provides significant
performance improvements over existing methods. Our code is available at
https://github.com/yuyudeep/hcmt.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Boosting Diffusion Models with Moving Average Sampling in Frequency
  Domain <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17870v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17870v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yurui Qian, Qi Cai, Yingwei Pan, Yehao Li, Ting Yao, Qibin Sun, Tao Mei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have recently brought a powerful revolution in image
generation. Despite showing impressive generative capabilities, most of these
models rely on the current sample to denoise the next one, possibly resulting
in denoising instability. In this paper, we reinterpret the iterative denoising
process as model optimization and leverage a moving average mechanism to
ensemble all the prior samples. Instead of simply applying moving average to
the denoised samples at different timesteps, we first map the denoised samples
to data space and then perform moving average to avoid distribution shift
across timesteps. In view that diffusion models evolve the recovery from
low-frequency components to high-frequency details, we further decompose the
samples into different frequency components and execute moving average
separately on each component. We name the complete approach "Moving Average
Sampling in Frequency domain (MASF)". MASF could be seamlessly integrated into
mainstream pre-trained diffusion models and sampling schedules. Extensive
experiments on both unconditional and conditional diffusion models demonstrate
that our MASF leads to superior performances compared to the baselines, with
almost negligible additional complexity cost.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GTA-HDR: A Large-Scale Synthetic <span class="highlight-title">Dataset</span> for HDR Image Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17837v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17837v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hrishav Bakul Barua, Kalin Stefanov, KokSheik Wong, Abhinav Dhall, Ganesh Krishnasamy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High Dynamic Range (HDR) content (i.e., images and videos) has a broad range
of applications. However, capturing HDR content from real-world scenes is
expensive and time- consuming. Therefore, the challenging task of
reconstructing visually accurate HDR images from their Low Dynamic Range (LDR)
counterparts is gaining attention in the vision research community. A major
challenge in this research problem is the lack of datasets, which capture
diverse scene conditions (e.g., lighting, shadows, weather, locations,
landscapes, objects, humans, buildings) and various image features (e.g.,
color, contrast, saturation, hue, luminance, brightness, radiance). To address
this gap, in this paper, we introduce GTA-HDR, a large-scale synthetic dataset
of photo-realistic HDR images sampled from the GTA-V video game. We perform
thorough evaluation of the proposed dataset, which demonstrates significant
qualitative and quantitative improvements of the state-of-the-art HDR image
reconstruction methods. Furthermore, we demonstrate the effectiveness of the
proposed dataset and its impact on additional computer vision tasks including
3D human pose estimation, human body part segmentation, and holistic scene
segmentation. The dataset, data collection pipeline, and evaluation code are
available at: https://github.com/HrishavBakulBarua/GTA-HDR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FastPerson: Enhancing Video Learning through Effective Video
  Summarization that Preserves Linguistic and Visual Contexts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17727v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17727v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kazuki Kawamura, Jun Rekimoto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quickly understanding lengthy lecture videos is essential for learners with
limited time and interest in various topics to improve their learning
efficiency. To this end, video summarization has been actively researched to
enable users to view only important scenes from a video. However, these studies
focus on either the visual or audio information of a video and extract
important segments in the video. Therefore, there is a risk of missing
important information when both the teacher's speech and visual information on
the blackboard or slides are important, such as in a lecture video. To tackle
this issue, we propose FastPerson, a video summarization approach that
considers both the visual and auditory information in lecture videos.
FastPerson creates summary videos by utilizing audio transcriptions along with
on-screen images and text, minimizing the risk of overlooking crucial
information for learners. Further, it provides a feature that allows learners
to switch between the summary and original videos for each chapter of the
video, enabling them to adjust the pace of learning based on their interests
and level of understanding. We conducted an evaluation with 40 participants to
assess the effectiveness of our method and confirmed that it reduced viewing
time by 53\% at the same level of comprehension as that when using traditional
video playback methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Panonut360: A Head and Eye Tracking <span class="highlight-title">Dataset</span> for Panoramic Video <span class="chip">ACM MM</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17708v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17708v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yutong Xu, Junhao Du, Jiahe Wang, Yuwei Ning, Sihan Zhou Yang Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid development and widespread application of VR/AR technology,
maximizing the quality of immersive panoramic video services that match users'
personal preferences and habits has become a long-standing challenge.
Understanding the saliency region where users focus, based on data collected
with HMDs, can promote multimedia encoding, transmission, and quality
assessment. At the same time, large-scale datasets are essential for
researchers and developers to explore short/long-term user behavior patterns
and train AI models related to panoramic videos. However, existing panoramic
video datasets often include low-frequency user head or eye movement data
through short-term videos only, lacking sufficient data for analyzing users'
Field of View (FoV) and generating video saliency regions.
  Driven by these practical factors, in this paper, we present a head and eye
tracking dataset involving 50 users (25 males and 25 females) watching 15
panoramic videos. The dataset provides details on the viewport and gaze
attention locations of users. Besides, we present some statistics samples
extracted from the dataset. For example, the deviation between head and eye
movements challenges the widely held assumption that gaze attention decreases
from the center of the FoV following a Gaussian distribution. Our analysis
reveals a consistent downward offset in gaze fixations relative to the FoV in
experimental settings involving multiple users and videos. That's why we name
the dataset Panonut, a saliency weighting shaped like a donut. Finally, we also
provide a script that generates saliency distributions based on given head or
eye coordinates and pre-generated saliency distribution map sets of each video
from the collected eye tracking data.
  The dataset is available on website: https://dianvrlab.github.io/Panonut360/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages,ACM MMSys'24 accepted</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dual Memory Networks: A Versatile Adaptation Approach for
  Vision-Language Models <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17589v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17589v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yabin Zhang, Wenjie Zhu, Hui Tang, Zhiyuan Ma, Kaiyang Zhou, Lei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the emergence of pre-trained vision-language models like CLIP, how to
adapt them to various downstream classification tasks has garnered significant
attention in recent research. The adaptation strategies can be typically
categorized into three paradigms: zero-shot adaptation, few-shot adaptation,
and the recently-proposed training-free few-shot adaptation. Most existing
approaches are tailored for a specific setting and can only cater to one or two
of these paradigms. In this paper, we introduce a versatile adaptation approach
that can effectively work under all three settings. Specifically, we propose
the dual memory networks that comprise dynamic and static memory components.
The static memory caches training data knowledge, enabling training-free
few-shot adaptation, while the dynamic memory preserves historical test
features online during the testing process, allowing for the exploration of
additional data insights beyond the training set. This novel capability
enhances model performance in the few-shot setting and enables model usability
in the absence of training data. The two memory networks employ the same
flexible memory interactive strategy, which can operate in a training-free mode
and can be further enhanced by incorporating learnable projection layers. Our
approach is tested across 11 datasets under the three task settings.
Remarkably, in the zero-shot scenario, it outperforms existing methods by over
3\% and even shows superior results against methods utilizing external training
data. Additionally, our method exhibits robust performance against natural
distribution shifts. Codes are available at \url{https://github.com/YBZh/DMN}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR2024; Codes are available at \url{https://github.com/YBZh/DMN}</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning to Visually Localize Sound Sources from Mixtures without Prior
  Source Knowledge <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17420v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17420v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongjin Kim, Sung Jin Um, Sangmin Lee, Jung Uk Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The goal of the multi-sound source localization task is to localize sound
sources from the mixture individually. While recent multi-sound source
localization methods have shown improved performance, they face challenges due
to their reliance on prior information about the number of objects to be
separated. In this paper, to overcome this limitation, we present a novel
multi-sound source localization method that can perform localization without
prior knowledge of the number of sound sources. To achieve this goal, we
propose an iterative object identification (IOI) module, which can recognize
sound-making objects in an iterative manner. After finding the regions of
sound-making objects, we devise object similarity-aware clustering (OSC) loss
to guide the IOI module to effectively combine regions of the same object but
also distinguish between different objects and backgrounds. It enables our
method to perform accurate localization of sound-making objects without any
prior knowledge. Extensive experimental results on the MUSIC and VGGSound
benchmarks show the significant performance improvements of the proposed method
over the existing methods for both single and multi-source. Our code is
available at: https://github.com/VisualAIKHU/NoPrior_MultiSSL
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AV2AV: Direct Audio-Visual Speech to Audio-Visual Speech Translation
  with Unified Audio-Visual Speech Representation <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.02512v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.02512v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeongsoo Choi, Se Jin Park, Minsu Kim, Yong Man Ro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a novel direct Audio-Visual Speech to Audio-Visual Speech
Translation (AV2AV) framework, where the input and output of the system are
multimodal (i.e., audio and visual speech). With the proposed AV2AV, two key
advantages can be brought: 1) We can perform real-like conversations with
individuals worldwide in a virtual meeting by utilizing our own primary
languages. In contrast to Speech-to-Speech Translation (A2A), which solely
translates between audio modalities, the proposed AV2AV directly translates
between audio-visual speech. This capability enhances the dialogue experience
by presenting synchronized lip movements along with the translated speech. 2)
We can improve the robustness of the spoken language translation system. By
employing the complementary information of audio-visual speech, the system can
effectively translate spoken language even in the presence of acoustic noise,
showcasing robust performance. To mitigate the problem of the absence of a
parallel AV2AV translation dataset, we propose to train our spoken language
translation system with the audio-only dataset of A2A. This is done by learning
unified audio-visual speech representations through self-supervised learning in
advance to train the translation system. Moreover, we propose an AV-Renderer
that can generate raw audio and video in parallel. It is designed with
zero-shot speaker modeling, thus the speaker in source audio-visual speech can
be maintained at the target translated audio-visual speech. The effectiveness
of AV2AV is evaluated with extensive experiments in a many-to-many language
translation setting. Demo page is available on
https://choijeongsoo.github.io/av2av.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024. Code & Demo: https://choijeongsoo.github.io/av2av</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Novel Approach to Industrial Defect Generation through Blended Latent
  Diffusion Model with Online Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.19330v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.19330v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanxi Li, Zhengxun Zhang, Hao Chen, Lin Wu, Bo Li, Deyin Liu, Mingwen Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Effectively addressing the challenge of industrial Anomaly Detection (AD)
necessitates an ample supply of defective samples, a constraint often hindered
by their scarcity in industrial contexts. This paper introduces a novel
algorithm designed to augment defective samples, thereby enhancing AD
performance. The proposed method tailors the blended latent diffusion model for
defect sample generation, employing a diffusion model to generate defective
samples in the latent space. A feature editing process, controlled by a
``trimap" mask and text prompts, refines the generated samples. The image
generation inference process is structured into three stages: a free diffusion
stage, an editing diffusion stage, and an online decoder adaptation stage. This
sophisticated inference strategy yields high-quality synthetic defective
samples with diverse pattern variations, leading to significantly improved AD
accuracies based on the augmented training set. Specifically, on the widely
recognized MVTec AD dataset, the proposed method elevates the state-of-the-art
(SOTA) performance of AD with augmented data by 1.5%, 1.9%, and 3.1% for AD
metrics AP, IAP, and IAP90, respectively. The implementation code of this work
can be found at the GitHub repository
https://github.com/GrandpaXun242/AdaBLDM.git
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages,7 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-25T00:00:00Z">2024-03-25</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Hybrid Approach To Aspect Based Sentiment Analysis Using Transfer
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17254v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17254v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gaurav Negi, Rajdeep Sarkar, Omnia Zayed, Paul Buitelaar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aspect-Based Sentiment Analysis (ABSA) aims to identify terms or multiword
expressions (MWEs) on which sentiments are expressed and the sentiment
polarities associated with them. The development of supervised models has been
at the forefront of research in this area. However, training these models
requires the availability of manually annotated datasets which is both
expensive and time-consuming. Furthermore, the available annotated datasets are
tailored to a specific domain, language, and text type. In this work, we
address this notable challenge in current state-of-the-art ABSA research. We
propose a hybrid approach for Aspect Based Sentiment Analysis using transfer
learning. The approach focuses on generating weakly-supervised annotations by
exploiting the strengths of both large language models (LLM) and traditional
syntactic dependencies. We utilise syntactic dependency structures of sentences
to complement the annotations generated by LLMs, as they may overlook
domain-specific aspect terms. Extensive experimentation on multiple datasets is
performed to demonstrate the efficacy of our hybrid method for the tasks of
aspect term extraction and aspect sentiment classification.
  Keywords: Aspect Based Sentiment Analysis, Syntactic Parsing, large language
model (LLM)
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TwoStep: Multi-agent Task Planning using Classical Planners and Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17246v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17246v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ishika Singh, David Traum, Jesse Thomason
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Classical planning formulations like the Planning Domain Definition Language
(PDDL) admit action sequences guaranteed to achieve a goal state given an
initial state if any are possible. However, reasoning problems defined in PDDL
do not capture temporal aspects of action taking, for example that two agents
in the domain can execute an action simultaneously if postconditions of each do
not interfere with preconditions of the other. A human expert can decompose a
goal into largely independent constituent parts and assign each agent to one of
these subgoals to take advantage of simultaneous actions for faster execution
of plan steps, each using only single agent planning. By contrast, large
language models (LLMs) used for directly inferring plan steps do not guarantee
execution success, but do leverage commonsense reasoning to assemble action
sequences. We combine the strengths of classical planning and LLMs by
approximating human intuitions for two-agent planning goal decomposition. We
demonstrate that LLM-based goal decomposition leads to faster planning times
than solving multi-agent PDDL problems directly while simultaneously achieving
fewer plan execution steps than a single agent plan alone and preserving
execution success. Additionally, we find that LLM-based approximations of
subgoals can achieve similar multi-agent execution steps than those specified
by human experts. Website and resources at https://glamor-usc.github.io/twostep
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SPLICE: A Singleton-Enhanced PipeLIne for Coreference REsolution <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17245v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17245v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yilun Zhu, Siyao Peng, Sameer Pradhan, Amir Zeldes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Singleton mentions, i.e.~entities mentioned only once in a text, are
important to how humans understand discourse from a theoretical perspective.
However previous attempts to incorporate their detection in end-to-end neural
coreference resolution for English have been hampered by the lack of singleton
mention spans in the OntoNotes benchmark. This paper addresses this limitation
by combining predicted mentions from existing nested NER systems and features
derived from OntoNotes syntax trees. With this approach, we create a near
approximation of the OntoNotes dataset with all singleton mentions, achieving
~94% recall on a sample of gold singletons. We then propose a two-step neural
mention and coreference resolution system, named SPLICE, and compare its
performance to the end-to-end approach in two scenarios: the OntoNotes test set
and the out-of-domain (OOD) OntoGUM corpus. Results indicate that reconstructed
singleton training yields results comparable to end-to-end systems for
OntoNotes, while improving OOD stability (+1.1 avg. F1). We conduct error
analysis for mention detection and delve into its impact on coreference
clustering, revealing that precision improvements deliver more substantial
benefits than increases in recall for resolving coreference chains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Role of $n$-gram Smoothing in the Age of Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17240v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17240v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luca Malagutti, Andrius Buinovskij, Anej Svete, Clara Meister, Afra Amini, Ryan Cotterell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For nearly three decades, language models derived from the $n$-gram
assumption held the state of the art on the task. The key to their success lay
in the application of various smoothing techniques that served to combat
overfitting. However, when neural language models toppled $n$-gram models as
the best performers, $n$-gram smoothing techniques became less relevant.
Indeed, it would hardly be an understatement to suggest that the line of
inquiry into $n$-gram smoothing techniques became dormant. This paper re-opens
the role classical $n$-gram smoothing techniques may play in the age of neural
language models. First, we draw a formal equivalence between label smoothing, a
popular regularization technique for neural language models, and add-$\lambda$
smoothing. Second, we derive a generalized framework for converting \emph{any}
$n$-gram smoothing technique into a regularizer compatible with neural language
models. Our empirical results find that our novel regularizers are comparable
to and, indeed, sometimes outperform label smoothing on language modeling and
machine translation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Making Sentence Embeddings Robust to User-Generated Content <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17220v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17220v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lydia Nishimwe, Benoît Sagot, Rachel Bawden
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  NLP models have been known to perform poorly on user-generated content (UGC),
mainly because it presents a lot of lexical variations and deviates from the
standard texts on which most of these models were trained. In this work, we
focus on the robustness of LASER, a sentence embedding model, to UGC data. We
evaluate this robustness by LASER's ability to represent non-standard sentences
and their standard counterparts close to each other in the embedding space.
Inspired by previous works extending LASER to other languages and modalities,
we propose RoLASER, a robust English encoder trained using a teacher-student
approach to reduce the distances between the representations of standard and
UGC sentences. We show that with training only on standard and synthetic
UGC-like data, RoLASER significantly improves LASER's robustness to both
natural and artificial UGC data by achieving up to 2x and 11x better scores. We
also perform a fine-grained analysis on artificial UGC data and find that our
model greatly outperforms LASER on its most challenging UGC phenomena such as
keyboard typos and social media abbreviations. Evaluation on downstream tasks
shows that RoLASER performs comparably to or better than LASER on standard
data, while consistently outperforming it on UGC data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ontology Completion with Natural Language Inference and Concept
  Embeddings: An Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17216v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17216v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Na Li, Thomas Bailleux, Zied Bouraoui, Steven Schockaert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of finding plausible knowledge that is missing from a
given ontology, as a generalisation of the well-studied taxonomy expansion
task. One line of work treats this task as a Natural Language Inference (NLI)
problem, thus relying on the knowledge captured by language models to identify
the missing knowledge. Another line of work uses concept embeddings to identify
what different concepts have in common, taking inspiration from cognitive
models for category based induction. These two approaches are intuitively
complementary, but their effectiveness has not yet been compared. In this
paper, we introduce a benchmark for evaluating ontology completion methods and
thoroughly analyse the strengths and weaknesses of both approaches. We find
that both approaches are indeed complementary, with hybrid strategies achieving
the best overall results. We also find that the task is highly challenging for
Large Language Models, even after fine-tuning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Extracting Social Support and Social Isolation Information from Clinical
  Psychiatry Notes: Comparing a Rule-based NLP System and a Large Language
  Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17199v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17199v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Braja Gopal Patra, Lauren A. Lepow, Praneet Kasi Reddy Jagadeesh Kumar, Veer Vekaria, Mohit Manoj Sharma, Prakash Adekkanattu, Brian Fennessy, Gavin Hynes, Isotta Landi, Jorge A. Sanchez-Ruiz, Euijung Ryu, Joanna M. Biernacka, Girish N. Nadkarni, Ardesheer Talati, Myrna Weissman, Mark Olfson, J. John Mann, Alexander W. Charney, Jyotishman Pathak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Background: Social support (SS) and social isolation (SI) are social
determinants of health (SDOH) associated with psychiatric outcomes. In
electronic health records (EHRs), individual-level SS/SI is typically
documented as narrative clinical notes rather than structured coded data.
Natural language processing (NLP) algorithms can automate the otherwise
labor-intensive process of data extraction.
  Data and Methods: Psychiatric encounter notes from Mount Sinai Health System
(MSHS, n=300) and Weill Cornell Medicine (WCM, n=225) were annotated and
established a gold standard corpus. A rule-based system (RBS) involving
lexicons and a large language model (LLM) using FLAN-T5-XL were developed to
identify mentions of SS and SI and their subcategories (e.g., social network,
instrumental support, and loneliness).
  Results: For extracting SS/SI, the RBS obtained higher macro-averaged
f-scores than the LLM at both MSHS (0.89 vs. 0.65) and WCM (0.85 vs. 0.82). For
extracting subcategories, the RBS also outperformed the LLM at both MSHS (0.90
vs. 0.62) and WCM (0.82 vs. 0.81).
  Discussion and Conclusion: Unexpectedly, the RBS outperformed the LLMs across
all metrics. Intensive review demonstrates that this finding is due to the
divergent approach taken by the RBS and LLM. The RBS were designed and refined
to follow the same specific rules as the gold standard annotations. Conversely,
the LLM were more inclusive with categorization and conformed to common
English-language understanding. Both approaches offer advantages and are made
available open-source for future testing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">GPT</span>-4 Understands Discourse at Least as Well as Humans Do 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17196v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17196v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Shultz, Jamie Wise, Ardavan Salehi Nobandegani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We test whether a leading AI system GPT-4 understands discourse as well as
humans do, using a standardized test of discourse comprehension. Participants
are presented with brief stories and then answer eight yes/no questions probing
their comprehension of the story. The questions are formatted to assess the
separate impacts of directness (stated vs. implied) and salience (main idea vs.
details). GPT-4 performs slightly, but not statistically significantly, better
than humans given the very high level of human performance. Both GPT-4 and
humans exhibit a strong ability to make inferences about information that is
not explicitly stated in a story, a critical test of understanding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 1 figure, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NUMTEMP: A real-world benchmark to verify claims with statistical and
  temporal expressions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17169v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17169v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Venktesh V, Abhijit Anand, Avishek Anand, Vinay Setty
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automated fact checking has gained immense interest to tackle the growing
misinformation in the digital era. Existing systems primarily focus on
synthetic claims on Wikipedia, and noteworthy progress has also been made on
real-world claims. In this work, we release Numtemp, a diverse, multi-domain
dataset focused exclusively on numerical claims, encompassing temporal,
statistical and diverse aspects with fine-grained metadata and an evidence
collection without leakage. This addresses the challenge of verifying
real-world numerical claims, which are complex and often lack precise
information, not addressed by existing works that mainly focus on synthetic
claims. We evaluate and quantify the limitations of existing solutions for the
task of verifying numerical claims. We also evaluate claim decomposition based
methods, numerical understanding based models and our best baselines achieves a
macro-F1 of 58.32. This demonstrates that Numtemp serves as a challenging
evaluation set for numerical claim verification.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reflecting the Male Gaze: Quantifying Female Objectification in 19th and
  20th Century Novels <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17158v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17158v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kexin Luo, Yue Mao, Bei Zhang, Sophie Hao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inspired by the concept of the male gaze (Mulvey, 1975) in literature and
media studies, this paper proposes a framework for analyzing gender bias in
terms of female objectification: the extent to which a text portrays female
individuals as objects of visual pleasure. Our framework measures female
objectification along two axes. First, we compute an agency bias score that
indicates whether male entities are more likely to appear in the text as
grammatical agents than female entities. Next, by analyzing the word embedding
space induced by a text (Caliskan et al., 2017), we compute an appearance bias
score that indicates whether female entities are more closely associated with
appearance-related words than male entities. Applying our framework to 19th and
20th century novels reveals evidence of female objectification in literature:
we find that novels written from a male perspective systematically objectify
female characters, while novels written from a female perspective do not
exhibit statistically significant objectification of any gender.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Task-Agnostic Detector for Insertion-Based Backdoor Attacks <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17155v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17155v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weimin Lyu, Xiao Lin, Songzhu Zheng, Lu Pang, Haibin Ling, Susmit Jha, Chao Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Textual backdoor attacks pose significant security threats. Current detection
approaches, typically relying on intermediate feature representation or
reconstructing potential triggers, are task-specific and less effective beyond
sentence classification, struggling with tasks like question answering and
named entity recognition. We introduce TABDet (Task-Agnostic Backdoor
Detector), a pioneering task-agnostic method for backdoor detection. TABDet
leverages final layer logits combined with an efficient pooling technique,
enabling unified logit representation across three prominent NLP tasks. TABDet
can jointly learn from diverse task-specific models, demonstrating superior
detection efficacy over traditional task-specific methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Findings of NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Outcome-Constrained Large Language Models for Countering Hate Speech 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17146v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17146v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lingzi Hong, Pengcheng Luo, Eduardo Blanco, Xiaoying Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Counterspeech that challenges or responds to hate speech has been seen as an
alternative to mitigate the negative impact of hate speech and foster
productive online communications. Research endeavors have been directed to
using language models for the automatic generation of counterspeech to assist
efforts in combating online hate. Existing research focuses on the generation
of counterspeech with certain linguistic attributes, such as being polite,
informative, and intent-driven. However, it remains unclear what impact the
counterspeech might have in an online environment. We first explore methods
that utilize large language models (LLM) to generate counterspeech constrained
by potential conversation outcomes. We build two conversation outcome
classifiers that predict the incivility level and the hater reentry behavior
following replies to hate with Reddit data, then propose four methods to
incorporate the desired outcomes, i.e., low conversation incivility and
non-hateful hater reentry, into the text generation process, including Prompt
with Instructions, Prompt and Select, LLM finetune, and LLM transformer
reinforcement learning (TRL). Evaluation results show effective strategies to
generate outcome-constrained counterspeech and the linguistic characteristics
of texts generated by different methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Guided Distant Supervision for Multilingual Relation Extraction Data:
  Adapting to a New Language <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17143v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17143v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alistair Plum, Tharindu Ranasinghe, Christoph Purschke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Relation extraction is essential for extracting and understanding
biographical information in the context of digital humanities and related
subjects. There is a growing interest in the community to build datasets
capable of training machine learning models to extract relationships. However,
annotating such datasets can be expensive and time-consuming, in addition to
being limited to English. This paper applies guided distant supervision to
create a large biographical relationship extraction dataset for German. Our
dataset, composed of more than 80,000 instances for nine relationship types, is
the largest biographical German relationship extraction dataset. We also create
a manually annotated dataset with 2000 instances to evaluate the models and
release it together with the dataset compiled using guided distant supervision.
We train several state-of-the-art machine learning models on the automatically
created dataset and release them as well. Furthermore, we experiment with
multilingual and cross-lingual experiments that could benefit many low-resource
languages.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to LREC-COLING 2024 (The 2024 Joint International Conference
  on Computational Linguistics, Language Resources and Evaluation)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MetaAligner: Conditional Weak-to-Strong Correction for Generalizable
  Multi-Objective Alignment of Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17141v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17141v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kailai Yang, Zhiwei Liu, Qianqian Xie, Tianlin Zhang, Nirui Song, Jimin Huang, Ziyan Kuang, Sophia Ananiadou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in large language models (LLMs) aim to tackle
heterogeneous human expectations and values via multi-objective preference
alignment. However, existing methods are parameter-adherent to the policy
model, leading to two key limitations: (1) the high-cost repetition of their
alignment algorithms for each new target model; (2) they cannot expand to
unseen objectives due to their static alignment objectives. In this work, we
propose Meta-Objective Aligner (MetaAligner), a model that performs conditional
weak-to-strong correction for weak responses to approach strong responses.
MetaAligner is the first policy-agnostic and generalizable method for
multi-objective preference alignment, which enables plug-and-play alignment by
decoupling parameter updates from the policy models and facilitates zero-shot
preference alignment for unseen objectives via in-context learning.
Experimental results show that MetaAligner achieves significant and balanced
improvements in multi-objective alignments on 11 policy models with up to 63x
more parameters, and outperforms previous alignment methods with down to 22.27x
less computational resources. The model also accurately aligns with unseen
objectives, marking the first step towards generalizable multi-objective
preference alignment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress, more general experimental results to come</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring the Generalization of Cancer Clinical Trial Eligibility
  Classifiers Across Diseases 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17135v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17135v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yumeng Yang, Ashley Gilliam, Ethan B Ludmir, Kirk Roberts
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Clinical trials are pivotal in medical research, and NLP can enhance their
success, with application in recruitment. This study aims to evaluate the
generalizability of eligibility classification across a broad spectrum of
clinical trials. Starting with phase 3 cancer trials, annotated with seven
eligibility exclusions, then to determine how well models can generalize to
non-cancer and non-phase 3 trials. To assess this, we have compiled eligibility
criteria data for five types of trials: (1) additional phase 3 cancer trials,
(2) phase 1 and 2 cancer trials, (3) heart disease trials, (4) type 2 diabetes
trials, and (5) observational trials for any disease, comprising 2,490
annotated eligibility criteria across seven exclusion types. Our results show
that models trained on the extensive cancer dataset can effectively handle
criteria commonly found in non-cancer trials, such as autoimmune diseases.
However, they struggle with criteria disproportionately prevalent in cancer
trials, like prior malignancy. We also experiment with few-shot learning,
demonstrating that a limited number of disease-specific examples can partially
overcome this performance gap. We are releasing this new dataset of annotated
eligibility statements to promote the development of cross-disease
generalization in clinical trial classification.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Strong Pull of Prior Knowledge in Large Language Models and Its
  Impact on Emotion Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17125v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17125v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Georgios Chochlakis, Alexandros Potamianos, Kristina Lerman, Shrikanth Narayanan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In-context Learning (ICL) has emerged as a powerful paradigm for performing
natural language tasks with Large Language Models (LLM) without updating the
models' parameters, in contrast to the traditional gradient-based finetuning.
The promise of ICL is that the LLM can adapt to perform the present task at a
competitive or state-of-the-art level at a fraction of the cost. The ability of
LLMs to perform tasks in this few-shot manner relies on their background
knowledge of the task (or task priors). However, recent work has found that,
unlike traditional learning, LLMs are unable to fully integrate information
from demonstrations that contrast task priors. This can lead to performance
saturation at suboptimal levels, especially for subjective tasks such as
emotion recognition, where the mapping from text to emotions can differ widely
due to variability in human annotations. In this work, we design experiments
and propose measurements to explicitly quantify the consistency of proxies of
LLM priors and their pull on the posteriors. We show that LLMs have strong yet
inconsistent priors in emotion recognition that ossify their predictions. We
also find that the larger the model, the stronger these effects become. Our
results suggest that caution is needed when using ICL with larger LLMs for
affect-centered tasks outside their pre-training domain and when interpreting
ICL results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages, 27 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Grounding Language Plans in Demonstrations Through Counterfactual
  Perturbations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17124v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17124v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanwei Wang, Tsun-Hsuan Wang, Jiayuan Mao, Michael Hagenow, Julie Shah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Grounding the common-sense reasoning of Large Language Models in physical
domains remains a pivotal yet unsolved problem for embodied AI. Whereas prior
works have focused on leveraging LLMs directly for planning in symbolic spaces,
this work uses LLMs to guide the search of task structures and constraints
implicit in multi-step demonstrations. Specifically, we borrow from
manipulation planning literature the concept of mode families, which group
robot configurations by specific motion constraints, to serve as an abstraction
layer between the high-level language representations of an LLM and the
low-level physical trajectories of a robot. By replaying a few human
demonstrations with synthetic perturbations, we generate coverage over the
demonstrations' state space with additional successful executions as well as
counterfactuals that fail the task. Our explanation-based learning framework
trains an end-to-end differentiable neural network to predict successful
trajectories from failures and as a by-product learns classifiers that ground
low-level states and images in mode families without dense labeling. The
learned grounding classifiers can further be used to translate language plans
into reactive policies in the physical domain in an interpretable manner. We
show our approach improves the interpretability and reactivity of imitation
learning through 2D navigation and simulated and real robot manipulation tasks.
Website: https://sites.google.com/view/grounding-plans
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Attribute First, then Generate: Locally-attributable Grounded Text
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17104v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17104v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aviv Slobodkin, Eran Hirsch, Arie Cattan, Tal Schuster, Ido Dagan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent efforts to address hallucinations in Large Language Models (LLMs) have
focused on attributed text generation, which supplements generated texts with
citations of supporting sources for post-generation fact-checking and
corrections. Yet, these citations often point to entire documents or
paragraphs, burdening users with extensive verification work. In this paper, we
introduce a locally-attributable text generation approach, prioritizing concise
attributions. Our method, named ``Attribute First, then Generate'', breaks down
the conventional end-to-end generation process into three intuitive steps:
content selection, sentence planning, and sequential sentence generation. By
initially identifying relevant source segments (``select first'') and then
conditioning the generation process on them (``then generate''), we ensure
these segments also act as the output's fine-grained attributions (``select''
becomes ``attribute''). Tested on Multi-document Summarization and Long-form
Question-answering, our method not only yields more concise citations than the
baselines but also maintains - and in some cases enhances - both generation
quality and attribution accuracy. Furthermore, it significantly reduces the
time required for fact verification by human assessors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating Large Language Models as Generative User Simulators for
  Conversational Recommendation <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09738v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09738v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Se-eun Yoon, Zhankui He, Jessica Maria Echterhoff, Julian McAuley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Synthetic users are cost-effective proxies for real users in the evaluation
of conversational recommender systems. Large language models show promise in
simulating human-like behavior, raising the question of their ability to
represent a diverse population of users. We introduce a new protocol to measure
the degree to which language models can accurately emulate human behavior in
conversational recommendation. This protocol is comprised of five tasks, each
designed to evaluate a key property that a synthetic user should exhibit:
choosing which items to talk about, expressing binary preferences, expressing
open-ended preferences, requesting recommendations, and giving feedback.
Through evaluation of baseline simulators, we demonstrate these tasks
effectively reveal deviations of language models from human behavior, and offer
insights on how to reduce the deviations with model selection and prompting
strategies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OffLanDat: A Community Based Implicit Offensive Language <span class="highlight-title">Dataset</span>
  Generated by Large Language Model Through <span class="highlight-title">Prompt</span> Engineering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.02472v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.02472v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amit Das, Mostafa Rahgouy, Dongji Feng, Zheng Zhang, Tathagata Bhattacharya, Nilanjana Raychawdhary, Mary Sandage, Lauramarie Pope, Gerry Dozier, Cheryl Seals
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The widespread presence of offensive languages on social media has resulted
in adverse effects on societal well-being. As a result, it has become very
important to address this issue with high priority. Offensive languages exist
in both explicit and implicit forms, with the latter being more challenging to
detect. Current research in this domain encounters several challenges. Firstly,
the existing datasets primarily rely on the collection of texts containing
explicit offensive keywords, making it challenging to capture implicitly
offensive contents that are devoid of these keywords. Secondly, usual
methodologies tend to focus solely on textual analysis, neglecting the valuable
insights that community information can provide. In this research paper, we
introduce a novel dataset OffLanDat, a community based implicit offensive
language dataset generated by ChatGPT containing data for 38 different target
groups. Despite limitations in generating offensive texts using ChatGPT due to
ethical constraints, we present a prompt-based approach that effectively
generates implicit offensive languages. To ensure data quality, we evaluate our
data with human. Additionally, we employ a prompt-based Zero-Shot method with
ChatGPT and compare the detection results between human annotation and ChatGPT
annotation. We utilize existing state-of-the-art models to see how effective
they are in detecting such languages. We will make our code and dataset public
for other researchers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing End-to-End Multi-Task Dialogue Systems: A Study on Intrinsic
  Motivation Reinforcement Learning Algorithms for Improved Training and
  Adaptability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.18040v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.18040v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Navin Kamuni, Hardik Shah, Sathishkumar Chintala, Naveen Kunchakuri, Sujatha Alla Old Dominion
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  End-to-end multi-task dialogue systems are usually designed with separate
modules for the dialogue pipeline. Among these, the policy module is essential
for deciding what to do in response to user input. This policy is trained by
reinforcement learning algorithms by taking advantage of an environment in
which an agent receives feedback in the form of a reward signal. The current
dialogue systems, however, only provide meagre and simplistic rewards.
Investigating intrinsic motivation reinforcement learning algorithms is the
goal of this study. Through this, the agent can quickly accelerate training and
improve its capacity to judge the quality of its actions by teaching it an
internal incentive system. In particular, we adapt techniques for random
network distillation and curiosity-driven reinforcement learning to measure the
frequency of state visits and encourage exploration by using semantic
similarity between utterances. Experimental results on MultiWOZ, a
heterogeneous dataset, show that intrinsic motivation-based debate systems
outperform policies that depend on extrinsic incentives. By adopting random
network distillation, for example, which is trained using semantic similarity
between user-system dialogues, an astounding average success rate of 73% is
achieved. This is a significant improvement over the baseline Proximal Policy
Optimization (PPO), which has an average success rate of 60%. In addition,
performance indicators such as booking rates and completion rates show a 10%
rise over the baseline. Furthermore, these intrinsic incentive models help
improve the system's policy's resilience in an increasing amount of domains.
This implies that they could be useful in scaling up to settings that cover a
wider range of domains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 1 figure, 18th IEEE International Conference on Semantic
  Computing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MBR and QE Finetuning: Training-time Distillation of the Best and Most
  Expensive Decoding Methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.10966v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.10966v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mara Finkelstein, Subhajit Naskar, Mehdi Mirzazadeh, Apurva Shah, Markus Freitag
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent research in decoding methods for Natural Language Generation (NLG)
tasks has shown that MAP decoding is not optimal, because model probabilities
do not always align with human preferences. Stronger decoding methods,
including Quality Estimation (QE) reranking and Minimum Bayes' Risk (MBR)
decoding, have since been proposed to mitigate the model-perplexity-vs-quality
mismatch. While these decoding methods achieve state-of-the-art performance,
they are prohibitively expensive to compute. In this work, we propose MBR
finetuning and QE finetuning which distill the quality gains from these
decoding methods at training time, while using an efficient decoding algorithm
at inference time. Using the canonical NLG task of Neural Machine Translation
(NMT), we show that even with self-training, these finetuning methods
significantly outperform the base model. Moreover, when using an external LLM
as a teacher model, these finetuning methods outperform finetuning on
human-generated references. These findings suggest new ways to leverage
monolingual data to achieve improvements in model quality that are on par with,
or even exceed, improvements from human-curated data, while maintaining maximum
efficiency during decoding.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Language Models (Mostly) Do Not Consider Emotion Triggers When
  Predicting Emotion <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.09602v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.09602v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Smriti Singh, Cornelia Caragea, Junyi Jessy Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Situations and events evoke emotions in humans, but to what extent do they
inform the prediction of emotion detection models? This work investigates how
well human-annotated emotion triggers correlate with features that models
deemed salient in their prediction of emotions. First, we introduce a novel
dataset EmoTrigger, consisting of 900 social media posts sourced from three
different datasets; these were annotated by experts for emotion triggers with
high agreement. Using EmoTrigger, we evaluate the ability of large language
models (LLMs) to identify emotion triggers, and conduct a comparative analysis
of the features considered important for these tasks between LLMs and
fine-tuned models. Our analysis reveals that emotion triggers are largely not
considered salient features for emotion prediction models, instead there is
intricate interplay between various features and the task of emotion detection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2024 Camera Ready</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Transfers over Several Programming Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.16937v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.16937v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Razan Baltaji, Saurabh Pujar, Louis Mandel, Martin Hirzel, Luca Buratti, Lav Varshney
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have become remarkably good at improving
developer productivity for high-resource programming languages. These models
use two kinds of data: large amounts of unlabeled code samples for pre-training
and relatively smaller amounts of labeled code samples for fine-tuning or
in-context learning. Unfortunately, many programming languages are
low-resource, lacking labeled samples for most tasks and often even lacking
unlabeled samples. Therefore, users of low-resource languages (e.g., legacy or
new languages) miss out on the benefits of LLMs. Cross-lingual transfer uses
data from a source language to improve model performance on a target language.
It has been well-studied for natural languages, but has received little
attention for programming languages. This paper reports extensive experiments
on four tasks using a transformer-based LLM and 11 to 41 programming languages
to explore the following questions. First, how well does cross-lingual transfer
work for a given task across different language pairs. Second, given a task and
target language, how should one choose a source language. Third, which
characteristics of a language pair are predictive of transfer performance, and
how does that depend on the given task. Our empirical study with 1,808
experiments reveals practical and scientific insights, such as Kotlin and
JavaScript being the most transferable source languages and different tasks
relying on substantially different features. Overall, we find that learning
transfers well across several programming languages.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 9 figures, 8 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Understanding the Effects of Noise in Text-to-SQL: An Examination of the
  BIRD-Bench Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.12243v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.12243v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Niklas Wretblad, Fredrik Gordh Riseby, Rahul Biswas, Amin Ahmadi, Oskar Holmström
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-SQL, which involves translating natural language into Structured
Query Language (SQL), is crucial for enabling broad access to structured
databases without expert knowledge. However, designing models for such tasks is
challenging due to numerous factors, including the presence of 'noise,' such as
ambiguous questions and syntactical errors. This study provides an in-depth
analysis of the distribution and types of noise in the widely used BIRD-Bench
benchmark and the impact of noise on models. While BIRD-Bench was created to
model dirty and noisy database values, it was not created to contain noise and
errors in the questions and gold queries. We found that noise in questions and
gold queries are prevalent in the dataset, with varying amounts across domains,
and with an uneven distribution between noise types. The presence of incorrect
gold SQL queries, which then generate incorrect gold answers, has a significant
impact on the benchmark's reliability. Surprisingly, when evaluating models on
corrected SQL queries, zero-shot baselines surpassed the performance of
state-of-the-art prompting methods. We conclude that informative noise labels
and reliable benchmarks are crucial to developing new Text-to-SQL methods that
can handle varying types of noise. All datasets, annotations, and code are
available at
https://github.com/niklaswretblad/the-effects-of-noise-in-text-to-SQL.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Othering and low status framing of immigrant cuisines in US restaurant
  <span class="highlight-title">review</span>s and large language models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.07645v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.07645v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiwei Luo, Kristina Gligorić, Dan Jurafsky
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Identifying implicit attitudes toward food can mitigate social prejudice due
to food's salience as a marker of ethnic identity. Stereotypes about food are
representational harms that may contribute to racialized discourse and
negatively impact economic outcomes for restaurants. Understanding the presence
of representational harms in online corpora in particular is important, given
the increasing use of large language models (LLMs) for text generation and
their tendency to reproduce attitudes in their training data. Through careful
linguistic analyses, we evaluate social theories about attitudes toward
immigrant cuisine in a large-scale study of framing differences in 2.1M English
language Yelp reviews. Controlling for factors such as restaurant price and
neighborhood racial diversity, we find that immigrant cuisines are more likely
to be othered using socially constructed frames of authenticity (e.g.,
"authentic," "traditional"), and that non-European cuisines (e.g., Indian,
Mexican) in particular are described as more exotic compared to European ones
(e.g., French). We also find that non-European cuisines are more likely to be
described as cheap and dirty, even after controlling for price, and even among
the most expensive restaurants. Finally, we show that reviews generated by LLMs
reproduce similar framing tendencies, pointing to the downstream retention of
these representational harms. Our results corroborate social theories of
gastronomic stereotyping, revealing racialized evaluative processes and
linguistic strategies through which they manifest.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICWSM '24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fusing Domain-Specific Content from Large Language Models into Knowledge
  Graphs for Enhanced Zero Shot Object State Classification <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12151v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12151v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Filippos Gouidis, Katerina Papantoniou, Konstantinos Papoutsakis Theodore Patkos, Antonis Argyros, Dimitris Plexousakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Domain-specific knowledge can significantly contribute to addressing a wide
variety of vision tasks. However, the generation of such knowledge entails
considerable human labor and time costs. This study investigates the potential
of Large Language Models (LLMs) in generating and providing domain-specific
information through semantic embeddings. To achieve this, an LLM is integrated
into a pipeline that utilizes Knowledge Graphs and pre-trained semantic vectors
in the context of the Vision-based Zero-shot Object State Classification task.
We thoroughly examine the behavior of the LLM through an extensive ablation
study. Our findings reveal that the integration of LLM-based embeddings, in
combination with general-purpose pre-trained embeddings, leads to substantial
performance improvements. Drawing insights from this ablation study, we conduct
a comparative analysis against competing models, thereby highlighting the
state-of-the-art performance achieved by the proposed approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the AAAI-MAKE 24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Visual Grounding Helps Learn Word Meanings in Low-Data Regimes <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.13257v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.13257v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengxu Zhuang, Evelina Fedorenko, Jacob Andreas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern neural language models (LMs) are powerful tools for modeling human
sentence production and comprehension, and their internal representations are
remarkably well-aligned with representations of language in the human brain.
But to achieve these results, LMs must be trained in distinctly un-human-like
ways - requiring orders of magnitude more language data than children receive
during development, and without perceptual or social context. Do models trained
more naturalistically -- with grounded supervision -- exhibit more humanlike
language learning? We investigate this question in the context of word
learning, a key sub-task in language acquisition. We train a diverse set of LM
architectures, with and without auxiliary visual supervision, on datasets of
varying scales. We then evaluate these models' learning of syntactic
categories, lexical relations, semantic features, word similarity, and
alignment with human neural representations. We find that visual supervision
can indeed improve the efficiency of word learning. However, these improvements
are limited: they are present almost exclusively in the low-data regime, and
sometimes canceled out by the inclusion of rich distributional signals from
text. The information conveyed by text and images is not redundant -- models
mainly driven by visual information yield qualitatively different from those
mainly driven by word co-occurrences. However, our results suggest that current
multimodal modeling approaches fail to effectively leverage visual information
to build human-like word representations from human-scale data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generative <span class="highlight-title">Pre-train</span>ing for Speech with Flow Matching <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.16338v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.16338v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander H. Liu, Matt Le, Apoorv Vyas, Bowen Shi, Andros Tjandra, Wei-Ning Hsu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative models have gained more and more attention in recent years for
their remarkable success in tasks that required estimating and sampling data
distribution to generate high-fidelity synthetic data. In speech,
text-to-speech synthesis and neural vocoder are good examples where generative
models have shined. While generative models have been applied to different
applications in speech, there exists no general-purpose generative model that
models speech directly. In this work, we take a step toward this direction by
showing a single pre-trained generative model can be adapted to different
downstream tasks with strong performance. Specifically, we pre-trained a
generative model, named SpeechFlow, on 60k hours of untranscribed speech with
Flow Matching and masked conditions. Experiment results show the pre-trained
generative model can be fine-tuned with task-specific data to match or surpass
existing expert models on speech enhancement, separation, and synthesis. Our
work suggested a foundational model for generation tasks in speech can be built
with generative pre-training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EXPLORA: A teacher-apprentice methodology for eliciting natural
  child-computer interactions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17264v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17264v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vanessa Figueiredo, Catherine Ann Cameron
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Investigating child-computer interactions within their contexts is vital for
designing technology that caters to children's needs. However, determining what
aspects of context are relevant for designing child-centric technology remains
a challenge. We introduce EXPLORA, a multimodal, multistage online methodology
comprising three pivotal stages: (1) building a teacher-apprentice
relationship,(2) learning from child-teachers, and (3) assessing and
reinforcing researcher-apprentice learning. Central to EXPLORA is the
collection of attitudinal data through pre-observation interviews, offering
researchers a deeper understanding of children's characteristics and contexts.
This informs subsequent online observations, allowing researchers to focus on
frequent interactions. Furthermore, researchers can validate preliminary
assumptions with children. A means-ends analysis framework aids in the
systematic analysis of data, shedding light on context, agency and
homework-information searching processes children employ in their activities.
To illustrate EXPLORA's capabilities, we present nine single case studies
investigating Brazilian child-caregiver dyads' (children ages 9-11) use of
technology in homework information-searching.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CADGL: Context-Aware Deep Graph Learning for Predicting Drug-Drug
  Interactions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17210v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17210v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Azmine Toushik Wasi, Taki Hasan Rafi, Raima Islam, Serbetar Karlo, Dong-Kyu Chae
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Examining Drug-Drug Interactions (DDIs) is a pivotal element in the process
of drug development. DDIs occur when one drug's properties are affected by the
inclusion of other drugs. Detecting favorable DDIs has the potential to pave
the way for creating and advancing innovative medications applicable in
practical settings. However, existing DDI prediction models continue to face
challenges related to generalization in extreme cases, robust feature
extraction, and real-life application possibilities. We aim to address these
challenges by leveraging the effectiveness of context-aware deep graph learning
by introducing a novel framework named CADGL. Based on a customized variational
graph autoencoder (VGAE), we capture critical structural and physio-chemical
information using two context preprocessors for feature extraction from two
different perspectives: local neighborhood and molecular context, in a
heterogeneous graphical structure. Our customized VGAE consists of a graph
encoder, a latent information encoder, and an MLP decoder. CADGL surpasses
other state-of-the-art DDI prediction models, excelling in predicting
clinically valuable novel DDIs, supported by rigorous case studies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 Pages, 4 Figures; In review in IEEE/ACM Transactions on
  Computational Biology and Bioinformatics</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generation of Asset Administration Shell with Large Language Model
  Agents: Interoperability in Digital Twins with Semantic Node 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17209v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17209v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuchen Xia, Zhewen Xiao, Nasser Jazdi, Michael Weyrich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This research introduces a novel approach for assisting the creation of Asset
Administration Shell (AAS) instances for digital twin modeling within the
context of Industry 4.0, aiming to enhance interoperability in smart
manufacturing and reduce manual effort. We construct a "semantic node" data
structure to capture the semantic essence of textual data. Then, a system
powered by large language models is designed and implemented to process
"semantic node" and generate AAS instance models from textual technical data.
Our evaluation demonstrates a 62-79% effective generation rate, indicating a
substantial proportion of manual creation effort can be converted into easier
validation effort, thereby reducing the time and cost in creating AAS instance
models. In our evaluation, a comparative analysis of different LLMs and an
in-depth ablation study of Retrieval-Augmented Generation (RAG) mechanisms
provide insights into the effectiveness of LLM systems for interpreting
technical concepts. Our findings emphasize LLMs' capability in automating AAS
instance creation, enhancing semantic interoperability, and contributing to the
broader field of semantic interoperability for digital twins in industrial
applications. The prototype implementation and evaluation results are released
on our GitHub Repository with the link: https://github.com/YuchenXia/AASbyLLM
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Pre-print, submitted to IEEE ACCESS, under peer-review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GOLF: Goal-Oriented Long-term liFe tasks supported by human-AI
  collaboration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17089v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17089v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ben Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of ChatGPT and similar large language models (LLMs) has
revolutionized the human-AI interaction and information-seeking process.
Leveraging LLMs as an alternative to search engines, users can now access
summarized information tailored to their queries, significantly reducing the
cognitive load associated with navigating vast information resources. This
shift underscores the potential of LLMs in redefining information access
paradigms. Drawing on the foundation of task-focused information retrieval and
LLMs' task planning ability, this research extends the scope of LLM
capabilities beyond routine task automation to support users in navigating
long-term and significant life tasks. It introduces the GOLF framework
(Goal-Oriented Long-term liFe tasks), which focuses on enhancing LLMs' ability
to assist in significant life decisions through goal orientation and long-term
planning. The methodology encompasses a comprehensive simulation study to test
the framework's efficacy, followed by model and human evaluations to develop a
dataset benchmark for long-term life tasks, and experiments across different
models and settings. By shifting the focus from short-term tasks to the broader
spectrum of long-term life goals, this research underscores the transformative
potential of LLMs in enhancing human decision-making processes and task
management, marking a significant step forward in the evolution of human-AI
collaboration.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reinforcement Learning-based Recommender Systems with Large Language
  Models for State Reward and Action Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16948v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16948v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jie Wang, Alexandros Karatzoglou, Ioannis Arapakis, Joemon M. Jose
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning (RL)-based recommender systems have demonstrated
promising performance in meeting user expectations by learning to make accurate
next-item recommendations from historical user-item interactions. However,
existing offline RL-based sequential recommendation methods face the challenge
of obtaining effective user feedback from the environment. Effectively modeling
the user state and shaping an appropriate reward for recommendation remains a
challenge. In this paper, we leverage language understanding capabilities and
adapt large language models (LLMs) as an environment (LE) to enhance RL-based
recommenders. The LE is learned from a subset of user-item interaction data,
thus reducing the need for large training data, and can synthesise user
feedback for offline data by: (i) acting as a state model that produces high
quality states that enrich the user representation, and (ii) functioning as a
reward model to accurately capture nuanced user preferences on actions.
Moreover, the LE allows to generate positive actions that augment the limited
offline training data. We propose a LE Augmentation (LEA) method to further
improve recommendation performance by optimising jointly the supervised
component and the RL policy, using the augmented actions and historical user
signals. We use LEA, the state and reward models in conjunction with
state-of-the-art RL recommenders and report experimental results on two
publicly available datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GloSIS: The Global Soil Information System Web Ontology 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16778v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16778v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raul Palma, Bogusz Janiak, Luís Moreira de Sousa, Kathi Schleidt, Tomáš Řezník, Fenny van Egmond, Johan Leenaars, Dimitrios Moshou, Abdul Mouazen, Peter Wilson, David Medyckyj-Scott, Alistair Ritchie, Yusuf Yigini, Ronald Vargas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Established in 2012 by members of the Food and Agriculture Organisation
(FAO), the Global Soil Partnership (GSP) is a global network of stakeholders
promoting sound land and soil management practices towards a sustainable world
food system. However, soil survey largely remains a local or regional activity,
bound to heterogeneous methods and conventions. Recognising the relevance of
global and trans-national policies towards sustainable land management
practices, the GSP elected data harmonisation and exchange as one of its key
lines of action. Building upon international standards and previous work
towards a global soil data ontology, an improved domain model was eventually
developed within the GSP [54], the basis for a Global Soil Information System
(GloSIS). This work also identified the Semantic Web as a possible avenue to
operationalise the domain model. This article presents the GloSIS web ontology,
an implementation of the GloSIS domain model with the Web Ontology Language
(OWL). Thoroughly employing a host of Semantic Web standards (SOSA, SKOS,
GeoSPARQL, QUDT), GloSIS lays out not only a soil data ontology but also an
extensive set of ready-to-use code-lists for soil description and
physio-chemical analysis. Various examples are provided on the provision and
use of GloSIS-compliant linked data, showcasing the contribution of this
ontology to the discovery, exploration, integration and access of soil data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ProCQA: A Large-scale Community-based Programming Question Answering
  <span class="highlight-title">Dataset</span> for Code Search <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16702v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16702v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zehan Li, Jianfei Zhang, Chuantao Yin, Yuanxin Ouyang, Wenge Rong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-based code question answering seeks to match user queries in
natural language to relevant code snippets. Previous approaches typically rely
on pretraining models using crafted bi-modal and uni-modal datasets to align
text and code representations. In this paper, we introduce ProCQA, a
large-scale programming question answering dataset extracted from the
StackOverflow community, offering naturally structured mixed-modal QA pairs. To
validate its effectiveness, we propose a modality-agnostic contrastive
pre-training approach to improve the alignment of text and code representations
of current code language models. Compared to previous models that primarily
employ bimodal and unimodal pairs extracted from CodeSearchNet for
pre-training, our model exhibits significant performance improvements across a
wide range of code retrieval benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Graph Augmentation for Recommendation <span class="chip">ICDE 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16656v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16656v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qianru Zhang, Lianghao Xia, Xuheng Cai, Siuming Yiu, Chao Huang, Christian S. Jensen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph augmentation with contrastive learning has gained significant attention
in the field of recommendation systems due to its ability to learn expressive
user representations, even when labeled data is limited. However, directly
applying existing GCL models to real-world recommendation environments poses
challenges. There are two primary issues to address. Firstly, the lack of
consideration for data noise in contrastive learning can result in noisy
self-supervised signals, leading to degraded performance. Secondly, many
existing GCL approaches rely on graph neural network (GNN) architectures, which
can suffer from over-smoothing problems due to non-adaptive message passing. To
address these challenges, we propose a principled framework called GraphAug.
This framework introduces a robust data augmentor that generates denoised
self-supervised signals, enhancing recommender systems. The GraphAug framework
incorporates a graph information bottleneck (GIB)-regularized augmentation
paradigm, which automatically distills informative self-supervision information
and adaptively adjusts contrastive view generation. Through rigorous
experimentation on real-world datasets, we thoroughly assessed the performance
of our novel GraphAug model. The outcomes consistently unveil its superiority
over existing baseline methods. The source code for our model is publicly
available at: https://github.com/HKUDS/GraphAug.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages and accepted by ICDE 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A comparative analysis of embedding models for patent similarity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16630v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16630v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Grazia Sveva Ascione, Valerio Sterzi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper makes two contributions to the field of text-based patent
similarity. First, it compares the performance of different kinds of
patent-specific pretrained embedding models, namely static word embeddings
(such as word2vec and doc2vec models) and contextual word embeddings (such as
transformers based models), on the task of patent similarity calculation.
Second, it compares specifically the performance of Sentence Transformers
(SBERT) architectures with different training phases on the patent similarity
task. To assess the models' performance, we use information about patent
interferences, a phenomenon in which two or more patent claims belonging to
different patent applications are proven to be overlapping by patent examiners.
Therefore, we use these interferences cases as a proxy for maximum similarity
between two patents, treating them as ground-truth to evaluate the performance
of the different embedding models. Our results point out that, first, Patent
SBERT-adapt-ub, the domain adaptation of the pretrained Sentence Transformer
architecture proposed in this research, outperforms the current
state-of-the-art in patent similarity. Second, they show that, in some cases,
large static models performances are still comparable to contextual ones when
trained on extensive data; thus, we believe that the superiority in the
performance of contextual embeddings may not be related to the actual
architecture but rather to the way the training phase is performed.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LARA: Linguistic-Adaptive Retrieval-Augmented LLMs for Multi-Turn Intent
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16504v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16504v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liu Junhua, Tan Yong Keat, Fu Bin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Following the significant achievements of large language models (LLMs),
researchers have employed in-context learning for text classification tasks.
However, these studies focused on monolingual, single-turn classification
tasks. In this paper, we introduce LARA (Linguistic-Adaptive
Retrieval-Augmented Language Models), designed to enhance accuracy in
multi-turn classification tasks across six languages, accommodating numerous
intents in chatbot interactions. Multi-turn intent classification is notably
challenging due to the complexity and evolving nature of conversational
contexts. LARA tackles these issues by combining a fine-tuned smaller model
with a retrieval-augmented mechanism, integrated within the architecture of
LLMs. This integration allows LARA to dynamically utilize past dialogues and
relevant intents, thereby improving the understanding of the context.
Furthermore, our adaptive retrieval techniques bolster the cross-lingual
capabilities of LLMs without extensive retraining and fine-tune. Comprehensive
experiments demonstrate that LARA achieves state-of-the-art performance on
multi-turn intent classification tasks, enhancing the average accuracy by 3.67%
compared to existing methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ InstUPR : Instruction-based Unsupervised Passage Reranking with Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16435v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16435v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chao-Wei Huang, Yun-Nung Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces InstUPR, an unsupervised passage reranking method based
on large language models (LLMs). Different from existing approaches that rely
on extensive training with query-document pairs or retrieval-specific
instructions, our method leverages the instruction-following capabilities of
instruction-tuned LLMs for passage reranking without any additional
fine-tuning. To achieve this, we introduce a soft score aggregation technique
and employ pairwise reranking for unsupervised passage reranking. Experiments
on the BEIR benchmark demonstrate that InstUPR outperforms unsupervised
baselines as well as an instruction-tuned reranker, highlighting its
effectiveness and superiority. Source code to reproduce all experiments is
open-sourced at https://github.com/MiuLab/InstUPR
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint. This manuscript was originally written and submitted in
  June 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Experiment with the Use of Chat<span class="highlight-title">GPT</span> for LCSH Subject Assignment on
  Electronic Theses and Dissertations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16424v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16424v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eric H. C. Chow, TJ Kao, Xiaoli Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study delves into the potential use of Large Language Models (LLMs) for
generating Library of Congress Subject Headings (LCSH). The authors employed
ChatGPT to generate subject headings for electronic theses and dissertations
(ETDs) based on their titles and summaries. The results revealed that although
some generated subject headings were valid, there were issues regarding
specificity and exhaustiveness. The study showcases that LLMs can serve as a
strategic response to the backlog of items awaiting cataloging in academic
libraries, while also offering a cost-effective approach for promptly
generating LCSH. Nonetheless, human catalogers remain essential for verifying
and enhancing the validity, exhaustiveness, and specificity of LCSH generated
by LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Play to Your Strengths: Collaborative Intelligence of Conventional
  Recommender Models and Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16378v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16378v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunjia Xi, Weiwen Liu, Jianghao Lin, Chuhan Wu, Bo Chen, Ruiming Tang, Weinan Zhang, Yong Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rise of large language models (LLMs) has opened new opportunities in
Recommender Systems (RSs) by enhancing user behavior modeling and content
understanding. However, current approaches that integrate LLMs into RSs solely
utilize either LLM or conventional recommender model (CRM) to generate final
recommendations, without considering which data segments LLM or CRM excel in.
To fill in this gap, we conduct experiments on MovieLens-1M and Amazon-Books
datasets, and compare the performance of a representative CRM (DCNv2) and an
LLM (LLaMA2-7B) on various groups of data samples. Our findings reveal that
LLMs excel in data segments where CRMs exhibit lower confidence and precision,
while samples where CRM excels are relatively challenging for LLM, requiring
substantial training data and a long training time for comparable performance.
This suggests potential synergies in the combination between LLM and CRM.
Motivated by these insights, we propose Collaborative Recommendation with
conventional Recommender and Large Language Model (dubbed \textit{CoReLLa}). In
this framework, we first jointly train LLM and CRM and address the issue of
decision boundary shifts through alignment loss. Then, the resource-efficient
CRM, with a shorter inference time, handles simple and moderate samples, while
LLM processes the small subset of challenging samples for CRM. Our experimental
results demonstrate that CoReLLa outperforms state-of-the-art CRM and LLM
methods significantly, underscoring its effectiveness in recommendation tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uncovering Selective State Space Model's Capabilities in Lifelong
  Sequential Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16371v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16371v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiyuan Yang, Yuanzi Li, Jingyu Zhao, Hanbing Wang, Muyang Ma, Jun Ma, Zhaochun Ren, Mengqi Zhang, Xin Xin, Zhumin Chen, Pengjie Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequential Recommenders have been widely applied in various online services,
aiming to model users' dynamic interests from their sequential interactions.
With users increasingly engaging with online platforms, vast amounts of
lifelong user behavioral sequences have been generated. However, existing
sequential recommender models often struggle to handle such lifelong sequences.
The primary challenges stem from computational complexity and the ability to
capture long-range dependencies within the sequence. Recently, a state space
model featuring a selective mechanism (i.e., Mamba) has emerged. In this work,
we investigate the performance of Mamba for lifelong sequential recommendation
(i.e., length>=2k). More specifically, we leverage the Mamba block to model
lifelong user sequences selectively. We conduct extensive experiments to
evaluate the performance of representative sequential recommendation models in
the setting of lifelong sequences. Experiments on two real-world datasets
demonstrate the superiority of Mamba. We found that RecMamba achieves
performance comparable to the representative model while significantly reducing
training duration by approximately 70% and memory costs by 80%. Codes and data
are available at \url{https://github.com/nancheng58/RecMamba}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhanced Facet Generation with LLM Editing <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16345v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16345v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joosung Lee, Jinhong Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In information retrieval, facet identification of a user query is an
important task. If a search service can recognize the facets of a user's query,
it has the potential to offer users a much broader range of search results.
Previous studies can enhance facet prediction by leveraging retrieved documents
and related queries obtained through a search engine. However, there are
challenges in extending it to other applications when a search engine operates
as part of the model. First, search engines are constantly updated. Therefore,
additional information may change during training and test, which may reduce
performance. The second challenge is that public search engines cannot search
for internal documents. Therefore, a separate search system needs to be built
to incorporate documents from private domains within the company. We propose
two strategies that focus on a framework that can predict facets by taking only
queries as input without a search engine. The first strategy is multi-task
learning to predict SERP. By leveraging SERP as a target instead of a source,
the proposed model deeply understands queries without relying on external
modules. The second strategy is to enhance the facets by combining Large
Language Model (LLM) and the small model. Overall performance improves when
small model and LLM are combined rather than facet generation individually.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating Large Language Models as Generative User Simulators for
  Conversational Recommendation <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09738v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09738v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Se-eun Yoon, Zhankui He, Jessica Maria Echterhoff, Julian McAuley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Synthetic users are cost-effective proxies for real users in the evaluation
of conversational recommender systems. Large language models show promise in
simulating human-like behavior, raising the question of their ability to
represent a diverse population of users. We introduce a new protocol to measure
the degree to which language models can accurately emulate human behavior in
conversational recommendation. This protocol is comprised of five tasks, each
designed to evaluate a key property that a synthetic user should exhibit:
choosing which items to talk about, expressing binary preferences, expressing
open-ended preferences, requesting recommendations, and giving feedback.
Through evaluation of baseline simulators, we demonstrate these tasks
effectively reveal deviations of language models from human behavior, and offer
insights on how to reduce the deviations with model selection and prompting
strategies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NoteLLM: A Retrievable Large Language Model for Note Recommendation <span class="chip">WWW'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.01744v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.01744v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chao Zhang, Shiwei Wu, Haoxin Zhang, Tong Xu, Yan Gao, Yao Hu, Di Wu, Enhong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  People enjoy sharing "notes" including their experiences within online
communities. Therefore, recommending notes aligned with user interests has
become a crucial task. Existing online methods only input notes into BERT-based
models to generate note embeddings for assessing similarity. However, they may
underutilize some important cues, e.g., hashtags or categories, which represent
the key concepts of notes. Indeed, learning to generate hashtags/categories can
potentially enhance note embeddings, both of which compress key note
information into limited content. Besides, Large Language Models (LLMs) have
significantly outperformed BERT in understanding natural languages. It is
promising to introduce LLMs into note recommendation. In this paper, we propose
a novel unified framework called NoteLLM, which leverages LLMs to address the
item-to-item (I2I) note recommendation. Specifically, we utilize Note
Compression Prompt to compress a note into a single special token, and further
learn the potentially related notes' embeddings via a contrastive learning
approach. Moreover, we use NoteLLM to summarize the note and generate the
hashtag/category automatically through instruction tuning. Extensive
validations on real scenarios demonstrate the effectiveness of our proposed
method compared with the online baseline and show major improvements in the
recommendation system of Xiaohongshu.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a WWW'24 full paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Word4Per: Zero-shot Composed Person Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.16515v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.16515v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Delong Liu, Haiwen Li, Zhicheng Zhao, Fei Su, Yuan Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Searching for specific person has great social benefits and security value,
and it often involves a combination of visual and textual information.
Conventional person retrieval methods, whether image-based or text-based,
usually fall short in effectively harnessing both types of information, leading
to the loss of accuracy. In this paper, a whole new task called Composed Person
Retrieval (CPR) is proposed to jointly utilize both image and text information
for target person retrieval. However, the supervised CPR requires very costly
manual annotation dataset, while there are currently no available resources. To
mitigate this issue, we firstly introduce the Zero-shot Composed Person
Retrieval (ZS-CPR), which leverages existing domain-related data to resolve the
CPR problem without expensive annotations. Secondly, to learn ZS-CPR model, we
propose a two-stage learning framework, Word4Per, where a lightweight Textual
Inversion Network (TINet) and a text-based person retrieval model based on
fine-tuned Contrastive Language-Image Pre-training (CLIP) network are learned
without utilizing any CPR data. Thirdly, a finely annotated Image-Text Composed
Person Retrieval (ITCPR) dataset is built as the benchmark to assess the
performance of the proposed Word4Per framework. Extensive experiments under
both Rank-1 and mAP demonstrate the effectiveness of Word4Per for the ZS-CPR
task, surpassing the comparative methods by over 10\%. The code and ITCPR
dataset will be publicly available at
https://github.com/Delong-liu-bupt/Word4Per.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GNNUERS: Fairness Explanation in GNNs for Recommendation via
  Counterfactual Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.06182v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.06182v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giacomo Medda, Francesco Fabbri, Mirko Marras, Ludovico Boratto, Gianni Fenu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nowadays, research into personalization has been focusing on explainability
and fairness. Several approaches proposed in recent works are able to explain
individual recommendations in a post-hoc manner or by explanation paths.
However, explainability techniques applied to unfairness in recommendation have
been limited to finding user/item features mostly related to biased
recommendations. In this paper, we devised a novel algorithm that leverages
counterfactuality methods to discover user unfairness explanations in the form
of user-item interactions. In our counterfactual framework, interactions are
represented as edges in a bipartite graph, with users and items as nodes. Our
bipartite graph explainer perturbs the topological structure to find an altered
version that minimizes the disparity in utility between the protected and
unprotected demographic groups. Experiments on four real-world graphs coming
from various domains showed that our method can systematically explain user
unfairness on three state-of-the-art GNN-based recommendation models. Moreover,
an empirical evaluation of the perturbed network uncovered relevant patterns
that justify the nature of the unfairness discovered by the generated
explanations. The source code and the preprocessed data sets are available at
https://github.com/jackmedda/RS-BGExplainer.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the resilience of Collaborative Learning-based Recommender Systems
  Against Community Detection Attack 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.08929v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.08929v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yacine Belal, Sonia Ben Mokhtar, Mohamed Maouche, Anthony Simonet-Boulogne
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Collaborative-learning-based recommender systems emerged following the
success of collaborative learning techniques such as Federated Learning (FL)
and Gossip Learning (GL). In these systems, users participate in the training
of a recommender system while maintaining their history of consumed items on
their devices. While these solutions seemed appealing for preserving the
privacy of the participants at first glance, recent studies have revealed that
collaborative learning can be vulnerable to various privacy attacks. In this
paper, we study the resilience of collaborative learning-based recommender
systems against a novel privacy attack called Community Detection Attack (CDA).
This attack enables an adversary to identify community members based on a
chosen set of items (eg., identifying users interested in specific
points-of-interest). Through experiments on three real recommendation datasets
using two state-of-the-art recommendation models, we evaluate the sensitivity
of an FL-based recommender system as well as two flavors of Gossip
Learning-based recommender systems to CDA. The results show that across all
models and datasets, the FL setting is more vulnerable to CDA compared to
Gossip settings. Furthermore, we assess two off-the-shelf mitigation
strategies, namely differential privacy (DP) and a \emph{Share less} policy,
which consists of sharing a subset of less sensitive model parameters. The
findings indicate a more favorable privacy-utility trade-off for the
\emph{Share less} strategy, particularly in FedRecs.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TRIP: Temporal Residual Learning with Image Noise Prior for
  Image-to-Video Diffusion Models <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17005v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17005v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhongwei Zhang, Fuchen Long, Yingwei Pan, Zhaofan Qiu, Ting Yao, Yang Cao, Tao Mei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in text-to-video generation have demonstrated the utility of
powerful diffusion models. Nevertheless, the problem is not trivial when
shaping diffusion models to animate static image (i.e., image-to-video
generation). The difficulty originates from the aspect that the diffusion
process of subsequent animated frames should not only preserve the faithful
alignment with the given image but also pursue temporal coherence among
adjacent frames. To alleviate this, we present TRIP, a new recipe of
image-to-video diffusion paradigm that pivots on image noise prior derived from
static image to jointly trigger inter-frame relational reasoning and ease the
coherent temporal modeling via temporal residual learning. Technically, the
image noise prior is first attained through one-step backward diffusion process
based on both static image and noised video latent codes. Next, TRIP executes a
residual-like dual-path scheme for noise prediction: 1) a shortcut path that
directly takes image noise prior as the reference noise of each frame to
amplify the alignment between the first frame and subsequent frames; 2) a
residual path that employs 3D-UNet over noised video and static image latent
codes to enable inter-frame relational reasoning, thereby easing the learning
of the residual noise for each frame. Furthermore, both reference and residual
noise of each frame are dynamically merged via attention mechanism for final
video generation. Extensive experiments on WebVid-10M, DTDB and MSR-VTT
datasets demonstrate the effectiveness of our TRIP for image-to-video
generation. Please see our project page at https://trip-i2v.github.io/TRIP/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024; Project page: https://trip-i2v.github.io/TRIP/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SD-DiT: Unleashing the Power of <span class="highlight-title">Self-supervised</span> Discrimination in
  Diffusion <span class="highlight-title">Transformer</span> <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17004v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17004v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Zhu, Yingwei Pan, Yehao Li, Ting Yao, Zhenglong Sun, Tao Mei, Chang Wen Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion Transformer (DiT) has emerged as the new trend of generative
diffusion models on image generation. In view of extremely slow convergence in
typical DiT, recent breakthroughs have been driven by mask strategy that
significantly improves the training efficiency of DiT with additional
intra-image contextual learning. Despite this progress, mask strategy still
suffers from two inherent limitations: (a) training-inference discrepancy and
(b) fuzzy relations between mask reconstruction & generative diffusion process,
resulting in sub-optimal training of DiT. In this work, we address these
limitations by novelly unleashing the self-supervised discrimination knowledge
to boost DiT training. Technically, we frame our DiT in a teacher-student
manner. The teacher-student discriminative pairs are built on the diffusion
noises along the same Probability Flow Ordinary Differential Equation (PF-ODE).
Instead of applying mask reconstruction loss over both DiT encoder and decoder,
we decouple DiT encoder and decoder to separately tackle discriminative and
generative objectives. In particular, by encoding discriminative pairs with
student and teacher DiT encoders, a new discriminative loss is designed to
encourage the inter-image alignment in the self-supervised embedding space.
After that, student samples are fed into student DiT decoder to perform the
typical generative diffusion task. Extensive experiments are conducted on
ImageNet dataset, and our method achieves a competitive balance between
training cost and generative capacity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VP3D: Unleashing 2D Visual <span class="highlight-title">Prompt</span> for Text-to-3D Generation <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17001v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17001v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Chen, Yingwei Pan, Haibo Yang, Ting Yao, Tao Mei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent innovations on text-to-3D generation have featured Score Distillation
Sampling (SDS), which enables the zero-shot learning of implicit 3D models
(NeRF) by directly distilling prior knowledge from 2D diffusion models.
However, current SDS-based models still struggle with intricate text prompts
and commonly result in distorted 3D models with unrealistic textures or
cross-view inconsistency issues. In this work, we introduce a novel Visual
Prompt-guided text-to-3D diffusion model (VP3D) that explicitly unleashes the
visual appearance knowledge in 2D visual prompt to boost text-to-3D generation.
Instead of solely supervising SDS with text prompt, VP3D first capitalizes on
2D diffusion model to generate a high-quality image from input text, which
subsequently acts as visual prompt to strengthen SDS optimization with explicit
visual appearance. Meanwhile, we couple the SDS optimization with additional
differentiable reward function that encourages rendering images of 3D models to
better visually align with 2D visual prompt and semantically match with text
prompt. Through extensive experiments, we show that the 2D Visual Prompt in our
VP3D significantly eases the learning of visual appearance of 3D models and
thus leads to higher visual fidelity with more detailed textures. It is also
appealing in view that when replacing the self-generating visual prompt with a
given reference image, VP3D is able to trigger a new task of stylized
text-to-3D generation. Our project page is available at
https://vp3d-cvpr24.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024; Project page: https://vp3d-cvpr24.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Spatial Adaptation and Temporal Coherence in Diffusion Models
  for Video Super-Resolution <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17000v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17000v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhikai Chen, Fuchen Long, Zhaofan Qiu, Ting Yao, Wengang Zhou, Jiebo Luo, Tao Mei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models are just at a tipping point for image super-resolution task.
Nevertheless, it is not trivial to capitalize on diffusion models for video
super-resolution which necessitates not only the preservation of visual
appearance from low-resolution to high-resolution videos, but also the temporal
consistency across video frames. In this paper, we propose a novel approach,
pursuing Spatial Adaptation and Temporal Coherence (SATeCo), for video
super-resolution. SATeCo pivots on learning spatial-temporal guidance from
low-resolution videos to calibrate both latent-space high-resolution video
denoising and pixel-space video reconstruction. Technically, SATeCo freezes all
the parameters of the pre-trained UNet and VAE, and only optimizes two
deliberately-designed spatial feature adaptation (SFA) and temporal feature
alignment (TFA) modules, in the decoder of UNet and VAE. SFA modulates frame
features via adaptively estimating affine parameters for each pixel,
guaranteeing pixel-wise guidance for high-resolution frame synthesis. TFA
delves into feature interaction within a 3D local window (tubelet) through
self-attention, and executes cross-attention between tubelet and its
low-resolution counterpart to guide temporal feature alignment. Extensive
experiments conducted on the REDS4 and Vid4 datasets demonstrate the
effectiveness of our approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Low-Latency and Energy-Efficient Hybrid P2P-CDN Live Video
  Streaming 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16985v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16985v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Reza Farahani, Christian Timmerer, Hermann Hellwagner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Streaming segmented videos over the Hypertext Transfer Protocol (HTTP) is an
increasingly popular approach in both live and video-on-demand (VoD)
applications. However, designing a scalable and adaptable framework that
reduces servers energy consumption and supports low latency and high quality
services, particularly for live video streaming scenarios, is still challenging
for Over-The-Top (OTT) service providers. To address such challenges, this
paper introduces a new hybrid P2P-CDN framework that leverages new networking
and computing paradigms, i.e., Network Function Virtualization (NFV) and edge
computing for live video streaming. The proposed framework introduces a
multi-layer architecture and a tree of possible actions therein (an action
tree), taking into account all available resources from peers, edge, and CDN
servers to efficiently distribute video fetching and transcoding tasks across a
hybrid P2P-CDN network, consequently enhancing the users latency and video
quality. We also discuss our testbed designed to validate the framework and
compare it with baseline methods. The experimental results indicate that the
proposed framework improves user Quality of Experience (QoE), reduces client
serving latency, and improves edge server energy consumption compared to
baseline approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 3 figures, Special Issue on Sustainable Multimedia
  Communications and Services, IEEE MMTC Communications</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Network-Assisted Delivery of Adaptive Video Streaming Services through
  CDN, SDN, and MEC 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16951v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16951v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Reza Farahani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimedia applications, mainly video streaming services, are currently the
dominant source of network load worldwide. In recent Video-on-Demand (VoD) and
live video streaming services, traditional streaming delivery techniques have
been replaced by adaptive solutions based on the HTTP protocol. Current trends
toward high-resolution (e.g., 8K) and/or low-latency VoD and live video
streaming pose new challenges to end-to-end (E2E) bandwidth demand and have
stringent delay requirements. To do this, video providers typically rely on
Content Delivery Networks (CDNs) to ensure that they provide scalable video
streaming services. To support future streaming scenarios involving millions of
users, it is necessary to increase the CDNs' efficiency. It is widely agreed
that these requirements may be satisfied by adopting emerging networking
techniques to present Network-Assisted Video Streaming (NAVS) methods.
Motivated by this, this thesis goes one step beyond traditional pure
client-based HAS algorithms by incorporating (an) in-network component(s) with
a broader view of the network to present completely transparent NAVS solutions
for HAS clients.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>PhD thesis defended in 22.08.2023
  (https://netlibrary.aau.at/obvuklhs/content/titleinfo/9173622)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unraveling Instance Associations: A Closer Look for Audio-Visual
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.02970v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.02970v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanhong Chen, Yuyuan Liu, Hu Wang, Fengbei Liu, Chong Wang, Helen Frazer, Gustavo Carneiro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio-visual segmentation (AVS) is a challenging task that involves
accurately segmenting sounding objects based on audio-visual cues. The
effectiveness of audio-visual learning critically depends on achieving accurate
cross-modal alignment between sound and visual objects. Successful audio-visual
learning requires two essential components: 1) a challenging dataset with
high-quality pixel-level multi-class annotated images associated with audio
files, and 2) a model that can establish strong links between audio information
and its corresponding visual object. However, these requirements are only
partially addressed by current methods, with training sets containing biased
audio-visual data, and models that generalise poorly beyond this biased
training set. In this work, we propose a new cost-effective strategy to build
challenging and relatively unbiased high-quality audio-visual segmentation
benchmarks. We also propose a new informative sample mining method for
audio-visual supervised contrastive learning to leverage discriminative
contrastive samples to enforce cross-modal understanding. We show empirical
results that demonstrate the effectiveness of our benchmark. Furthermore,
experiments conducted on existing AVS datasets and on our new benchmark show
that our method achieves state-of-the-art (SOTA) segmentation accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code is available at https://github.com/cyh-0/CAVP</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Contrastive <span class="highlight-title">Pre-Train</span>ing with Multi-View Fusion for No-Reference Point
  Cloud Quality Assessment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10066v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10066v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyu Shan, Yujie Zhang, Qi Yang, Haichen Yang, Yiling Xu, Jenq-Neng Hwang, Xiaozhong Xu, Shan Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  No-reference point cloud quality assessment (NR-PCQA) aims to automatically
evaluate the perceptual quality of distorted point clouds without available
reference, which have achieved tremendous improvements due to the utilization
of deep neural networks. However, learning-based NR-PCQA methods suffer from
the scarcity of labeled data and usually perform suboptimally in terms of
generalization. To solve the problem, we propose a novel contrastive
pre-training framework tailored for PCQA (CoPA), which enables the pre-trained
model to learn quality-aware representations from unlabeled data. To obtain
anchors in the representation space, we project point clouds with different
distortions into images and randomly mix their local patches to form mixed
images with multiple distortions. Utilizing the generated anchors, we constrain
the pre-training process via a quality-aware contrastive loss following the
philosophy that perceptual quality is closely related to both content and
distortion. Furthermore, in the model fine-tuning stage, we propose a
semantic-guided multi-view fusion module to effectively integrate the features
of projected images from multiple perspectives. Extensive experiments show that
our method outperforms the state-of-the-art PCQA methods on popular benchmarks.
Further investigations demonstrate that CoPA can also benefit existing
learning-based PCQA models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multimodal Interaction Modeling via <span class="highlight-title">Self-Supervised</span> Multi-Task Learning
  for <span class="highlight-title">Review</span> Helpfulness Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.18107v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.18107v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        HongLin Gong, Mengzhao Jia, Liqiang Jing
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In line with the latest research, the task of identifying helpful reviews
from a vast pool of user-generated textual and visual data has become a
prominent area of study. Effective modal representations are expected to
possess two key attributes: consistency and differentiation. Current methods
designed for Multimodal Review Helpfulness Prediction (MRHP) face limitations
in capturing distinctive information due to their reliance on uniform
multimodal annotation. The process of adding varied multimodal annotations is
not only time-consuming but also labor-intensive. To tackle these challenges,
we propose an auto-generated scheme based on multi-task learning to generate
pseudo labels. This approach allows us to simultaneously train for the global
multimodal interaction task and the separate cross-modal interaction subtasks,
enabling us to learn and leverage both consistency and differentiation
effectively. Subsequently, experimental results validate the effectiveness of
pseudo labels, and our approach surpasses previous textual and multimodal
baseline models on two widely accessible benchmark datasets, providing a
solution to the MRHP problem.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages,4 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cartoon Hallucinations Detection: Pose-aware In Context Visual Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15048v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15048v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bumsoo Kim, Wonseop Shin, Kyuchul Lee, Sanghyun Seo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale Text-to-Image (TTI) models have become a common approach for
generating training data in various generative fields. However, visual
hallucinations, which contain perceptually critical defects, remain a concern,
especially in non-photorealistic styles like cartoon characters. We propose a
novel visual hallucination detection system for cartoon character images
generated by TTI models. Our approach leverages pose-aware in-context visual
learning (PA-ICVL) with Vision-Language Models (VLMs), utilizing both RGB
images and pose information. By incorporating pose guidance from a fine-tuned
pose estimator, we enable VLMs to make more accurate decisions. Experimental
results demonstrate significant improvements in identifying visual
hallucinations compared to baseline methods relying solely on RGB images. This
research advances TTI models by mitigating visual hallucinations, expanding
their potential in non-photorealistic domains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 12 figures, 1 table, Project page:
  https://gh-bumsookim.github.io/Cartoon-Hallucinations-Detection/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Noisy-Correspondence Learning for Text-to-Image Person Re-identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.09911v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.09911v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Qin, Yingke Chen, Dezhong Peng, Xi Peng, Joey Tianyi Zhou, Peng Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-image person re-identification (TIReID) is a compelling topic in the
cross-modal community, which aims to retrieve the target person based on a
textual query. Although numerous TIReID methods have been proposed and achieved
promising performance, they implicitly assume the training image-text pairs are
correctly aligned, which is not always the case in real-world scenarios. In
practice, the image-text pairs inevitably exist under-correlated or even
false-correlated, a.k.a noisy correspondence (NC), due to the low quality of
the images and annotation errors. To address this problem, we propose a novel
Robust Dual Embedding method (RDE) that can learn robust visual-semantic
associations even with NC. Specifically, RDE consists of two main components:
1) A Confident Consensus Division (CCD) module that leverages the dual-grained
decisions of dual embedding modules to obtain a consensus set of clean training
data, which enables the model to learn correct and reliable visual-semantic
associations. 2) A Triplet Alignment Loss (TAL) relaxes the conventional
Triplet Ranking loss with the hardest negative samples to a log-exponential
upper bound over all negative ones, thus preventing the model collapse under NC
and can also focus on hard-negative samples for promising performance. We
conduct extensive experiments on three public benchmarks, namely CUHK-PEDES,
ICFG-PEDES, and RSTPReID, to evaluate the performance and robustness of our
RDE. Our method achieves state-of-the-art results both with and without
synthetic noisy correspondences on all three datasets. Code is available at
https://github.com/QinYang79/RDE.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-24T00:00:00Z">2024-03-24</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ultra Low-Cost Two-Stage Multimodal System for Non-Normative Behavior
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16151v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16151v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Albert Lu, Stephen Cranefield
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The online community has increasingly been inundated by a toxic wave of
harmful comments. In response to this growing challenge, we introduce a
two-stage ultra-low-cost multimodal harmful behavior detection method designed
to identify harmful comments and images with high precision and recall rates.
We first utilize the CLIP-ViT model to transform tweets and images into
embeddings, effectively capturing the intricate interplay of semantic meaning
and subtle contextual clues within texts and images. Then in the second stage,
the system feeds these embeddings into a conventional machine learning
classifier like SVM or logistic regression, enabling the system to be trained
rapidly and to perform inference at an ultra-low cost. By converting tweets
into rich multimodal embeddings through the CLIP-ViT model and utilizing them
to train conventional machine learning classifiers, our system is not only
capable of detecting harmful textual information with near-perfect performance,
achieving precision and recall rates above 99\% but also demonstrates the
ability to zero-shot harmful images without additional training, thanks to its
multimodal embedding input. This capability empowers our system to identify
unseen harmful images without requiring extensive and costly image datasets.
Additionally, our system quickly adapts to new harmful content; if a new
harmful content pattern is identified, we can fine-tune the classifier with the
corresponding tweets' embeddings to promptly update the system. This makes it
well suited to addressing the ever-evolving nature of online harmfulness,
providing online communities with a robust, generalizable, and cost-effective
tool to safeguard their communities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>to be appear in International Workshop on Coordination,
  Organizations, Institutions, Norms and Ethics for Governance of Multi-Agent
  Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Complementary Recommendation in E-commerce: Definition, Approaches, and
  Future Directions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16135v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16135v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linyue Li, Zhijuan Du
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, complementary recommendation has received extensive
attention in the e-commerce domain. In this paper, we comprehensively summarize
and compare 34 representative studies conducted between 2009 and 2024. Firstly,
we compare the data and methods used for modeling complementary relationships
between products, including simple complementarity and more complex scenarios
such as asymmetric complementarity, the coexistence of substitution and
complementarity relationships between products, and varying degrees of
complementarity between different pairs of products. Next, we classify and
compare the models based on the research problems of complementary
recommendation, such as diversity, personalization, and cold-start.
Furthermore, we provide a comparative analysis of experimental results from
different studies conducted on the same dataset, which helps identify the
strengths and weaknesses of the research. Compared to previous surveys, this
paper provides a more updated and comprehensive summary of the research,
discusses future research directions, and contributes to the advancement of
this field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages,9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RankingSHAP -- Listwise Feature Attribution Explanations for Ranking
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16085v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16085v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maria Heuss, Maarten de Rijke, Avishek Anand
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Feature attributions are a commonly used explanation type, when we want to
posthoc explain the prediction of a trained model. Yet, they are not very well
explored in IR. Importantly, feature attribution has rarely been rigorously
defined, beyond attributing the most important feature the highest value. What
it means for a feature to be more important than others is often left vague.
Consequently, most approaches focus on just selecting the most important
features and under utilize or even ignore the relative importance within
features. In this work, we rigorously define the notion of feature attribution
for ranking models, and list essential properties that a valid attribution
should have. We then propose RankingSHAP as a concrete instantiation of a
list-wise ranking attribution method. Contrary to current explanation
evaluation schemes that focus on selections, we propose two novel evaluation
paradigms for evaluating attributions over learning-to-rank models. We evaluate
RankingSHAP for commonly used learning-to-rank datasets to showcase the more
nuanced use of an attribution method while highlighting the limitations of
selection-based explanations. In a simulated experiment we design an
interpretable model to demonstrate how list-wise ranking attributes can be used
to investigate model decisions and evaluate the explanations qualitatively.
Because of the contrastive nature of the ranking task, our understanding of
ranking model decisions can substantially benefit from feature attribution
explanations like RankingSHAP.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Knowledge-aware Dual-side Attribute-enhanced Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16037v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16037v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taotian Pang, Xingyu Lou, Fei Zhao, Zhen Wu, Kuiyao Dong, Qiuying Peng, Yue Qi, Xinyu Dai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  \textit{Knowledge-aware} recommendation methods (KGR) based on \textit{graph
neural networks} (GNNs) and \textit{contrastive learning} (CL) have achieved
promising performance. However, they fall short in modeling fine-grained user
preferences and further fail to leverage the \textit{preference-attribute
connection} to make predictions, leading to sub-optimal performance. To address
the issue, we propose a method named \textit{\textbf{K}nowledge-aware
\textbf{D}ual-side \textbf{A}ttribute-enhanced \textbf{R}ecommendation} (KDAR).
Specifically, we build \textit{user preference representations} and
\textit{attribute fusion representations} upon the attribute information in
knowledge graphs, which are utilized to enhance \textit{collaborative
filtering} (CF) based user and item representations, respectively. To
discriminate the contribution of each attribute in these two types of
attribute-based representations, a \textit{multi-level collaborative alignment
contrasting} mechanism is proposed to align the importance of attributes with
CF signals. Experimental results on four benchmark datasets demonstrate the
superiority of KDAR over several state-of-the-art baselines. Further analyses
verify the effectiveness of our method. The code of KDAR is released at:
\href{https://github.com/TJTP/KDAR}{https://github.com/TJTP/KDAR}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Our Model Achieves Excellent Performance on MovieLens: What Does it
  Mean? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.09985v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.09985v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu-chen Fan, Yitong Ji, Jie Zhang, Aixin Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A typical benchmark dataset for recommender system (RecSys) evaluation
consists of user-item interactions generated on a platform within a time
period. The interaction generation mechanism partially explains why a user
interacts with (e.g., like, purchase, rate) an item, and the context of when a
particular interaction happened. In this study, we conduct a meticulous
analysis of the MovieLens dataset and explain the potential impact of using the
dataset for evaluating recommendation algorithms. We make a few main findings
from our analysis. First, there are significant differences in user
interactions at the different stages when a user interacts with the MovieLens
platform. The early interactions largely define the user portrait which affects
the subsequent interactions. Second, user interactions are highly affected by
the candidate movies that are recommended by the platform's internal
recommendation algorithm(s). Third, changing the order of user interactions
makes it more difficult for sequential algorithms to capture the progressive
interaction process. We further discuss the discrepancy between the interaction
generation mechanism that is employed by the MovieLens system and that of
typical real-world recommendation scenarios. In summary, the MovieLens platform
demonstrates an efficient and effective way of collecting user preferences to
address cold-starts. However, models that achieve excellent recommendation
accuracy on the MovieLens dataset may not demonstrate superior performance in
practice, for at least two kinds of differences: (i) the differences in the
contexts of user-item interaction generation, and (ii) the differences in user
knowledge about the item collections. While results on MovieLens can be useful
as a reference, they should not be solely relied upon as the primary
justification for the effectiveness of a recommendation system model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Spacerini: Plug-and-play Search Engines with Pyserini and Hugging Face 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.14534v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.14534v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christopher Akiki, Odunayo Ogundepo, Aleksandra Piktus, Xinyu Zhang, Akintunde Oladipo, Jimmy Lin, Martin Potthast
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Spacerini, a tool that integrates the Pyserini toolkit for
reproducible information retrieval research with Hugging Face to enable the
seamless construction and deployment of interactive search engines. Spacerini
makes state-of-the-art sparse and dense retrieval models more accessible to
non-IR practitioners while minimizing deployment effort. This is useful for NLP
researchers who want to better understand and validate their research by
performing qualitative analyses of training corpora, for IR researchers who
want to demonstrate new retrieval models integrated into the growing Pyserini
ecosystem, and for third parties reproducing the work of other researchers.
Spacerini is open source and includes utilities for loading, preprocessing,
indexing, and deploying search engines locally and remotely. We demonstrate a
portfolio of 13 search engines created with Spacerini for different use cases.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CFAT: Unleashing TriangularWindows for Image Super-resolution <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16143v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16143v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhisek Ray, Gaurav Kumar, Maheshkumar H. Kolekar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer-based models have revolutionized the field of image
super-resolution (SR) by harnessing their inherent ability to capture complex
contextual features. The overlapping rectangular shifted window technique used
in transformer architecture nowadays is a common practice in super-resolution
models to improve the quality and robustness of image upscaling. However, it
suffers from distortion at the boundaries and has limited unique shifting
modes. To overcome these weaknesses, we propose a non-overlapping triangular
window technique that synchronously works with the rectangular one to mitigate
boundary-level distortion and allows the model to access more unique sifting
modes. In this paper, we propose a Composite Fusion Attention Transformer
(CFAT) that incorporates triangular-rectangular window-based local attention
with a channel-based global attention technique in image super-resolution. As a
result, CFAT enables attention mechanisms to be activated on more image pixels
and captures long-range, multi-scale features to improve SR performance. The
extensive experimental results and ablation study demonstrate the effectiveness
of CFAT in the SR domain. Our proposed model shows a significant 0.7 dB
performance improvement over other state-of-the-art SR architectures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Landmark-Guided Cross-Speaker Lip Reading with Mutual Information
  Regularization <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16071v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16071v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linzhi Wu, Xingyu Zhang, Yakun Zhang, Changyan Zheng, Tiejun Liu, Liang Xie, Ye Yan, Erwei Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lip reading, the process of interpreting silent speech from visual lip
movements, has gained rising attention for its wide range of realistic
applications. Deep learning approaches greatly improve current lip reading
systems. However, lip reading in cross-speaker scenarios where the speaker
identity changes, poses a challenging problem due to inter-speaker variability.
A well-trained lip reading system may perform poorly when handling a brand new
speaker. To learn a speaker-robust lip reading model, a key insight is to
reduce visual variations across speakers, avoiding the model overfitting to
specific speakers. In this work, in view of both input visual clues and latent
representations based on a hybrid CTC/attention architecture, we propose to
exploit the lip landmark-guided fine-grained visual clues instead of
frequently-used mouth-cropped images as input features, diminishing
speaker-specific appearance characteristics. Furthermore, a max-min mutual
information regularization approach is proposed to capture speaker-insensitive
latent representations. Experimental evaluations on public lip reading datasets
demonstrate the effectiveness of the proposed approach under the intra-speaker
and inter-speaker conditions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mixture-of-<span class="highlight-title">Prompt</span>-Experts for Multi-modal Semantic Understanding <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11311v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11311v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zichen Wu, Hsiu-Yuan Huang, Fanyi Qu, Yunfang Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep multimodal semantic understanding that goes beyond the mere superficial
content relation mining has received increasing attention in the realm of
artificial intelligence. The challenges of collecting and annotating
high-quality multi-modal data have underscored the significance of few-shot
learning. In this paper, we focus on two critical tasks under this context:
few-shot multi-modal sarcasm detection (MSD) and multi-modal sentiment analysis
(MSA). To address them, we propose Mixture-of-Prompt-Experts with Block-Aware
Prompt Fusion (MoPE-BAF), a novel multi-modal soft prompt framework based on
the unified vision-language model (VLM). Specifically, we design three experts
of soft prompts: a text prompt and an image prompt that extract
modality-specific features to enrich the single-modal representation, and a
unified prompt to assist multi-modal interaction. Additionally, we reorganize
Transformer layers into several blocks and introduce cross-modal prompt
attention between adjacent blocks, which smoothens the transition from
single-modal representation to multi-modal fusion. On both MSD and MSA datasets
in few-shot setting, our proposed model not only surpasses the 8.2B model
InstructBLIP with merely 2% parameters (150M), but also significantly
outperforms other widely-used prompt methods on VLMs or task-specific methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LREC-COLING 2024, Long Paper</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-23T00:00:00Z">2024-03-23</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Model, Analyze, and Comprehend User Interactions and Various Attributes
  within a Social Media Platform 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15937v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15937v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Kaykobad Reza, S M Maksudul Alam, Yiran Luo, Youzhe Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  How can we effectively model, analyze, and comprehend user interactions and
various attributes within a social media platform based on post-comment
relationship? In this study, we propose a novel graph-based approach to model
and analyze user interactions within a social media platform based on
post-comment relationship. We construct a user interaction graph from social
media data and analyze it to gain insights into community dynamics, user
behavior, and content preferences. Our investigation reveals that while 56.05%
of the active users are strongly connected within the community, only 0.8% of
them significantly contribute to its dynamics. Moreover, we observe temporal
variations in community activity, with certain periods experiencing heightened
engagement. Additionally, our findings highlight a correlation between user
activity and popularity showing that more active users are generally more
popular. Alongside these, a preference for positive and informative content is
also observed where 82.41% users preferred positive and informative content.
Overall, our study provides a comprehensive framework for understanding and
managing online communities, leveraging graph-based techniques to gain valuable
insights into user behavior and community dynamics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 Pages, 8 Figures, 3 Tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Human-Like Machine Comprehension: Few-Shot Relational Learning
  in Visually-Rich Documents <span class="chip">COLING2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15765v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15765v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Wang, Tang Li, Chenhui Chu, Nengjun Zhu, Rui Wang, Pinpin Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Key-value relations are prevalent in Visually-Rich Documents (VRDs), often
depicted in distinct spatial regions accompanied by specific color and font
styles. These non-textual cues serve as important indicators that greatly
enhance human comprehension and acquisition of such relation triplets. However,
current document AI approaches often fail to consider this valuable prior
information related to visual and spatial features, resulting in suboptimal
performance, particularly when dealing with limited examples. To address this
limitation, our research focuses on few-shot relational learning, specifically
targeting the extraction of key-value relation triplets in VRDs. Given the
absence of a suitable dataset for this task, we introduce two new few-shot
benchmarks built upon existing supervised benchmark datasets. Furthermore, we
propose a variational approach that incorporates relational 2D-spatial priors
and prototypical rectification techniques. This approach aims to generate
relation representations that are more aware of the spatial context and unseen
relation in a manner similar to human perception. Experimental results
demonstrate the effectiveness of our proposed method by showcasing its ability
to outperform existing methods. This study also opens up new possibilities for
practical applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 7 figures, accepted by LERC-COLING2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ User-Side Realization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15757v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15757v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryoma Sato
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Users are dissatisfied with services. Since the service is not tailor-made
for a user, it is natural for dissatisfaction to arise. The problem is, that
even if users are dissatisfied, they often do not have the means to resolve
their dissatisfaction. The user cannot alter the source code of the service,
nor can they force the service provider to change. The user has no choice but
to remain dissatisfied or quit the service. User-side realization offers
proactive solutions to this problem by providing general algorithms to deal
with common problems on the user's side. These algorithms run on the user's
side and solve the problems without having the service provider change the
service itself.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Doctoral Thesis</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ghost Sentence: A Tool for Everyday Users to Copyright Data from Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15740v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15740v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuai Zhao, Linchao Zhu, Ruijie Quan, Yi Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Web user data plays a central role in the ecosystem of pre-trained large
language models (LLMs) and their fine-tuned variants. Billions of data are
crawled from the web and fed to LLMs. How can \textit{\textbf{everyday web
users}} confirm if LLMs misuse their data without permission? In this work, we
suggest that users repeatedly insert personal passphrases into their documents,
enabling LLMs to memorize them. These concealed passphrases in user documents,
referred to as \textit{ghost sentences}, once they are identified in the
generated content of LLMs, users can be sure that their data is used for
training. To explore the effectiveness and usage of this copyrighting tool, we
define the \textit{user training data identification} task with ghost
sentences. Multiple datasets from various sources at different scales are
created and tested with LLMs of different sizes. For evaluation, we introduce a
last $k$ words verification manner along with two metrics: document and user
identification accuracy. In the specific case of instruction tuning of a 3B
LLaMA model, 11 out of 16 users with ghost sentences identify their data within
the generation content. These 16 users contribute 383 examples to $\sim$1.8M
training documents. For continuing pre-training of a 1.1B TinyLlama model, 61
out of 64 users with ghost sentences identify their data within the LLM output.
These 64 users contribute 1156 examples to $\sim$10M training documents.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint, work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ QueryExplorer: An Interactive Query Generation Assistant for Search and
  Exploration <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15667v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15667v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaustubh D. Dhole, Shivam Bajaj, Ramraj Chandradevan, Eugene Agichtein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Formulating effective search queries remains a challenging task, particularly
when users lack expertise in a specific domain or are not proficient in the
language of the content. Providing example documents of interest might be
easier for a user. However, such query-by-example scenarios are prone to
concept drift, and the retrieval effectiveness is highly sensitive to the query
generation method, without a clear way to incorporate user feedback. To enable
exploration and to support Human-In-The-Loop experiments we propose
QueryExplorer -- an interactive query generation, reformulation, and retrieval
interface with support for HuggingFace generation models and PyTerrier's
retrieval pipelines and datasets, and extensive logging of human feedback. To
allow users to create and modify effective queries, our demo supports
complementary approaches of using LLMs interactively, assisting the user with
edits and feedback at multiple stages of the query formulation process. With
support for recording fine-grained interactions and user annotations,
QueryExplorer can serve as a valuable experimental and research platform for
annotation, qualitative evaluation, and conducting Human-in-the-Loop (HITL)
experiments for complex search tasks where users struggle to formulate queries.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2024 Demonstration Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large Language Models for Generative Recommendation: A <span class="highlight-title">Survey</span> and
  Visionary Discussions <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.01157v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.01157v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Li, Yongfeng Zhang, Dugang Liu, Li Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLM) not only have revolutionized the field of natural
language processing (NLP) but also have the potential to reshape many other
fields, e.g., recommender systems (RS). However, most of the related work
treats an LLM as a component of the conventional recommendation pipeline (e.g.,
as a feature extractor), which may not be able to fully leverage the generative
power of LLM. Instead of separating the recommendation process into multiple
stages, such as score computation and re-ranking, this process can be
simplified to one stage with LLM: directly generating recommendations from the
complete pool of items. This survey reviews the progress, methods, and future
directions of LLM-based generative recommendation by examining three questions:
1) What generative recommendation is, 2) Why RS should advance to generative
recommendation, and 3) How to implement LLM-based generative recommendation for
various RS tasks. We hope that this survey can provide the context and guidance
needed to explore this interesting and emerging topic.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a conference paper at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ No more optimization rules: LLM-enabled policy-based multi-modal query
  optimizer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13597v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13597v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Wang, Haodi Ma, Daisy Zhe Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language model (LLM) has marked a pivotal moment in the field of
machine learning and deep learning. Recently its capability for query planning
has been investigated, including both single-modal and multi-modal queries.
However, there is no work on the query optimization capability of LLM. As a
critical (or could even be the most important) step that significantly impacts
the execution performance of the query plan, such analysis and attempts should
not be missed. From another aspect, existing query optimizers are usually
rule-based or rule-based + cost-based, i.e., they are dependent on manually
created rules to complete the query plan rewrite/transformation. Given the fact
that modern optimizers include hundreds to thousands of rules, designing a
multi-modal query optimizer following a similar way is significantly
time-consuming since we will have to enumerate as many multi-modal optimization
rules as possible, which has not been well addressed today. In this paper, we
investigate the query optimization ability of LLM and use LLM to design LaPuda,
a novel LLM and Policy based multi-modal query optimizer. Instead of
enumerating specific and detailed rules, LaPuda only needs a few abstract
policies to guide LLM in the optimization, by which much time and human effort
are saved. Furthermore, to prevent LLM from making mistakes or negative
optimization, we borrow the idea of gradient descent and propose a guided cost
descent (GCD) algorithm to perform the optimization, such that the optimization
can be kept in the correct direction. In our evaluation, our methods
consistently outperform the baselines in most cases. For example, the optimized
plans generated by our methods result in 1~3x higher execution speed than those
by the baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Yifan and Haodi contribute equally to the work</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ In-context Learning with Retrieved Demonstrations for Language Models: A
  <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.11624v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.11624v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Man Luo, Xin Xu, Yue Liu, Panupong Pasupat, Mehran Kazemi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models, especially pre-trained large language models, have showcased
remarkable abilities as few-shot in-context learners (ICL), adept at adapting
to new tasks with just a few demonstrations in the input context. However, the
model's ability to perform ICL is sensitive to the choice of the few-shot
demonstrations. Instead of using a fixed set of demonstrations, one recent
development is to retrieve demonstrations tailored to each input query. The
implementation of demonstration retrieval is relatively straightforward,
leveraging existing databases and retrieval systems. This not only improves the
efficiency and scalability of the learning process but also has been shown to
reduce biases inherent in manual example selection. In light of the encouraging
results and growing research in ICL with retrieved demonstrations, we conduct
an extensive review of studies in this area. In this survey, we discuss and
compare different design choices for retrieval models, retrieval training
procedures, and inference algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Group Benefits Instances Selection for Data Purification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15694v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15694v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenhuang Cai, Chuanyi Zhang, Dan Huang, Yuanbo Chen, Xiuyun Guan, Yazhou Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Manually annotating datasets for training deep models is very labor-intensive
and time-consuming. To overcome such inferiority, directly leveraging web
images to conduct training data becomes a natural choice. Nevertheless, the
presence of label noise in web data usually degrades the model performance.
Existing methods for combating label noise are typically designed and tested on
synthetic noisy datasets. However, they tend to fail to achieve satisfying
results on real-world noisy datasets. To this end, we propose a method named
GRIP to alleviate the noisy label problem for both synthetic and real-world
datasets. Specifically, GRIP utilizes a group regularization strategy that
estimates class soft labels to improve noise robustness. Soft label supervision
reduces overfitting on noisy labels and learns inter-class similarities to
benefit classification. Furthermore, an instance purification operation
globally identifies noisy labels by measuring the difference between each
training sample and its class soft label. Through operations at both group and
instance levels, our approach integrates the advantages of noise-robust and
noise-cleaning methods and remarkably alleviates the performance degradation
caused by noisy labels. Comprehensive experimental results on synthetic and
real-world datasets demonstrate the superiority of GRIP over the existing
state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by IEEE Intelligent Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DS-NeRV: Implicit Neural Video Representation with Decomposed Static and
  Dynamic Codes <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15679v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15679v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Yan, Zhihui Ke, Xiaobo Zhou, Tie Qiu, Xidong Shi, Dadong Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Implicit neural representations for video (NeRV) have recently become a novel
way for high-quality video representation. However, existing works employ a
single network to represent the entire video, which implicitly confuse static
and dynamic information. This leads to an inability to effectively compress the
redundant static information and lack the explicitly modeling of global
temporal-coherent dynamic details. To solve above problems, we propose DS-NeRV,
which decomposes videos into sparse learnable static codes and dynamic codes
without the need for explicit optical flow or residual supervision. By setting
different sampling rates for two codes and applying weighted sum and
interpolation sampling methods, DS-NeRV efficiently utilizes redundant static
information while maintaining high-frequency details. Additionally, we design a
cross-channel attention-based (CCA) fusion module to efficiently fuse these two
codes for frame decoding. Our approach achieves a high quality reconstruction
of 31.2 PSNR with only 0.35M parameters thanks to separate static and dynamic
codes representation and outperforms existing NeRV methods in many downstream
tasks. Our project website is at https://haoyan14.github.io/DS-NeRV.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024. Project page at https://haoyan14.github.io/DS-NeRV</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ K-pop Lyric Translation: <span class="highlight-title">Dataset</span>, Analysis, and Neural-Modelling <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.11093v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.11093v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haven Kim, Jongmin Jung, Dasaem Jeong, Juhan Nam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lyric translation, a field studied for over a century, is now attracting
computational linguistics researchers. We identified two limitations in
previous studies. Firstly, lyric translation studies have predominantly focused
on Western genres and languages, with no previous study centering on K-pop
despite its popularity. Second, the field of lyric translation suffers from a
lack of publicly available datasets; to the best of our knowledge, no such
dataset exists. To broaden the scope of genres and languages in lyric
translation studies, we introduce a novel singable lyric translation dataset,
approximately 89\% of which consists of K-pop song lyrics. This dataset aligns
Korean and English lyrics line-by-line and section-by-section. We leveraged
this dataset to unveil unique characteristics of K-pop lyric translation,
distinguishing it from other extensively studied genres, and to construct a
neural lyric translation model, thereby underscoring the importance of a
dedicated dataset for singable lyric translations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-22T00:00:00Z">2024-03-22</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Spectral Initialization for High-Dimensional Phase Retrieval with Biased
  Spatial Directions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15548v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15548v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pierre Bousseyroux, Marc Potters
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We explore a spectral initialization method that plays a central role in
contemporary research on signal estimation in nonconvex scenarios. In a
noiseless phase retrieval framework, we precisely analyze the method's
performance in the high-dimensional limit when sensing vectors follow a
multivariate Gaussian distribution for two rotationally invariant models of the
covariance matrix C. In the first model C is a projector on a lower dimensional
space while in the second it is a Wishart matrix. Our analytical results extend
the well-established case when C is the identity matrix. Our examination shows
that the introduction of biased spatial directions leads to a substantial
improvement in the spectral method's effectiveness, particularly when the
number of measurements is less than the signal's dimension. This extension also
consistently reveals a phase transition phenomenon dependent on the ratio
between sample size and signal dimension. Surprisingly, both of these models
share the same threshold value.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LimGen: Probing the LLMs for Generating Suggestive Limitations of
  Research Papers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15529v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15529v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdur Rahman Bin Md Faizullah, Ashok Urlana, Rahul Mishra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Examining limitations is a crucial step in the scholarly research reviewing
process, revealing aspects where a study might lack decisiveness or require
enhancement. This aids readers in considering broader implications for further
research. In this article, we present a novel and challenging task of
Suggestive Limitation Generation (SLG) for research papers. We compile a
dataset called LimGen, encompassing 4068 research papers and their associated
limitations from the ACL anthology. We investigate several approaches to
harness large language models (LLMs) for producing suggestive limitations, by
thoroughly examining the related challenges, practical insights, and potential
opportunities. Our LimGen dataset and code can be accessed at
https://github.com/armbf/LimGen.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fundus: A Simple-to-Use News Scraper Optimized for High Quality
  Extractions <span class="chip">ACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15279v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15279v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Max Dallabetta, Conrad Dobberstein, Adrian Breiding, Alan Akbik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces Fundus, a user-friendly news scraper that enables users
to obtain millions of high-quality news articles with just a few lines of code.
Unlike existing news scrapers, we use manually crafted, bespoke content
extractors that are specifically tailored to the formatting guidelines of each
supported online newspaper. This allows us to optimize our scraping for quality
such that retrieved news articles are textually complete and without HTML
artifacts. Further, our framework combines both crawling (retrieving HTML from
the web or large web archives) and content extraction into a single pipeline.
By providing a unified interface for a predefined collection of newspapers, we
aim to make Fundus broadly usable even for non-technical users. This paper
gives an overview of the framework, discusses our design choices, and presents
a comparative evaluation against other popular news scrapers. Our evaluation
shows that Fundus yields significantly higher quality extractions (complete and
artifact-free news articles) than prior work. The framework is available on
GitHub under https://github.com/flairNLP/fundus and can be simply installed
using pip.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 4 figures, submitted to ACL 2024, for a screencast see
  https://www.youtube.com/watch?v=9GJExMelhdI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FollowIR: Evaluating and Teaching Information Retrieval Models to Follow
  Instructions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15246v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15246v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Orion Weller, Benjamin Chang, Sean MacAvaney, Kyle Lo, Arman Cohan, Benjamin Van Durme, Dawn Lawrie, Luca Soldaini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern Large Language Models (LLMs) are capable of following long and complex
instructions that enable a diverse amount of user tasks. However, despite
Information Retrieval (IR) models using LLMs as the backbone of their
architectures, nearly all of them still only take queries as input, with no
instructions. For the handful of recent models that do take instructions, it's
unclear how they use them. We introduce our dataset FollowIR, which contains a
rigorous instruction evaluation benchmark as well as a training set for helping
IR models learn to better follow real-world instructions. FollowIR builds off
the long history of the TREC conferences: as TREC provides human annotators
with instructions (also known as narratives) to determine document relevance,
so should IR models be able to understand and decide relevance based on these
detailed instructions. Our evaluation benchmark starts with three deeply judged
TREC collections and alters the annotator instructions, re-annotating relevant
documents. Through this process, we can measure how well IR models follow
instructions, through a new pairwise evaluation framework. Our results indicate
that existing retrieval models fail to correctly use instructions, using them
for basic keywords and struggling to understand long-form information. However,
we show that it is possible for IR models to learn to follow complex
instructions: our new FollowIR-7B model has significant improvements (over 13%)
after fine-tuning on our training set.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GTC: GNN-<span class="highlight-title">Transformer</span> Co-contrastive Learning for <span class="highlight-title">Self-supervised</span>
  Heterogeneous Graph Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15520v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15520v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yundong Sun, Dongjie Zhu, Yansong Wang, Zhaoshuo Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks (GNNs) have emerged as the most powerful weapon for
various graph tasks due to the message-passing mechanism's great local
information aggregation ability. However, over-smoothing has always hindered
GNNs from going deeper and capturing multi-hop neighbors. Unlike GNNs,
Transformers can model global information and multi-hop interactions via
multi-head self-attention and a proper Transformer structure can show more
immunity to the over-smoothing problem. So, can we propose a novel framework to
combine GNN and Transformer, integrating both GNN's local information
aggregation and Transformer's global information modeling ability to eliminate
the over-smoothing problem? To realize this, this paper proposes a
collaborative learning scheme for GNN-Transformer and constructs GTC
architecture. GTC leverages the GNN and Transformer branch to encode node
information from different views respectively, and establishes contrastive
learning tasks based on the encoded cross-view information to realize
self-supervised heterogeneous graph representation. For the Transformer branch,
we propose Metapath-aware Hop2Token and CG-Hetphormer, which can cooperate with
GNN to attentively encode neighborhood information from different levels. As
far as we know, this is the first attempt in the field of graph representation
learning to utilize both GNN and Transformer to collaboratively capture
different view information and conduct cross-view contrastive learning. The
experiments on real datasets show that GTC exhibits superior performance
compared with state-of-the-art methods. Codes can be available at
https://github.com/PHD-lanyu/GTC.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bilateral Unsymmetrical Graph Contrastive Learning for Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15075v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15075v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaheng Yu, Jing Li, Yue He, Kai Zhu, Shuyi Zhang, Wen Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent methods utilize graph contrastive Learning within graph-structured
user-item interaction data for collaborative filtering and have demonstrated
their efficacy in recommendation tasks. However, they ignore that the
difference relation density of nodes between the user- and item-side causes the
adaptability of graphs on bilateral nodes to be different after multi-hop graph
interaction calculation, which limits existing models to achieve ideal results.
To solve this issue, we propose a novel framework for recommendation tasks
called Bilateral Unsymmetrical Graph Contrastive Learning (BusGCL) that
consider the bilateral unsymmetry on user-item node relation density for sliced
user and item graph reasoning better with bilateral slicing contrastive
training. Especially, taking into account the aggregation ability of
hypergraph-based graph convolutional network (GCN) in digging implicit
similarities is more suitable for user nodes, embeddings generated from three
different modules: hypergraph-based GCN, GCN and perturbed GCN, are sliced into
two subviews by the user- and item-side respectively, and selectively combined
into subview pairs bilaterally based on the characteristics of inter-node
relation structure. Furthermore, to align the distribution of user and item
embeddings after aggregation, a dispersing loss is leveraged to adjust the
mutual distance between all embeddings for maintaining learning ability.
Comprehensive experiments on two public datasets have proved the superiority of
BusGCL in comparison to various recommendation methods. Other models can simply
utilize our bilateral slicing contrastive learning to enhance recommending
performance without incurring extra expenses.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Language Modeling for Content-enriched Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.10435v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.10435v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junzhe Jiang, Shang Qu, Mingyue Cheng, Qi Liu, Zhiding Liu, Hao Zhang, Rujiao Zhang, Kai Zhang, Rui Li, Jiatong Li, Min Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems are indispensable in the realm of online applications,
and sequential recommendation has enjoyed considerable prevalence due to its
capacity to encapsulate the dynamic shifts in user interests. However, previous
sequential modeling methods still have limitations in capturing contextual
information. The primary reason is the lack of understanding of domain-specific
knowledge and item-related textual content by language models. Fortunately, the
emergence of powerful language models has unlocked the potential to incorporate
extensive world knowledge into recommendation algorithms, enabling them to go
beyond simple item attributes and truly understand the world surrounding user
preferences. To achieve this, we propose LANCER, which leverages the semantic
understanding capabilities of pre-trained language models to generate
personalized recommendations. Our approach bridges the gap between language
models and recommender systems, resulting in more human-like recommendations.
We demonstrate the effectiveness of our approach through a series of
experiments conducted on multiple benchmark datasets, showing promising results
and providing valuable insights into the influence of our model on sequential
recommendation tasks. Furthermore, our experimental codes are publicly
available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On Image Search in Histopathology 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.08699v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.08699v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        H. R. Tizhoosh, Liron Pantanowitz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pathology images of histopathology can be acquired from camera-mounted
microscopes or whole slide scanners. Utilizing similarity calculations to match
patients based on these images holds significant potential in research and
clinical contexts. Recent advancements in search technologies allow for
implicit quantification of tissue morphology across diverse primary sites,
facilitating comparisons and enabling inferences about diagnosis, and
potentially prognosis, and predictions for new patients when compared against a
curated database of diagnosed and treated cases. In this paper, we
comprehensively review the latest developments in image search technologies for
histopathology, offering a concise overview tailored for computational
pathology researchers seeking effective, fast and efficient image search
methods in their work.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A chapter in the Book "Artificial INtelligence in Digital Pathology"
  by Cohen and Chauhan, 2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dialogue Understandability: Why are we streaming movies with subtitles? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15336v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15336v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Helard Becerra, Alessandro Ragano, Diptasree Debnath, Asad Ullah, Crisron Rudolf Lucas, Martin Walsh, Andrew Hines
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Watching movies and TV shows with subtitles enabled is not simply down to
audibility or speech intelligibility. A variety of evolving factors related to
technological advances, cinema production and social behaviour challenge our
perception and understanding. This study seeks to formalise and give context to
these influential factors under a wider and novel term referred to as Dialogue
Understandability. We propose a working definition for Dialogue
Understandability being a listener's capacity to follow the story without undue
cognitive effort or concentration being required that impacts their Quality of
Experience (QoE). The paper identifies, describes and categorises the factors
that influence Dialogue Understandability mapping them over the QoE framework,
a media streaming lifecycle, and the stakeholders involved. We then explore
available measurement tools in the literature and link them to the factors they
could potentially be used for. The maturity and suitability of these tools is
evaluated over a set of pilot experiments. Finally, we reflect on the gaps that
still need to be filled, what we can measure and what not, future subjective
experiments, and new research trends that could help us to fully characterise
Dialogue Understandability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Experimental Studies of Metaverse Streaming 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15256v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15256v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haopeng Wang, Roberto Martinez-Velazquez, Haiwei Dong, Abdulmotaleb El Saddik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Metaverse aims to construct a large, unified, immersive, and shared digital
realm by combining various technologies, namely XR (extended reality),
blockchain, and digital twin, among others. This article explores the Metaverse
from the perspective of multimedia communication by conducting and analyzing
real-world experiments on four different Metaverse platforms: VR (virtual
reality) Vircadia, VR Mozilla Hubs, VRChat, and MR (mixed reality) Virtual
City. We first investigate the traffic patterns and network performance in the
three VR platforms. After raising the challenges of the Metaverse streaming and
investigating the potential methods to enhance Metaverse performance, we
propose a remote rendering architecture and verify its advantages through a
prototype involving the campus network and MR multimodal interaction by
comparison with local rendering.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE Consumer Electronics Magazine</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Not All Attention is Needed: Parameter and Computation Efficient
  Transfer Learning for Multi-modal Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15226v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15226v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiong Wu, Weihao Ye, Yiyi Zhou, Xiaoshuai Sun, Rongrong Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a novel parameter and computation efficient tuning
method for Multi-modal Large Language Models (MLLMs), termed Efficient
Attention Skipping (EAS). Concretely, we first reveal that multi-head
attentions (MHAs), the main computational overhead of MLLMs, are often
redundant to downstream tasks. Based on this observation, EAS evaluates the
attention redundancy and skips the less important MHAs to speed up inference.
Besides, we also propose a novel propagation-of-information adapter (PIA) to
serve the attention skipping of EAS and keep parameter efficiency, which can be
further re-parameterized into feed-forward networks (FFNs) for zero-extra
latency. To validate EAS, we apply it to a recently proposed MLLM called LaVIN
and a classic VL pre-trained model called METER, and conduct extensive
experiments on a set of benchmarks. The experiments show that EAS not only
retains high performance and parameter efficiency, but also greatly speeds up
inference speed. For instance, LaVIN-EAS can obtain 89.98\% accuracy on
ScineceQA while speeding up inference by 2.2 times to LaVIN
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Picture Is Worth a Graph: Blueprint Debate on Graph for Multimodal
  Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14972v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14972v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changmeng Zheng, Dayong Liang, Wengyu Zhang, Xiao-Yong Wei, Tat-Seng Chua, Qing Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a pilot study aimed at introducing multi-agent debate
into multimodal reasoning. The study addresses two key challenges: the
trivialization of opinions resulting from excessive summarization and the
diversion of focus caused by distractor concepts introduced from images. These
challenges stem from the inductive (bottom-up) nature of existing debating
schemes. To address the issue, we propose a deductive (top-down) debating
approach called Blueprint Debate on Graphs (BDoG). In BDoG, debates are
confined to a blueprint graph to prevent opinion trivialization through
world-level summarization. Moreover, by storing evidence in branches within the
graph, BDoG mitigates distractions caused by frequent but irrelevant concepts.
Extensive experiments validate BDoG, achieving state-of-the-art results in
Science QA and MMBench with significant improvements over previous methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MSAC: Multiple Speech Attribute Control Method for Reliable Speech
  Emotion Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.04025v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.04025v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Pan, Yuguang Yang, Yuheng Huang, Jixun Yao, Jingjing Yin, Yanni Hu, Heng Lu, Lei Ma, Jianjun Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite notable progress, speech emotion recognition (SER) remains
challenging due to the intricate and ambiguous nature of speech emotion,
particularly in wild world. While current studies primarily focus on
recognition and generalization abilities, our research pioneers an
investigation into the reliability of SER methods in the presence of semantic
data shifts and explores how to exert fine-grained control over various
attributes inherent in speech signals to enhance speech emotion modeling. In
this paper, we first introduce MSAC-SERNet, a novel unified SER framework
capable of simultaneously handling both single-corpus and cross-corpus SER.
Specifically, concentrating exclusively on the speech emotion attribute, a
novel CNN-based SER model is presented to extract discriminative emotional
representations, guided by additive margin softmax loss. Considering
information overlap between various speech attributes, we propose a novel
learning paradigm based on correlations of different speech attributes, termed
Multiple Speech Attribute Control (MSAC), which empowers the proposed SER model
to simultaneously capture fine-grained emotion-related features while
mitigating the negative impact of emotion-agnostic representations.
Furthermore, we make a first attempt to examine the reliability of the
MSAC-SERNet framework using out-of-distribution detection methods. Experiments
on both single-corpus and cross-corpus SER scenarios indicate that MSAC-SERNet
not only consistently outperforms the baseline in all aspects, but achieves
superior performance compared to state-of-the-art SER approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FunQA: Towards Surprising Video Comprehension 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.14899v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.14899v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Binzhu Xie, Sicheng Zhang, Zitang Zhou, Bo Li, Yuanhan Zhang, Jack Hessel, Jingkang Yang, Ziwei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Surprising videos, such as funny clips, creative performances, or visual
illusions, attract significant attention. Enjoyment of these videos is not
simply a response to visual stimuli; rather, it hinges on the human capacity to
understand (and appreciate) commonsense violations depicted in these videos. We
introduce FunQA, a challenging video question-answering (QA) dataset
specifically designed to evaluate and enhance the depth of video reasoning
based on counter-intuitive and fun videos. Unlike most video QA benchmarks
which focus on less surprising contexts, e.g., cooking or instructional videos,
FunQA covers three previously unexplored types of surprising videos: 1)
HumorQA, 2) CreativeQA, and 3) MagicQA. For each subset, we establish rigorous
QA tasks designed to assess the model's capability in counter-intuitive
timestamp localization, detailed video description, and reasoning around
counter-intuitiveness. We also pose higher-level tasks, such as attributing a
fitting and vivid title to the video and scoring the video creativity. In
total, the FunQA benchmark consists of 312K free-text QA pairs derived from
4.3K video clips, spanning a total of 24 video hours. Moreover, we propose
FunMentor, an agent designed for Vision-Language Models (VLMs) that uses
multi-turn dialogues to enhance models' understanding of counter-intuitiveness.
Extensive experiments with existing VLMs demonstrate the effectiveness of
FunMentor and reveal significant performance gaps for the FunQA videos across
spatial-temporal reasoning, visual-centered reasoning, and free-text
generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://funqa-benchmark.github.io/ Codebase:
  https://github.com/Jingkang50/FunQA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ToonAging: Face Re-Aging upon Artistic Portrait Style Transfer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.02733v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.02733v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bumsoo Kim, Abdul Muqeet, Kyuchul Lee, Sanghyun Seo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Face re-aging is a prominent field in computer vision and graphics, with
significant applications in photorealistic domains such as movies, advertising,
and live streaming. Recently, the need to apply face re-aging to
non-photorealistic images, like comics, illustrations, and animations, has
emerged as an extension in various entertainment sectors. However, the lack of
a network that can seamlessly edit the apparent age in NPR images has limited
these tasks to a naive, sequential approach. This often results in unpleasant
artifacts and a loss of facial attributes due to domain discrepancies. In this
paper, we introduce a novel one-stage method for face re-aging combined with
portrait style transfer, executed in a single generative step. We leverage
existing face re-aging and style transfer networks, both trained within the
same PR domain. Our method uniquely fuses distinct latent vectors, each
responsible for managing aging-related attributes and NPR appearance. By
adopting an exemplar-based approach, our method offers greater flexibility
compared to domain-level fine-tuning approaches, which typically require
separate training or fine-tuning for each domain. This effectively addresses
the limitation of requiring paired datasets for re-aging and domain-level,
data-driven approaches for stylization. Our experiments show that our model can
effortlessly generate re-aged images while simultaneously transferring the
style of examples, maintaining both natural appearance and controllability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 15 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Virbo: Multimodal Multilingual Avatar Video Generation in Digital
  Marketing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11700v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11700v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juan Zhang, Jiahao Chen, Cheng Wang, Zhiwang Yu, Tangquan Qi, Can Liu, Di Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the widespread popularity of internet celebrity marketing all over the
world, short video production has gradually become a popular way of presenting
products information. However, the traditional video production industry
usually includes series of procedures as script writing, video filming in a
professional studio, video clipping, special effects rendering, customized
post-processing, and so forth. Not to mention that multilingual videos is not
accessible for those who could not speak multilingual languages. These
complicated procedures usually needs a professional team to complete, and this
made short video production costly in both time and money. This paper presents
an intelligent system that supports the automatic generation of talking avatar
videos, namely Virbo. With simply a user-specified script, Virbo could use a
deep generative model to generate a target talking videos. Meanwhile, the
system also supports multimodal inputs to customize the video with specified
face, specified voice and special effects. This system also integrated a
multilingual customization module that supports generate multilingual talking
avatar videos in a batch with hundreds of delicate templates and creative
special effects. Through a series of user studies and demo tests, we found that
Virbo can generate talking avatar videos that maintained a high quality of
videos as those from a professional team while reducing the entire production
costs significantly. This intelligent system will effectively promote the video
production industry and facilitate the internet marketing neglecting of
language barriers and cost challenges.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AnyV2V: A Plug-and-Play Framework For Any Video-to-Video Editing Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14468v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14468v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Max Ku, Cong Wei, Weiming Ren, Harry Yang, Wenhu Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video-to-video editing involves editing a source video along with additional
control (such as text prompts, subjects, or styles) to generate a new video
that aligns with the source video and the provided control. Traditional methods
have been constrained to certain editing types, limiting their ability to meet
the wide range of user demands. In this paper, we introduce AnyV2V, a novel
training-free framework designed to simplify video editing into two primary
steps: (1) employing an off-the-shelf image editing model (e.g.
InstructPix2Pix, InstantID, etc) to modify the first frame, (2) utilizing an
existing image-to-video generation model (e.g. I2VGen-XL) for DDIM inversion
and feature injection. In the first stage, AnyV2V can plug in any existing
image editing tools to support an extensive array of video editing tasks.
Beyond the traditional prompt-based editing methods, AnyV2V also can support
novel video editing tasks, including reference-based style transfer,
subject-driven editing, and identity manipulation, which were unattainable by
previous methods. In the second stage, AnyV2V can plug in any existing
image-to-video models to perform DDIM inversion and intermediate feature
injection to maintain the appearance and motion consistency with the source
video. On the prompt-based editing, we show that AnyV2V can outperform the
previous best approach by 35\% on prompt alignment, and 25\% on human
preference. On the three novel tasks, we show that AnyV2V also achieves a high
success rate. We believe AnyV2V will continue to thrive due to its ability to
seamlessly integrate the fast-evolving image editing methods. Such
compatibility can help AnyV2V to increase its versatility to cater to diverse
user demands.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-21T00:00:00Z">2024-03-21</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating the Performance of LLMs on Technical Language Processing
  tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15503v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15503v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrew Kernycky, David Coleman, Christopher Spence, Udayan Das
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we present the results of an evaluation study of the
perfor-mance of LLMs on Technical Language Processing tasks. Humans are often
confronted with tasks in which they have to gather information from dispar-ate
sources and require making sense of large bodies of text. These tasks can be
significantly complex for humans and often require deep study including
rereading portions of a text. Towards simplifying the task of gathering
in-formation we evaluated LLMs with chat interfaces for their ability to
provide answers to standard questions that a human can be expected to answer
based on their reading of a body of text. The body of text under study is Title
47 of the United States Code of Federal Regulations (CFR) which describes
regula-tions for commercial telecommunications as governed by the Federal
Com-munications Commission (FCC). This has been a body of text of interest
be-cause our larger research concerns the issue of making sense of information
related to Wireless Spectrum Governance and usage in an automated manner to
support Dynamic Spectrum Access. The information concerning this wireless
spectrum domain is found in many disparate sources, with Title 47 of the CFR
being just one of many. Using a range of LLMs and providing the required CFR
text as context we were able to quantify the performance of those LLMs on the
specific task of answering the questions below.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Medical Support in the Arabic Language Through Personalized
  Chat<span class="highlight-title">GPT</span> Assistance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15501v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15501v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamed Issa, Ahmed Abdelwahed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This Paper discusses the growing popularity of online medical diagnosis as an
alternative to traditional doctor visits. It highlights the limitations of
existing tools and emphasizes the advantages of using ChatGPT, which provides
real-time, personalized medical diagnosis at no cost. The paragraph summarizes
a research study that evaluated the performance of ChatGPT in Arabic medical
diagnosis. The study involved compiling a dataset of disease information and
generating multiple messages for each disease using different prompting
techniques. ChatGPT's performance was assessed by measuring the similarity
between its responses and the actual diseases. The results showed promising
performance, with average scores of around 76% for similarity measures. Various
prompting techniques were used, and chain prompting demonstrated a relative
advantage. The study also recorded an average response time of 6.12 seconds for
the ChatGPT API, which is considered acceptable but has room for improvement.
While ChatGPT cannot replace human doctors entirely, the findings suggest its
potential in emergency cases and addressing general medical inquiries. Overall,
the study highlights ChatGPT's viability as a valuable tool in the medical
field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper was presented at The International conference for Arabic
  language and applied linguistics</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ gTBLS: Generating Tables from Text by Conditional Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14457v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14457v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anirudh Sundar, Christopher Richardson, Larry Heck
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Distilling large, unstructured text into a structured, condensed form such as
tables is an open research problem. One of the primary challenges in
automatically generating tables is ensuring their syntactic validity. Prior
approaches address this challenge by including additional parameters in the
Transformer's attention mechanism to attend to specific rows and column
headers. In contrast to this single-stage method, this paper presents a
two-stage approach called Generative Tables (gTBLS). The first stage infers
table structure (row and column headers) from the text. The second stage
formulates questions using these headers and fine-tunes a causal language model
to answer them. Furthermore, the gTBLS approach is amenable to the utilization
of pre-trained Large Language Models in a zero-shot configuration, presenting a
solution for table generation in situations where fine-tuning is not feasible.
gTBLS improves prior approaches by up to 10% in BERTScore on the table
construction task and up to 20% on the table content generation task of the
E2E, WikiTableText, WikiBio, and RotoWire datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Knowledge-Enhanced Recommendation with User-Centric Subgraph Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14377v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14377v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangyi Liu, Quanming Yao, Yongqi Zhang, Lei Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommendation systems, as widely implemented nowadays on various platforms,
recommend relevant items to users based on their preferences. The classical
methods which rely on user-item interaction matrices has limitations,
especially in scenarios where there is a lack of interaction data for new
items. Knowledge graph (KG)-based recommendation systems have emerged as a
promising solution. However, most KG-based methods adopt node embeddings, which
do not provide personalized recommendations for different users and cannot
generalize well to the new items. To address these limitations, we propose
Knowledge-enhanced User-Centric subgraph Network (KUCNet), a subgraph learning
approach with graph neural network (GNN) for effective recommendation. KUCNet
constructs a U-I subgraph for each user-item pair that captures both the
historical information of user-item interactions and the side information
provided in KG. An attention-based GNN is designed to encode the U-I subgraphs
for recommendation. Considering efficiency, the pruned user-centric computation
graph is further introduced such that multiple U-I subgraphs can be
simultaneously computed and that the size can be pruned by Personalized
PageRank. Our proposed method achieves accurate, efficient, and interpretable
recommendations especially for new items. Experimental results demonstrate the
superiority of KUCNet over state-of-the-art KG-based and collaborative
filtering (CF)-based methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FIT-RAG: Black-Box RAG with Factual Information and Token Reduction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14374v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14374v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuren Mao, Xuemei Dong, Wenyi Xu, Yunjun Gao, Bin Wei, Ying Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to the extraordinarily large number of parameters, fine-tuning Large
Language Models (LLMs) to update long-tail or out-of-date knowledge is
impractical in lots of applications. To avoid fine-tuning, we can alternatively
treat a LLM as a black-box (i.e., freeze the parameters of the LLM) and augment
it with a Retrieval-Augmented Generation (RAG) system, namely black-box RAG.
Recently, black-box RAG has achieved success in knowledge-intensive tasks and
has gained much attention. Existing black-box RAG methods typically fine-tune
the retriever to cater to LLMs' preferences and concatenate all the retrieved
documents as the input, which suffers from two issues: (1) Ignorance of Factual
Information. The LLM preferred documents may not contain the factual
information for the given question, which can mislead the retriever and hurt
the effectiveness of black-box RAG; (2) Waste of Tokens. Simply concatenating
all the retrieved documents brings large amounts of unnecessary tokens for
LLMs, which degenerates the efficiency of black-box RAG. To address these
issues, this paper proposes a novel black-box RAG framework which utilizes the
factual information in the retrieval and reduces the number of tokens for
augmentation, dubbed FIT-RAG. FIT-RAG utilizes the factual information by
constructing a bi-label document scorer. Besides, it reduces the tokens by
introducing a self-knowledge recognizer and a sub-document-level token reducer.
FIT-RAG achieves both superior effectiveness and efficiency, which is validated
by extensive experiments across three open-domain question-answering datasets:
TriviaQA, NQ and PopQA. FIT-RAG can improve the answering accuracy of
Llama2-13B-Chat by 14.3\% on TriviaQA, 19.9\% on NQ and 27.5\% on PopQA,
respectively. Furthermore, it can save approximately half of the tokens on
average across the three datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding the Ranking Loss for Recommendation with Sparse User
  Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14144v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14144v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhutian Lin, Junwei Pan, Shangyu Zhang, Ximei Wang, Xi Xiao, Shudong Huang, Lei Xiao, Jie Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Click-through rate (CTR) prediction holds significant importance in the realm
of online advertising. While many existing approaches treat it as a binary
classification problem and utilize binary cross entropy (BCE) as the
optimization objective, recent advancements have indicated that combining BCE
loss with ranking loss yields substantial performance improvements. However,
the full efficacy of this combination loss remains incompletely understood. In
this paper, we uncover a new challenge associated with BCE loss in scenarios
with sparse positive feedback, such as CTR prediction: the gradient vanishing
for negative samples. Subsequently, we introduce a novel perspective on the
effectiveness of ranking loss in CTR prediction, highlighting its ability to
generate larger gradients on negative samples, thereby mitigating their
optimization issues and resulting in improved classification ability. Our
perspective is supported by extensive theoretical analysis and empirical
evaluation conducted on publicly available datasets. Furthermore, we
successfully deployed the ranking loss in Tencent's online advertising system,
achieving notable lifts of 0.70% and 1.26% in Gross Merchandise Value (GMV) for
two main scenarios. The code for our approach is openly accessible at the
following GitHub repository:
https://github.com/SkylerLinn/Understanding-the-Ranking-Loss.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ M3: A Multi-Task Mixed-Objective Learning Framework for Open-Domain
  Multi-Hop Dense Sentence Retrieval <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14074v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14074v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Bai, Anthony Colas, Christan Grant, Daisy Zhe Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent research, contrastive learning has proven to be a highly effective
method for representation learning and is widely used for dense retrieval.
However, we identify that relying solely on contrastive learning can lead to
suboptimal retrieval performance. On the other hand, despite many retrieval
datasets supporting various learning objectives beyond contrastive learning,
combining them efficiently in multi-task learning scenarios can be challenging.
In this paper, we introduce M3, an advanced recursive Multi-hop dense sentence
retrieval system built upon a novel Multi-task Mixed-objective approach for
dense text representation learning, addressing the aforementioned challenges.
Our approach yields state-of-the-art performance on a large-scale open-domain
fact verification benchmark dataset, FEVER. Code and data are available at:
https://github.com/TonyBY/M3
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EasyInstruct: An Easy-to-use Instruction Processing Framework for Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.03049v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.03049v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixin Ou, Ningyu Zhang, Honghao Gui, Ziwen Xu, Shuofei Qiao, Yida Xue, Runnan Fang, Kangwei Liu, Lei Li, Zhen Bi, Guozhou Zheng, Huajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, instruction tuning has gained increasing attention and
emerged as a crucial technique to enhance the capabilities of Large Language
Models (LLMs). To construct high-quality instruction datasets, many instruction
processing approaches have been proposed, aiming to achieve a delicate balance
between data quantity and data quality. Nevertheless, due to inconsistencies
that persist among various instruction processing methods, there is no standard
open-source instruction processing implementation framework available for the
community, which hinders practitioners from further developing and advancing.
To facilitate instruction processing research and development, we present
EasyInstruct, an easy-to-use instruction processing framework for LLMs, which
modularizes instruction generation, selection, and prompting, while also
considering their combination and interaction. EasyInstruct is publicly
released and actively maintained at https://github.com/zjunlp/EasyInstruct,
along with an online demo app and a demo video for quick-start, calling for
broader research centered on instruction data and synthetic data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project website: https://zjunlp.github.io/project/EasyInstruct Code:
  https://github.com/zjunlp/EasyInstruct Video: https://youtu.be/rfQOWYfziFo
  Demo: https://huggingface.co/spaces/zjunlp/EasyInstruct</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Discrete Semantic Tokenization for Deep CTR Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08206v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08206v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qijiong Liu, Hengchang Hu, Jiahao Wu, Jieming Zhu, Min-Yen Kan, Xiao-Ming Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Incorporating item content information into click-through rate (CTR)
prediction models remains a challenge, especially with the time and space
constraints of industrial scenarios. The content-encoding paradigm, which
integrates user and item encoders directly into CTR models, prioritizes space
over time. In contrast, the embedding-based paradigm transforms item and user
semantics into latent embeddings, subsequently caching them to optimize
processing time at the expense of space. In this paper, we introduce a new
semantic-token paradigm and propose a discrete semantic tokenization approach,
namely UIST, for user and item representation. UIST facilitates swift training
and inference while maintaining a conservative memory footprint. Specifically,
UIST quantizes dense embedding vectors into discrete tokens with shorter
lengths and employs a hierarchical mixture inference module to weigh the
contribution of each user--item token pair. Our experimental results on news
recommendation showcase the effectiveness and efficiency (about 200-fold space
compression) of UIST for CTR prediction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>TheWebConf 2024 accepted paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pushing the Limits: Concurrency Detection in Acyclic Sound Free-Choice
  Workflow Nets in $O(P^2 + T^2)$ 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.16097v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.16097v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas M. Prinz, Julien Klaus, Nick R. T. P. van Beest
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Concurrency is an important aspect of Petri nets to describe and simulate the
behavior of complex systems. Knowing which places and transitions could be
executed in parallel helps to understand nets and enables analysis techniques
and the computation of other properties, such as causality, exclusivity, etc..
All techniques based on concurrency detection depend on the efficiency of this
detection methodology. Kovalyov and Esparza have developed algorithms that
compute all concurrent places in $O\big((P+T)TP^2\big)$ for live and bounded
nets (where $P$ and $T$ are the numbers of places and transitions) and in
$O\big(P(P+T)^2\big)$ for live and bounded free-choice nets. Although these
algorithms have a reasonably good computational complexity, large numbers of
concurrent pairs of nodes may still lead to long computation times. This paper
complements the palette of concurrency detection algorithms with the Concurrent
Paths (CP) algorithm for sound free-choice workflow nets. The algorithm allows
parallelization and has a worst-case computational complexity of $O(P^2 + T^2)$
for acyclic nets and of $O(P^3 + PT^2)$ for cyclic nets. Although the
computational complexity of cyclic nets has not improved, the evaluation shows
the benefits of CP, especially, if the net contains many nodes in concurrency
relation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 14 figures, 5 algorithms</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TensorBank: Tensor Lakehouse for Foundation Model Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.02094v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.02094v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Romeo Kienzler, Leonardo Pondian Tizzei, Benedikt Blumenstiel, Zoltan Arnold Nagy, S. Karthik Mukkavilli, Johannes Schmude, Marcus Freitag, Michael Behrendt, Daniel Salles Civitarese, Naomi Simumba, Daiki Kimura, Hendrik Hamann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Storing and streaming high dimensional data for foundation model training
became a critical requirement with the rise of foundation models beyond natural
language. In this paper we introduce TensorBank, a petabyte scale tensor
lakehouse capable of streaming tensors from Cloud Object Store (COS) to GPU
memory at wire speed based on complex relational queries. We use Hierarchical
Statistical Indices (HSI) for query acceleration. Our architecture allows to
directly address tensors on block level using HTTP range reads. Once in GPU
memory, data can be transformed using PyTorch transforms. We provide a generic
PyTorch dataset type with a corresponding dataset factory translating
relational queries and requested transformations as an instance. By making use
of the HSI, irrelevant blocks can be skipped without reading them as those
indices contain statistics on their content at different hierarchical
resolution levels. This is an opinionated architecture powered by open
standards and making heavy use of open-source technology. Although, hardened
for production use using geospatial-temporal data, this architecture
generalizes to other use case like computer vision, computational neuroscience,
biological sequence analysis and more.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ StreamingT2V: Consistent, Dynamic, and Extendable Long Video Generation
  from Text 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14773v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14773v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Roberto Henschel, Levon Khachatryan, Daniil Hayrapetyan, Hayk Poghosyan, Vahram Tadevosyan, Zhangyang Wang, Shant Navasardyan, Humphrey Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-video diffusion models enable the generation of high-quality videos
that follow text instructions, making it easy to create diverse and individual
content. However, existing approaches mostly focus on high-quality short video
generation (typically 16 or 24 frames), ending up with hard-cuts when naively
extended to the case of long video synthesis. To overcome these limitations, we
introduce StreamingT2V, an autoregressive approach for long video generation of
80, 240, 600, 1200 or more frames with smooth transitions. The key components
are:(i) a short-term memory block called conditional attention module (CAM),
which conditions the current generation on the features extracted from the
previous chunk via an attentional mechanism, leading to consistent chunk
transitions, (ii) a long-term memory block called appearance preservation
module, which extracts high-level scene and object features from the first
video chunk to prevent the model from forgetting the initial scene, and (iii) a
randomized blending approach that enables to apply a video enhancer
autoregressively for infinitely long videos without inconsistencies between
chunks. Experiments show that StreamingT2V generates high motion amount. In
contrast, all competing image-to-video methods are prone to video stagnation
when applied naively in an autoregressive manner. Thus, we propose with
StreamingT2V a high-quality seamless text-to-long video generator that
outperforms competitors with consistency and motion. Our code will be available
at: https://github.com/Picsart-AI-Research/StreamingT2V
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://github.com/Picsart-AI-Research/StreamingT2V</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bringing Robots Home: The Rise of AI Robots in Consumer Electronics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14449v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14449v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haiwei Dong, Yang Liu, Ted Chu, Abdulmotaleb El Saddik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  On March 18, 2024, NVIDIA unveiled Project GR00T, a general-purpose
multimodal generative AI model designed specifically for training humanoid
robots. Preceding this event, Tesla's unveiling of the Optimus Gen 2 humanoid
robot on December 12, 2023, underscored the profound impact robotics is poised
to have on reshaping various facets of our daily lives. While robots have long
dominated industrial settings, their presence within our homes is a burgeoning
phenomenon. This can be attributed, in part, to the complexities of domestic
environments and the challenges of creating robots that can seamlessly
integrate into our daily routines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE Consumer Electronics Magazine</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ M3FAS: An Accurate and Robust MultiModal Mobile Face Anti-Spoofing
  System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.12831v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.12831v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenqi Kong, Kexin Zheng, Yibing Liu, Shiqi Wang, Anderson Rocha, Haoliang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Face presentation attacks (FPA), also known as face spoofing, have brought
increasing concerns to the public through various malicious applications, such
as financial fraud and privacy leakage. Therefore, safeguarding face
recognition systems against FPA is of utmost importance. Although existing
learning-based face anti-spoofing (FAS) models can achieve outstanding
detection performance, they lack generalization capability and suffer
significant performance drops in unforeseen environments. Many methodologies
seek to use auxiliary modality data (e.g., depth and infrared maps) during the
presentation attack detection (PAD) to address this limitation. However, these
methods can be limited since (1) they require specific sensors such as depth
and infrared cameras for data capture, which are rarely available on commodity
mobile devices, and (2) they cannot work properly in practical scenarios when
either modality is missing or of poor quality. In this paper, we devise an
accurate and robust MultiModal Mobile Face Anti-Spoofing system named M3FAS to
overcome the issues above. The primary innovation of this work lies in the
following aspects: (1) To achieve robust PAD, our system combines visual and
auditory modalities using three commonly available sensors: camera, speaker,
and microphone; (2) We design a novel two-branch neural network with three
hierarchical feature aggregation modules to perform cross-modal feature fusion;
(3). We propose a multi-head training strategy, allowing the model to output
predictions from the vision, acoustic, and fusion heads, resulting in a more
flexible PAD. Extensive experiments have demonstrated the accuracy, robustness,
and flexibility of M3FAS under various challenging experimental settings. The
source code and dataset are available at: https://github.com/ChenqiKONG/M3FAS/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Multimodal Cooperation via Fine-grained Modality Valuation <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.06255v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.06255v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yake Wei, Ruoxuan Feng, Zihe Wang, Di Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One primary topic of multimodal learning is to jointly incorporate
heterogeneous information from different modalities. However, most models often
suffer from unsatisfactory multimodal cooperation, which cannot jointly utilize
all modalities well. Some methods are proposed to identify and enhance the
worse learnt modality, but they are often hard to provide the fine-grained
observation of multimodal cooperation at sample-level with theoretical support.
Hence, it is essential to reasonably observe and improve the fine-grained
cooperation between modalities, especially when facing realistic scenarios
where the modality discrepancy could vary across different samples. To this
end, we introduce a sample-level modality valuation metric to evaluate the
contribution of each modality for each sample. Via modality valuation, we
observe that modality discrepancy indeed could be different at sample-level,
beyond the global contribution discrepancy at dataset-level. We further analyze
this issue and improve cooperation between modalities at sample-level by
enhancing the discriminative ability of low-contributing modalities in a
targeted manner. Overall, our methods reasonably observe the fine-grained
uni-modal contribution and achieve considerable improvement. The source code
and dataset are available at
\url{https://github.com/GeWu-Lab/Valuate-and-Enhance-Multimodal-Cooperation}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Bi-directional Attention Network for Image Super-Resolution Quality
  Assessment <span class="chip">ICME</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10406v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10406v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixiao Li, Xiaoyuan Yang, Jun Fu, Guanghui Yue, Wei Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There has emerged a growing interest in exploring efficient quality
assessment algorithms for image super-resolution (SR). However, employing deep
learning techniques, especially dual-branch algorithms, to automatically
evaluate the visual quality of SR images remains challenging. Existing SR image
quality assessment (IQA) metrics based on two-stream networks lack interactions
between branches. To address this, we propose a novel full-reference IQA
(FR-IQA) method for SR images. Specifically, producing SR images and evaluating
how close the SR images are to the corresponding HR references are separate
processes. Based on this consideration, we construct a deep Bi-directional
Attention Network (BiAtten-Net) that dynamically deepens visual attention to
distortions in both processes, which aligns well with the human visual system
(HVS). Experiments on public SR quality databases demonstrate the superiority
of our proposed BiAtten-Net over state-of-the-art quality assessment methods.
In addition, the visualization results and ablation study show the
effectiveness of bi-directional attention.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 3 figures, published to 2024 IEEE International Conference
  on Multimedia and Expo (ICME)</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-20T00:00:00Z">2024-03-20</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging High-Resolution Features for Improved Deep Hashing-based
  Image Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13747v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13747v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aymene Berriche, Mehdi Adjal Zakaria, Riyadh Baghdadi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep hashing techniques have emerged as the predominant approach for
efficient image retrieval. Traditionally, these methods utilize pre-trained
convolutional neural networks (CNNs) such as AlexNet and VGG-16 as feature
extractors. However, the increasing complexity of datasets poses challenges for
these backbone architectures in capturing meaningful features essential for
effective image retrieval. In this study, we explore the efficacy of employing
high-resolution features learned through state-of-the-art techniques for image
retrieval tasks. Specifically, we propose a novel methodology that utilizes
High-Resolution Networks (HRNets) as the backbone for the deep hashing task,
termed High-Resolution Hashing Network (HHNet). Our approach demonstrates
superior performance compared to existing methods across all tested benchmark
datasets, including CIFAR-10, NUS-WIDE, MS COCO, and ImageNet. This performance
improvement is more pronounced for complex datasets, which highlights the need
to learn high-resolution features for intricate image retrieval tasks.
Furthermore, we conduct a comprehensive analysis of different HRNet
configurations and provide insights into the optimal architecture for the deep
hashing task
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Large Language Model Enhanced Sequential Recommender for Joint Video
  and Comment Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13574v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13574v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bowen Zheng, Zihan Lin, Enze Liu, Chen Yang, Enyang Bai, Cheng Ling, Wayne Xin Zhao, Ji-Rong Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In online video platforms, reading or writing comments on interesting videos
has become an essential part of the video watching experience. However,
existing video recommender systems mainly model users' interaction behaviors
with videos, lacking consideration of comments in user behavior modeling. In
this paper, we propose a novel recommendation approach called LSVCR by
leveraging user interaction histories with both videos and comments, so as to
jointly conduct personalized video and comment recommendation. Specifically,
our approach consists of two key components, namely sequential recommendation
(SR) model and supplemental large language model (LLM) recommender. The SR
model serves as the primary recommendation backbone (retained in deployment) of
our approach, allowing for efficient user preference modeling. Meanwhile, we
leverage the LLM recommender as a supplemental component (discarded in
deployment) to better capture underlying user preferences from heterogeneous
interaction behaviors. In order to integrate the merits of the SR model and the
supplemental LLM recommender, we design a twostage training paradigm. The first
stage is personalized preference alignment, which aims to align the preference
representations from both components, thereby enhancing the semantics of the SR
model. The second stage is recommendation-oriented fine-tuning, in which the
alignment-enhanced SR model is fine-tuned according to specific objectives.
Extensive experiments in both video and comment recommendation tasks
demonstrate the effectiveness of LSVCR. Additionally, online A/B testing on the
KuaiShou platform verifies the actual benefits brought by our approach. In
particular, we achieve a significant overall gain of 4.13% in comment watch
time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Unified Optimal Transport Framework for Cross-Modal Retrieval with
  Noisy Labels 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13480v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13480v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haochen Han, Minnan Luo, Huan Liu, Fang Nan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cross-modal retrieval (CMR) aims to establish interaction between different
modalities, among which supervised CMR is emerging due to its flexibility in
learning semantic category discrimination. Despite the remarkable performance
of previous supervised CMR methods, much of their success can be attributed to
the well-annotated data. However, even for unimodal data, precise annotation is
expensive and time-consuming, and it becomes more challenging with the
multimodal scenario. In practice, massive multimodal data are collected from
the Internet with coarse annotation, which inevitably introduces noisy labels.
Training with such misleading labels would bring two key challenges --
enforcing the multimodal samples to \emph{align incorrect semantics} and
\emph{widen the heterogeneous gap}, resulting in poor retrieval performance. To
tackle these challenges, this work proposes UOT-RCL, a Unified framework based
on Optimal Transport (OT) for Robust Cross-modal Retrieval. First, we propose a
semantic alignment based on partial OT to progressively correct the noisy
labels, where a novel cross-modal consistent cost function is designed to blend
different modalities and provide precise transport cost. Second, to narrow the
discrepancy in multi-modal data, an OT-based relation alignment is proposed to
infer the semantic-level cross-modal matching. Both of these two components
leverage the inherent correlation among multi-modal data to facilitate
effective cost function. The experiments on three widely-used cross-modal
retrieval datasets demonstrate that our UOT-RCL surpasses the state-of-the-art
approaches and significantly improves the robustness against noisy labels.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DESIRE-ME: Domain-Enhanced Supervised Information REtrieval using
  Mixture-of-Experts <span class="chip">ECIR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13468v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13468v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pranav Kasela, Gabriella Pasi, Raffaele Perego, Nicola Tonellotto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Open-domain question answering requires retrieval systems able to cope with
the diverse and varied nature of questions, providing accurate answers across a
broad spectrum of query types and topics. To deal with such topic heterogeneity
through a unique model, we propose DESIRE-ME, a neural information retrieval
model that leverages the Mixture-of-Experts framework to combine multiple
specialized neural models. We rely on Wikipedia data to train an effective
neural gating mechanism that classifies the incoming query and that weighs the
predictions of the different domain-specific experts correspondingly. This
allows DESIRE-ME to specialize adaptively in multiple domains. Through
extensive experiments on publicly available datasets, we show that our proposal
can effectively generalize domain-enhanced neural models. DESIRE-ME excels in
handling open-domain questions adaptively, boosting by up to 12% in NDCG@10 and
22% in P@1, the underlying state-of-the-art dense retrieval model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the 46th European Conference on Information Retrieval
  (ECIR 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ USE: Dynamic User Modeling with Stateful Sequence Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13344v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13344v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhihan Zhou, Qixiang Fang, Leonardo Neves, Francesco Barbieri, Yozen Liu, Han Liu, Maarten W. Bos, Ron Dotsch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  User embeddings play a crucial role in user engagement forecasting and
personalized services. Recent advances in sequence modeling have sparked
interest in learning user embeddings from behavioral data. Yet behavior-based
user embedding learning faces the unique challenge of dynamic user modeling. As
users continuously interact with the apps, user embeddings should be
periodically updated to account for users' recent and long-term behavior
patterns. Existing methods highly rely on stateless sequence models that lack
memory of historical behavior. They have to either discard historical data and
use only the most recent data or reprocess the old and new data jointly. Both
cases incur substantial computational overhead. To address this limitation, we
introduce User Stateful Embedding (USE). USE generates user embeddings and
reflects users' evolving behaviors without the need for exhaustive reprocessing
by storing previous model states and revisiting them in the future.
Furthermore, we introduce a novel training objective named future W-behavior
prediction to transcend the limitations of next-token prediction by forecasting
a broader horizon of upcoming user behaviors. By combining it with the Same
User Prediction, a contrastive learning-based objective that predicts whether
different segments of behavior sequences belong to the same user, we further
improve the embeddings' distinctiveness and representativeness. We conducted
experiments on 8 downstream tasks using Snapchat users' behavioral logs in both
static (i.e., fixed user behavior sequences) and dynamic (i.e., periodically
updated user behavior sequences) settings. We demonstrate USE's superior
performance over established baselines. The results underscore USE's
effectiveness and efficiency in integrating historical and recent user behavior
sequences into user embeddings in dynamic user modeling.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Harnessing Large Language Models for Text-Rich Sequential Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13325v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13325v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhi Zheng, Wenshuo Chao, Zhaopeng Qiu, Hengshu Zhu, Hui Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in Large Language Models (LLMs) have been changing the
paradigm of Recommender Systems (RS). However, when items in the recommendation
scenarios contain rich textual information, such as product descriptions in
online shopping or news headlines on social media, LLMs require longer texts to
comprehensively depict the historical user behavior sequence. This poses
significant challenges to LLM-based recommenders, such as over-length
limitations, extensive time and space overheads, and suboptimal model
performance. To this end, in this paper, we design a novel framework for
harnessing Large Language Models for Text-Rich Sequential Recommendation
(LLM-TRSR). Specifically, we first propose to segment the user historical
behaviors and subsequently employ an LLM-based summarizer for summarizing these
user behavior blocks. Particularly, drawing inspiration from the successful
application of Convolutional Neural Networks (CNN) and Recurrent Neural
Networks (RNN) models in user modeling, we introduce two unique summarization
techniques in this paper, respectively hierarchical summarization and recurrent
summarization. Then, we construct a prompt text encompassing the user
preference summary, recent user interactions, and candidate item information
into an LLM-based recommender, which is subsequently fine-tuned using
Supervised Fine-Tuning (SFT) techniques to yield our final recommendation
model. We also use Low-Rank Adaptation (LoRA) for Parameter-Efficient
Fine-Tuning (PEFT). We conduct experiments on two public datasets, and the
results clearly demonstrate the effectiveness of our approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Flickr30K-CFQ: A Compact and Fragmented Query <span class="highlight-title">Dataset</span> for Text-image
  Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13317v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13317v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyu Liu, Yaoxian Song, Xuwu Wang, Zhu Xiangru, Zhixu Li, Wei Song, Tiefeng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the explosive growth of multi-modal information on the Internet,
unimodal search cannot satisfy the requirement of Internet applications.
Text-image retrieval research is needed to realize high-quality and efficient
retrieval between different modalities. Existing text-image retrieval research
is mostly based on general vision-language datasets (e.g. MS-COCO, Flickr30K),
in which the query utterance is rigid and unnatural (i.e. verbosity and
formality). To overcome the shortcoming, we construct a new Compact and
Fragmented Query challenge dataset (named Flickr30K-CFQ) to model text-image
retrieval task considering multiple query content and style, including compact
and fine-grained entity-relation corpus. We propose a novel query-enhanced
text-image retrieval method using prompt engineering based on LLM. Experiments
show that our proposed Flickr30-CFQ reveals the insufficiency of existing
vision-language datasets in realistic text-image tasks. Our LLM-based
Query-enhanced method applied on different existing text-image retrieval models
improves query understanding performance both on public dataset and our
challenge set Flickr30-CFQ with over 0.9% and 2.4% respectively. Our project
can be available anonymously in https://sites.google.com/view/Flickr30K-cfq.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Semantic Search Engine for Mathlib4 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13310v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13310v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guoxiong Gao, Haocheng Ju, Jiedong Jiang, Zihan Qin, Bin Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The interactive theorem prover, Lean, enables the verification of formal
mathematical proofs and is backed by an expanding community. Central to this
ecosystem is its mathematical library, mathlib4, which lays the groundwork for
the formalization of an expanding range of mathematical theories. However,
searching for theorems in mathlib4 can be challenging. To successfully search
in mathlib4, users often need to be familiar with its naming conventions or
documentation strings. Therefore, creating a semantic search engine that can be
used easily by individuals with varying familiarity with mathlib4 is very
important. In this paper, we present a semantic search engine for mathlib4 that
accepts informal queries and finds the relevant theorems. We also establish a
benchmark for assessing the performance of various search engines for mathlib4.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Analysis on Matching Mechanisms and Token Pruning for
  Late-interaction Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13291v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13291v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qi Liu, Gang Guo, Jiaxin Mao, Zhicheng Dou, Ji-Rong Wen, Hao Jiang, Xinyu Zhang, Zhao Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the development of pre-trained language models, the dense retrieval
models have become promising alternatives to the traditional retrieval models
that rely on exact match and sparse bag-of-words representations. Different
from most dense retrieval models using a bi-encoder to encode each query or
document into a dense vector, the recently proposed late-interaction
multi-vector models (i.e., ColBERT and COIL) achieve state-of-the-art retrieval
effectiveness by using all token embeddings to represent documents and queries
and modeling their relevance with a sum-of-max operation. However, these
fine-grained representations may cause unacceptable storage overhead for
practical search systems. In this study, we systematically analyze the matching
mechanism of these late-interaction models and show that the sum-of-max
operation heavily relies on the co-occurrence signals and some important words
in the document. Based on these findings, we then propose several simple
document pruning methods to reduce the storage overhead and compare the
effectiveness of different pruning methods on different late-interaction
models. We also leverage query pruning methods to further reduce the retrieval
latency. We conduct extensive experiments on both in-domain and out-domain
datasets and show that some of the used pruning methods can significantly
improve the efficiency of these late-interaction models without substantially
hurting their retrieval effectiveness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACM Transactions on Information Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Legal Case Retrieval with Brain Signals 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13242v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13242v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruizhe Zhang, Qingyao Ai, Ziyi Ye, Yueyue Wu, Xiaohui Xie, Yiqun Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The tasks of legal case retrieval have received growing attention from the IR
community in the last decade. Relevance feedback techniques with implicit user
feedback (e.g., clicks) have been demonstrated to be effective in traditional
search tasks (e.g., Web search). In legal case retrieval, however, collecting
relevance feedback faces a couple of challenges that are difficult to resolve
under existing feedback paradigms. First, legal case retrieval is a complex
task as users often need to understand the relationship between legal cases in
detail to correctly judge their relevance. Traditional feedback signal such as
clicks is too coarse to use as they do not reflect any fine-grained relevance
information. Second, legal case documents are usually long, users often need
even tens of minutes to read and understand them. Simple behavior signal such
as clicks and eye-tracking fixations can hardly be useful when users almost
click and examine every part of the document. In this paper, we explore the
possibility of solving the feedback problem in legal case retrieval with brain
signal. Recent advances in brain signal processing have shown that human
emotional can be collected in fine grains through Brain-Machine Interfaces
(BMI) without interrupting the users in their tasks. Therefore, we propose a
framework for legal case retrieval that uses EEG signal to optimize retrieval
results. We collected and create a legal case retrieval dataset with users EEG
signal and propose several methods to extract effective EEG features for
relevance feedback. Our proposed features achieve a 71% accuracy for feedback
prediction with an SVM-RFE model, and our proposed ranking method that takes
into account the diverse needs of users can significantly improve user
satisfaction for legal case retrieval. Experiment results show that re-ranked
result list make user more satisfied.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RecMind: Large Language Model Powered Agent For Recommendation <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.14296v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.14296v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yancheng Wang, Ziyan Jiang, Zheng Chen, Fan Yang, Yingxue Zhou, Eunah Cho, Xing Fan, Xiaojiang Huang, Yanbin Lu, Yingzhen Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While the recommendation system (RS) has advanced significantly through deep
learning, current RS approaches usually train and fine-tune models on
task-specific datasets, limiting their generalizability to new recommendation
tasks and their ability to leverage external knowledge due to model scale and
data size constraints. Thus, we designed an LLM-powered autonomous recommender
agent, RecMind, which is capable of leveraging external knowledge, utilizing
tools with careful planning to provide zero-shot personalized recommendations.
We propose a Self-Inspiring algorithm to improve the planning ability. At each
intermediate step, the LLM self-inspires to consider all previously explored
states to plan for the next step. This mechanism greatly improves the model's
ability to comprehend and utilize historical information in planning for
recommendation. We evaluate RecMind's performance in various recommendation
scenarios. Our experiment shows that RecMind outperforms existing zero/few-shot
LLM-based recommendation baseline methods in various tasks and achieves
comparable performance to a fully trained recommendation model P5.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NAACL 2024 (Findings)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Investigating the Effects of Sparse Attention on Cross-Encoders <span class="chip">ECIR'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.17649v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.17649v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ferdinand Schlatt, Maik Fröbe, Matthias Hagen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cross-encoders are effective passage and document re-rankers but less
efficient than other neural or classic retrieval models. A few previous studies
have applied windowed self-attention to make cross-encoders more efficient.
However, these studies did not investigate the potential and limits of
different attention patterns or window sizes. We close this gap and
systematically analyze how token interactions can be reduced without harming
the re-ranking effectiveness. Experimenting with asymmetric attention and
different window sizes, we find that the query tokens do not need to attend to
the passage or document tokens for effective re-ranking and that very small
window sizes suffice. In our experiments, even windows of 4 tokens still yield
effectiveness on par with previous cross-encoders while reducing the memory
requirements by at least 22% / 59% and being 1% / 43% faster at inference time
for passages / documents.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ECIR'24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ WaZI: A Learned and Workload-aware Z-Index <span class="chip">EDBT 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04268v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04268v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sachith Pai, Michael Mathioudakis, Yanhao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learned indexes fit machine learning (ML) models to the data and use them to
make query operations more time and space-efficient. Recent works propose using
learned spatial indexes to improve spatial query performance by optimizing the
storage layout or internal search structures according to the data
distribution. However, only a few learned indexes exploit the query workload
distribution to enhance their performance. In addition, building and updating
learned spatial indexes are often costly on large datasets due to the
inefficiency of (re)training ML models. In this paper, we present WaZI, a
learned and workload-aware variant of the Z-index, which jointly optimizes the
storage layout and search structures, as a viable solution for the above
challenges of spatial indexing. Specifically, we first formulate a cost
function to measure the performance of a Z-index on a dataset for a range-query
workload. Then, we optimize the Z-index structure by minimizing the cost
function through adaptive partitioning and ordering for index construction.
Moreover, we design a novel page-skipping mechanism to improve the query
performance of WaZI by reducing access to irrelevant data pages. Our extensive
experiments show that the WaZI index improves range query time by 40% on
average over the baselines while always performing better or comparably to
state-of-the-art spatial indexes. Additionally, it also maintains good point
query performance. Generally, WaZI provides favorable tradeoffs among query
latency, construction time, and index size.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Camera-ready version accepted to EDBT 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ERASE: Benchmarking Feature Selection Methods for Deep Recommender
  Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12660v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12660v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengyue Jia, Yejing Wang, Zhaocheng Du, Xiangyu Zhao, Yichao Wang, Bo Chen, Wanyu Wang, Huifeng Guo, Ruiming Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Recommender Systems (DRS) are increasingly dependent on a large number
of feature fields for more precise recommendations. Effective feature selection
methods are consequently becoming critical for further enhancing the accuracy
and optimizing storage efficiencies to align with the deployment demands. This
research area, particularly in the context of DRS, is nascent and faces three
core challenges. Firstly, variant experimental setups across research papers
often yield unfair comparisons, obscuring practical insights. Secondly, the
existing literature's lack of detailed analysis on selection attributes, based
on large-scale datasets and a thorough comparison among selection techniques
and DRS backbones, restricts the generalizability of findings and impedes
deployment on DRS. Lastly, research often focuses on comparing the peak
performance achievable by feature selection methods, an approach that is
typically computationally infeasible for identifying the optimal
hyperparameters and overlooks evaluating the robustness and stability of these
methods. To bridge these gaps, this paper presents ERASE, a comprehensive
bEnchmaRk for feAture SElection for DRS. ERASE comprises a thorough evaluation
of eleven feature selection methods, covering both traditional and deep
learning approaches, across four public datasets, private industrial datasets,
and a real-world commercial platform, achieving significant enhancement. Our
code is available online for ease of reproduction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLatrieval: LLM-Verified Retrieval for Verifiable Generation <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.07838v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.07838v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaonan Li, Changtai Zhu, Linyang Li, Zhangyue Yin, Tianxiang Sun, Xipeng Qiu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Verifiable generation aims to let the large language model (LLM) generate
text with supporting documents, which enables the user to flexibly verify the
answer and makes the LLM's output more reliable. Retrieval plays a crucial role
in verifiable generation. Specifically, the retrieved documents not only
supplement knowledge to help the LLM generate correct answers, but also serve
as supporting evidence for the user to verify the LLM's output. However, the
widely used retrievers become the bottleneck of the entire pipeline and limit
the overall performance. Their capabilities are usually inferior to LLMs since
they often have much fewer parameters than the large language model and have
not been demonstrated to scale well to the size of LLMs. If the retriever does
not correctly find the supporting documents, the LLM can not generate the
correct and verifiable answer, which overshadows the LLM's remarkable
abilities. To address these limitations, we propose \LLatrieval (Large Language
Model Verified Retrieval), where the LLM updates the retrieval result until it
verifies that the retrieved documents can sufficiently support answering the
question. Thus, the LLM can iteratively provide feedback to retrieval and
facilitate the retrieval result to fully support verifiable generation.
Experiments show that LLatrieval significantly outperforms extensive baselines
and achieves state-of-the-art results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NAACL 2024 (Main Conference)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Aligning and Training Framework for Multimodal Recommendations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12384v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12384v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Liu, Kangning Zhang, Xiangyuan Ren, Yanhua Huang, Jiarui Jin, Yingjie Qin, Ruilong Su, Ruiwen Xu, Weinan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the development of multimedia applications, multimodal recommendations
are playing an essential role, as they can leverage rich contexts beyond user
interactions. Existing methods mainly regard multimodal information as an
auxiliary, using them to help learn ID features; however, there exist semantic
gaps among multimodal content features and ID features, for which directly
using multimodal information as an auxiliary would lead to misalignment in
representations of users and items. In this paper, we first systematically
investigate the misalignment issue in multimodal recommendations, and propose a
solution named AlignRec. In AlignRec, the recommendation objective is
decomposed into three alignments, namely alignment within contents, alignment
between content and categorical ID, and alignment between users and items. Each
alignment is characterized by a specific objective function and is integrated
into our multimodal recommendation framework. To effectively train our
AlignRec, we propose starting from pre-training the first alignment to obtain
unified multimodal features and subsequently training the following two
alignments together with these features as input. As it is essential to analyze
whether each multimodal feature helps in training, we design three new classes
of metrics to evaluate intermediate performance. Our extensive experiments on
three real-world datasets consistently verify the superiority of AlignRec
compared to nine baselines. We also find that the multimodal features generated
by AlignRec are better than currently used ones, which are to be open-sourced.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, add some necessary explanations, revise typos</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ I^3 Retriever: Incorporating Implicit Interaction in <span class="highlight-title">Pre-train</span>ed
  Language Models for Passage Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02371v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02371v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qian Dong, Yiding Liu, Qingyao Ai, Haitao Li, Shuaiqiang Wang, Yiqun Liu, Dawei Yin, Shaoping Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Passage retrieval is a fundamental task in many information systems, such as
web search and question answering, where both efficiency and effectiveness are
critical concerns. In recent years, neural retrievers based on pre-trained
language models (PLM), such as dual-encoders, have achieved huge success. Yet,
studies have found that the performance of dual-encoders are often limited due
to the neglecting of the interaction information between queries and candidate
passages. Therefore, various interaction paradigms have been proposed to
improve the performance of vanilla dual-encoders. Particularly, recent
state-of-the-art methods often introduce late-interaction during the model
inference process. However, such late-interaction based methods usually bring
extensive computation and storage cost on large corpus. Despite their
effectiveness, the concern of efficiency and space footprint is still an
important factor that limits the application of interaction-based neural
retrieval models. To tackle this issue, we incorporate implicit interaction
into dual-encoders, and propose I^3 retriever. In particular, our implicit
interaction paradigm leverages generated pseudo-queries to simulate
query-passage interaction, which jointly optimizes with query and passage
encoders in an end-to-end manner. It can be fully pre-computed and cached, and
its inference process only involves simple dot product operation of the query
vector and passage vector, which makes it as efficient as the vanilla dual
encoders. We conduct comprehensive experiments on MSMARCO and TREC2019 Deep
Learning Datasets, demonstrating the I^3 retriever's superiority in terms of
both effectiveness and efficiency. Moreover, the proposed implicit interaction
is compatible with special pre-training and knowledge distillation for passage
retrieval, which brings a new state-of-the-art performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DanceCamera3D: 3D Camera Movement Synthesis with Music and Dance <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13667v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13667v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zixuan Wang, Jia Jia, Shikun Sun, Haozhe Wu, Rong Han, Zhenyu Li, Di Tang, Jiaqing Zhou, Jiebo Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Choreographers determine what the dances look like, while cameramen determine
the final presentation of dances. Recently, various methods and datasets have
showcased the feasibility of dance synthesis. However, camera movement
synthesis with music and dance remains an unsolved challenging problem due to
the scarcity of paired data. Thus, we present DCM, a new multi-modal 3D
dataset, which for the first time combines camera movement with dance motion
and music audio. This dataset encompasses 108 dance sequences (3.2 hours) of
paired dance-camera-music data from the anime community, covering 4 music
genres. With this dataset, we uncover that dance camera movement is
multifaceted and human-centric, and possesses multiple influencing factors,
making dance camera synthesis a more challenging task compared to camera or
dance synthesis alone. To overcome these difficulties, we propose
DanceCamera3D, a transformer-based diffusion model that incorporates a novel
body attention loss and a condition separation strategy. For evaluation, we
devise new metrics measuring camera movement quality, diversity, and dancer
fidelity. Utilizing these metrics, we conduct extensive experiments on our DCM
dataset, providing both quantitative and qualitative evidence showcasing the
effectiveness of our DanceCamera3D model. Code and video demos are available at
https://github.com/Carmenw1203/DanceCamera3D-Official.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accept to CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VSTAR: Generative Temporal Nursing for Longer Dynamic Video Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13501v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13501v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yumeng Li, William Beluch, Margret Keuper, Dan Zhang, Anna Khoreva
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite tremendous progress in the field of text-to-video (T2V) synthesis,
open-sourced T2V diffusion models struggle to generate longer videos with
dynamically varying and evolving content. They tend to synthesize quasi-static
videos, ignoring the necessary visual change-over-time implied in the text
prompt. At the same time, scaling these models to enable longer, more dynamic
video synthesis often remains computationally intractable. To address this
challenge, we introduce the concept of Generative Temporal Nursing (GTN), where
we aim to alter the generative process on the fly during inference to improve
control over the temporal dynamics and enable generation of longer videos. We
propose a method for GTN, dubbed VSTAR, which consists of two key ingredients:
1) Video Synopsis Prompting (VSP) - automatic generation of a video synopsis
based on the original single prompt leveraging LLMs, which gives accurate
textual guidance to different visual states of longer videos, and 2) Temporal
Attention Regularization (TAR) - a regularization technique to refine the
temporal attention units of the pre-trained T2V diffusion models, which enables
control over the video dynamics. We experimentally showcase the superiority of
the proposed approach in generating longer, visually appealing videos over
existing open-sourced T2V models. We additionally analyze the temporal
attention maps realized with and without VSTAR, demonstrating the importance of
applying our method to mitigate neglect of the desired visual change over time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://yumengli007.github.io/VSTAR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Unified Optimal Transport Framework for Cross-Modal Retrieval with
  Noisy Labels 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13480v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13480v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haochen Han, Minnan Luo, Huan Liu, Fang Nan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cross-modal retrieval (CMR) aims to establish interaction between different
modalities, among which supervised CMR is emerging due to its flexibility in
learning semantic category discrimination. Despite the remarkable performance
of previous supervised CMR methods, much of their success can be attributed to
the well-annotated data. However, even for unimodal data, precise annotation is
expensive and time-consuming, and it becomes more challenging with the
multimodal scenario. In practice, massive multimodal data are collected from
the Internet with coarse annotation, which inevitably introduces noisy labels.
Training with such misleading labels would bring two key challenges --
enforcing the multimodal samples to \emph{align incorrect semantics} and
\emph{widen the heterogeneous gap}, resulting in poor retrieval performance. To
tackle these challenges, this work proposes UOT-RCL, a Unified framework based
on Optimal Transport (OT) for Robust Cross-modal Retrieval. First, we propose a
semantic alignment based on partial OT to progressively correct the noisy
labels, where a novel cross-modal consistent cost function is designed to blend
different modalities and provide precise transport cost. Second, to narrow the
discrepancy in multi-modal data, an OT-based relation alignment is proposed to
infer the semantic-level cross-modal matching. Both of these two components
leverage the inherent correlation among multi-modal data to facilitate
effective cost function. The experiments on three widely-used cross-modal
retrieval datasets demonstrate that our UOT-RCL surpasses the state-of-the-art
approaches and significantly improves the robustness against noisy labels.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ IVAC-P2L: Leveraging Irregular Repetition Priors for Improving Video
  Action Counting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11959v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11959v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hang Wang, Zhi-Qi Cheng, Youtian Du, Lei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video Action Counting (VAC) is crucial in analyzing sports, fitness, and
everyday activities by quantifying repetitive actions in videos. However,
traditional VAC methods have overlooked the complexity of action repetitions,
such as interruptions and the variability in cycle duration. Our research
addresses the shortfall by introducing a novel approach to VAC, called
Irregular Video Action Counting (IVAC). IVAC prioritizes modeling irregular
repetition patterns in videos, which we define through two primary aspects:
Inter-cycle Consistency and Cycle-interval Inconsistency. Inter-cycle
Consistency ensures homogeneity in the spatial-temporal representations of
cycle segments, signifying action uniformity within cycles. Cycle-interval
inconsistency highlights the importance of distinguishing between cycle
segments and intervals based on their inherent content differences. To
encapsulate these principles, we propose a new methodology that includes
consistency and inconsistency modules, supported by a unique pull-push loss
(P2L) mechanism. The IVAC-P2L model applies a pull loss to promote coherence
among cycle segment features and a push loss to clearly distinguish features of
cycle segments from interval segments. Empirical evaluations conducted on the
RepCount dataset demonstrate that the IVAC-P2L model sets a new benchmark in
VAC task performance. Furthermore, the model demonstrates exceptional
adaptability and generalization across various video contents, outperforming
existing models on two additional datasets, UCFRep and Countix, without the
need for dataset-specific optimization. These results confirm the efficacy of
our approach in addressing irregular repetitions in videos and pave the way for
further advancements in video analysis and understanding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Source code: https://github.com/hwang-cs-ime/IVAC-P2L</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PiGW: A Plug-in Generative Watermarking Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12053v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12053v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Ma, Mengxi Guo, Li Yuming, Hengyuan Zhang, Cong Ma, Yuan Li, Xiaodong Xie, Shanghang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Integrating watermarks into generative images is a critical strategy for
protecting intellectual property and enhancing artificial intelligence
security. This paper proposes Plug-in Generative Watermarking (PiGW) as a
general framework for integrating watermarks into generative images. More
specifically, PiGW embeds watermark information into the initial noise using a
learnable watermark embedding network and an adaptive frequency spectrum mask.
Furthermore, it optimizes training costs by gradually increasing timesteps.
Extensive experiments demonstrate that PiGW enables embedding watermarks into
the generated image with negligible quality loss while achieving true
invisibility and high resistance to noise attacks. Moreover, PiGW can serve as
a plugin for various commonly used generative structures and multimodal
generative content types. Finally, we demonstrate how PiGW can also be utilized
for detecting generated images, contributing to the promotion of secure AI
development. The project code will be made available on GitHub.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Improve experimental content</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ICE: Interactive 3D Game Character Editing via Dialogue 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12667v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12667v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoqian Wu, Yunjie Wu, Zhipeng Hu, Lincheng Li, Weijie Chen, Rui Zhao, Changjie Fan, Xin Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-driven in-game 3D character auto-customization systems eliminate the
complicated process of manipulating intricate character control parameters.
However, current methods are limited by their single-round generation,
incapable of further editing and fine-grained modification. In this paper, we
propose an Interactive Character Editing framework (ICE) to achieve a
multi-round dialogue-based refinement process. In a nutshell, our ICE offers a
more user-friendly way to enable players to convey creative ideas iteratively
while ensuring that created characters align with the expectations of players.
Specifically, we propose an Instruction Parsing Module (IPM) that utilizes
large language models (LLMs) to parse multi-round dialogues into clear editing
instruction prompts in each round. To reliably and swiftly modify character
control parameters at a fine-grained level, we propose a Semantic-guided
Low-dimension Parameter Solver (SLPS) that edits character control parameters
according to prompts in a zero-shot manner. Our SLPS first localizes the
character control parameters related to the fine-grained modification, and then
optimizes the corresponding parameters in a low-dimension space to avoid
unrealistic results. Extensive experimental results demonstrate the
effectiveness of our proposed ICE for in-game character creation and the
superior editing performance of ICE. Project page: https://iceedit.github.io/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MIntRec2.0: A Large-scale Benchmark <span class="highlight-title">Dataset</span> for Multimodal Intent
  Recognition and Out-of-scope Detection in Conversations <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10943v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10943v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanlei Zhang, Xin Wang, Hua Xu, Qianrui Zhou, Kai Gao, Jianhua Su, jinyue Zhao, Wenrui Li, Yanting Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal intent recognition poses significant challenges, requiring the
incorporation of non-verbal modalities from real-world contexts to enhance the
comprehension of human intentions. Existing benchmark datasets are limited in
scale and suffer from difficulties in handling out-of-scope samples that arise
in multi-turn conversational interactions. We introduce MIntRec2.0, a
large-scale benchmark dataset for multimodal intent recognition in multi-party
conversations. It contains 1,245 dialogues with 15,040 samples, each annotated
within a new intent taxonomy of 30 fine-grained classes. Besides 9,304 in-scope
samples, it also includes 5,736 out-of-scope samples appearing in multi-turn
contexts, which naturally occur in real-world scenarios. Furthermore, we
provide comprehensive information on the speakers in each utterance, enriching
its utility for multi-party conversational research. We establish a general
framework supporting the organization of single-turn and multi-turn dialogue
data, modality feature extraction, multimodal fusion, as well as in-scope
classification and out-of-scope detection. Evaluation benchmarks are built
using classic multimodal fusion methods, ChatGPT, and human evaluators. While
existing methods incorporating nonverbal information yield improvements,
effectively leveraging context information and detecting out-of-scope samples
remains a substantial challenge. Notably, large language models exhibit a
significant performance gap compared to humans, highlighting the limitations of
machine learning methods in the cognitive intent understanding task. We believe
that MIntRec2.0 will serve as a valuable resource, providing a pioneering
foundation for research in human-machine conversational interactions, and
significantly facilitating related applications. The full dataset and codes are
available at https://github.com/thuiar/MIntRec2.0.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in ICLR 2024; The abstract is slightly modified due to the
  length limitation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Content-aware Masked Image Modeling <span class="highlight-title">Transformer</span> for Stereo Image
  Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08505v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08505v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinjie Zhang, Shenyuan Gao, Zhening Liu, Jiawei Shao, Xingtong Ge, Dailan He, Tongda Xu, Yan Wang, Jun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing learning-based stereo image codec adopt sophisticated transformation
with simple entropy models derived from single image codecs to encode latent
representations. However, those entropy models struggle to effectively capture
the spatial-disparity characteristics inherent in stereo images, which leads to
suboptimal rate-distortion results. In this paper, we propose a stereo image
compression framework, named CAMSIC. CAMSIC independently transforms each image
to latent representation and employs a powerful decoder-free Transformer
entropy model to capture both spatial and disparity dependencies, by
introducing a novel content-aware masked image modeling (MIM) technique. Our
content-aware MIM facilitates efficient bidirectional interaction between prior
information and estimated tokens, which naturally obviates the need for an
extra Transformer decoder. Experiments show that our stereo image codec
achieves state-of-the-art rate-distortion performance on two stereo image
datasets Cityscapes and InStereo2K with fast encoding and decoding speed.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-19T00:00:00Z">2024-03-19</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ InBox: Recommendation with Knowledge Graph using Interest Box Embedding <span class="chip">VLDB 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12649v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12649v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zezhong Xu, Yincen Qu, Wen Zhang, Lei Liang, Huajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge graphs (KGs) have become vitally important in modern recommender
systems, effectively improving performance and interpretability. Fundamentally,
recommender systems aim to identify user interests based on historical
interactions and recommend suitable items. However, existing works overlook two
key challenges: (1) an interest corresponds to a potentially large set of
related items, and (2) the lack of explicit, fine-grained exploitation of KG
information and interest connectivity. This leads to an inability to reflect
distinctions between entities and interests when modeling them in a single way.
Additionally, the granularity of concepts in the knowledge graphs used for
recommendations tends to be coarse, failing to match the fine-grained nature of
user interests. This homogenization limits the precise exploitation of
knowledge graph data and interest connectivity. To address these limitations,
we introduce a novel embedding-based model called InBox. Specifically, various
knowledge graph entities and relations are embedded as points or boxes, while
user interests are modeled as boxes encompassing interaction history.
Representing interests as boxes enables containing collections of item points
related to that interest. We further propose that an interest comprises diverse
basic concepts, and box intersection naturally supports concept combination.
Across three training steps, InBox significantly outperforms state-of-the-art
methods like HAKG and KGIN on recommendation tasks. Further analysis provides
meaningful insights into the variable value of different KG data for
recommendations. In summary, InBox advances recommender systems through
box-based interest and concept modeling for sophisticated knowledge graph
exploitation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>VLDB 2024 under submission</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Context-based Fast Recommendation Strategy for Long User Behavior
  Sequence in Meituan Waimai <span class="chip">WWW 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12566v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12566v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhichao Feng, Junjiie Xie, Kaiyuan Li, Yu Qin, Pengfei Wang, Qianzhong Li, Bin Yin, Xiang Li, Wei Lin, Shangguang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the recommender system of Meituan Waimai, we are dealing with
ever-lengthening user behavior sequences, which pose an increasing challenge to
modeling user preference effectively. Existing sequential recommendation models
often fail to capture long-term dependencies or are too complex, complicating
the fulfillment of Meituan Waimai's unique business needs. To better model user
interests, we consider selecting relevant sub-sequences from users' extensive
historical behaviors based on their preferences. In this specific scenario,
we've noticed that the contexts in which users interact have a significant
impact on their preferences. For this purpose, we introduce a novel method
called Context-based Fast Recommendation Strategy to tackle the issue of long
sequences. We first identify contexts that share similar user preferences with
the target context and then locate the corresponding PoIs based on these
identified contexts. This approach eliminates the necessity to select a
sub-sequence for every candidate PoI, thereby avoiding high time complexity.
Specifically, we implement a prototype-based approach to pinpoint contexts that
mirror similar user preferences. To amplify accuracy and interpretability, we
employ JS divergence of PoI attributes such as categories and prices as a
measure of similarity between contexts. A temporal graph integrating both
prototype and context nodes helps incorporate temporal information. We then
identify appropriate prototypes considering both target contexts and short-term
user preferences. Following this, we utilize contexts aligned with these
prototypes to generate a sub-sequence, aimed at predicting CTR and CTCVR scores
with target attention. Since its inception in 2023, this strategy has been
adopted in Meituan Waimai's display recommender system, leading to a 4.6% surge
in CTR and a 4.2% boost in GMV.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, accepted by WWW 2024 Industry Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Listwise Generative Retrieval Models via a Sequential Learning Process 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12499v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12499v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yubao Tang, Ruqing Zhang, Jiafeng Guo, Maarten de Rijke, Wei Chen, Xueqi Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, a novel generative retrieval (GR) paradigm has been proposed, where
a single sequence-to-sequence model is learned to directly generate a list of
relevant document identifiers (docids) given a query. Existing GR models
commonly employ maximum likelihood estimation (MLE) for optimization: this
involves maximizing the likelihood of a single relevant docid given an input
query, with the assumption that the likelihood for each docid is independent of
the other docids in the list. We refer to these models as the pointwise
approach in this paper. While the pointwise approach has been shown to be
effective in the context of GR, it is considered sub-optimal due to its
disregard for the fundamental principle that ranking involves making
predictions about lists. In this paper, we address this limitation by
introducing an alternative listwise approach, which empowers the GR model to
optimize the relevance at the docid list level. Specifically, we view the
generation of a ranked docid list as a sequence learning process: at each step
we learn a subset of parameters that maximizes the corresponding generation
likelihood of the $i$-th docid given the (preceding) top $i-1$ docids. To
formalize the sequence learning process, we design a positional conditional
probability for GR. To alleviate the potential impact of beam search on the
generation quality during inference, we perform relevance calibration on the
generation likelihood of model-generated docids according to relevance grades.
We conduct extensive experiments on representative binary and multi-graded
relevance datasets. Our empirical results demonstrate that our method
outperforms state-of-the-art GR baselines in terms of retrieval performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACM Transactions on Information Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interpretable User Satisfaction Estimation for Conversational Systems
  with Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12388v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12388v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ying-Chun Lin, Jennifer Neville, Jack W. Stokes, Longqi Yang, Tara Safavi, Mengting Wan, Scott Counts, Siddharth Suri, Reid Andersen, Xiaofeng Xu, Deepak Gupta, Sujay Kumar Jauhar, Xia Song, Georg Buscher, Saurabh Tiwary, Brent Hecht, Jaime Teevan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate and interpretable user satisfaction estimation (USE) is critical for
understanding, evaluating, and continuously improving conversational systems.
Users express their satisfaction or dissatisfaction with diverse conversational
patterns in both general-purpose (ChatGPT and Bing Copilot) and task-oriented
(customer service chatbot) conversational systems. Existing approaches based on
featurized ML models or text embeddings fall short in extracting generalizable
patterns and are hard to interpret. In this work, we show that LLMs can extract
interpretable signals of user satisfaction from their natural language
utterances more effectively than embedding-based approaches. Moreover, an LLM
can be tailored for USE via an iterative prompting framework using supervision
from labeled examples. The resulting method, Supervised Prompting for User
satisfaction Rubrics (SPUR), not only has higher accuracy but is more
interpretable as it scores user satisfaction via learned rubrics with a
detailed breakdown.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Semantic Consistency: Dirichlet Energy Driven Robust Multi-Modal
  Entity Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.17859v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.17859v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanyi Wang, Haifeng Sun, Jiabo Wang, Jingyu Wang, Wei Tang, Qi Qi, Shaoling Sun, Jianxin Liao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In Multi-Modal Knowledge Graphs (MMKGs), Multi-Modal Entity Alignment (MMEA)
is crucial for identifying identical entities across diverse modal attributes.
However, semantic inconsistency, mainly due to missing modal attributes, poses
a significant challenge. Traditional approaches rely on attribute
interpolation, but this often introduces modality noise, distorting the
original semantics. Moreover, the lack of a universal theoretical framework
limits advancements in achieving semantic consistency. This study introduces a
novel approach, DESAlign, which addresses these issues by applying a
theoretical framework based on Dirichlet energy to ensure semantic consistency.
We discover that semantic inconsistency leads to model overfitting to modality
noise, causing performance fluctuations, particularly when modalities are
missing. DESAlign innovatively combats over-smoothing and interpolates absent
semantics using existing modalities. Our approach includes a multi-modal
knowledge graph learning strategy and a propagation technique that employs
existing semantic features to compensate for missing ones, providing explicit
Euler solutions. Comprehensive evaluations across 60 benchmark splits,
including monolingual and bilingual scenarios, demonstrate that DESAlign
surpasses existing methods, setting a new standard in performance. Further
testing with high rates of missing modalities confirms its robustness, offering
an effective solution to semantic inconsistency in real-world MMKGs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2307.16210 by other authors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RA-Rec: An Efficient ID Representation Alignment Framework for LLM-based
  Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.04527v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.04527v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaohan Yu, Li Zhang, Xin Zhao, Yue Wang, Zhongrui Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLM) have recently emerged as a powerful tool for a
variety of natural language processing tasks, bringing a new surge of combining
LLM with recommendation systems, termed as LLM-based RS. Current approaches
generally fall into two main paradigms, the ID direct usage paradigm and the ID
translation paradigm, noting their core weakness stems from lacking
recommendation knowledge and uniqueness. To address this limitation, we propose
a new paradigm, ID representation, which incorporates pre-trained ID embeddings
into LLMs in a complementary manner. In this work, we present RA-Rec, an
efficient ID representation alignment framework for LLM-based recommendation,
which is compatible with multiple ID-based methods and LLM architectures.
Specifically, we treat ID embeddings as soft prompts and design an innovative
alignment module and an efficient tuning method with tailored data construction
for alignment. Extensive experiments demonstrate RA-Rec substantially
outperforms current state-of-the-art methods, achieving up to 3.0% absolute
HitRate@100 improvements while utilizing less than 10x training data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EasyEdit: An Easy-to-use Knowledge Editing Framework for Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.07269v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.07269v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peng Wang, Ningyu Zhang, Bozhong Tian, Zekun Xi, Yunzhi Yao, Ziwen Xu, Mengru Wang, Shengyu Mao, Xiaohan Wang, Siyuan Cheng, Kangwei Liu, Yuansheng Ni, Guozhou Zheng, Huajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) usually suffer from knowledge cutoff or fallacy
issues, which means they are unaware of unseen events or generate text with
incorrect facts owing to outdated/noisy data. To this end, many knowledge
editing approaches for LLMs have emerged -- aiming to subtly inject/edit
updated knowledge or adjust undesired behavior while minimizing the impact on
unrelated inputs. Nevertheless, due to significant differences among various
knowledge editing methods and the variations in task setups, there is no
standard implementation framework available for the community, which hinders
practitioners from applying knowledge editing to applications. To address these
issues, we propose EasyEdit, an easy-to-use knowledge editing framework for
LLMs. It supports various cutting-edge knowledge editing approaches and can be
readily applied to many well-known LLMs such as T5, GPT-J, LlaMA, etc.
Empirically, we report the knowledge editing results on LlaMA-2 with EasyEdit,
demonstrating that knowledge editing surpasses traditional fine-tuning in terms
of reliability and generalization. We have released the source code on GitHub,
along with Google Colab tutorials and comprehensive documentation for beginners
to get started. Besides, we present an online system for real-time knowledge
editing, and a demo video.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code: https://github.com/zjunlp/EasyEdit HF Demo:
  https://huggingface.co/spaces/zjunlp/EasyEdit Video:
  https://youtu.be/Gm6T0QaaskU Docs: https://zjunlp.gitbook.io/easyedit</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can LLM Substitute Human Labeling? A Case Study of Fine-grained Chinese
  Address Entity Recognition <span class="highlight-title">Dataset</span> for UAV Delivery <span class="chip">WWW'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06097v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06097v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Yao, Sichun Luo, Haohan Zhao, Guanzhi Deng, Linqi Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present CNER-UAV, a fine-grained \textbf{C}hinese \textbf{N}ame
\textbf{E}ntity \textbf{R}ecognition dataset specifically designed for the task
of address resolution in \textbf{U}nmanned \textbf{A}erial \textbf{V}ehicle
delivery systems. The dataset encompasses a diverse range of five categories,
enabling comprehensive training and evaluation of NER models. To construct this
dataset, we sourced the data from a real-world UAV delivery system and
conducted a rigorous data cleaning and desensitization process to ensure
privacy and data integrity. The resulting dataset, consisting of around 12,000
annotated samples, underwent human experts and \textbf{L}arge \textbf{L}anguage
\textbf{M}odel annotation. We evaluated classical NER models on our dataset and
provided in-depth analysis. The dataset and models are publicly available at
\url{https://github.com/zhhvvv/CNER-UAV}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by TheWebConf'24 (WWW'24) as a Resource Paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Establishing Performance Baselines in Fine-Tuning, Retrieval-Augmented
  Generation and Soft-<span class="highlight-title">Prompt</span>ing for Non-Specialist LLM Users 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.05903v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.05903v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jennifer Dodgson, Lin Nanzheng, Julian Peh, Akira Rafhael Janson Pattirane, Alfath Daryl Alhajir, Eko Ridho Dinarto, Joseph Lim, Syed Danyal Ahmad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Research into methods for improving the performance of large language models
(LLMs) through fine-tuning, retrieval-augmented generation (RAG) and
soft-prompting has tended to focus on the use of highly technical or high-cost
techniques, making many of the newly discovered approaches comparatively
inaccessible to non-technical users. In this paper we tested an unmodified
version of GPT 3.5, a fine-tuned version, and the same unmodified model when
given access to a vectorised RAG database, both in isolation and in combination
with a basic, non-algorithmic soft prompt. In each case we tested the model's
ability to answer a set of 100 questions relating primarily to events that
occurred after September 2021 (the point at which GPT 3.5's training data set
ends). We found that if commercial platforms are used and default settings are
applied with no iteration in order to establish a baseline set of outputs, a
fine-tuned model outperforms GPT 3.5 Turbo, while the RAG approach
out-performed both. The application of a soft prompt significantly improved the
performance of each approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, LaTeX; typos corrected, using the correct term 'system
  prompting' instead of 'soft prompting'</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring the Impact of Large Language Models on Recommender Systems: An
  Extensive <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.18590v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.18590v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arpita Vats, Vinija Jain, Rahul Raja, Aman Chadha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The paper underscores the significance of Large Language Models (LLMs) in
reshaping recommender systems, attributing their value to unique reasoning
abilities absent in traditional recommenders. Unlike conventional systems
lacking direct user interaction data, LLMs exhibit exceptional proficiency in
recommending items, showcasing their adeptness in comprehending intricacies of
language. This marks a fundamental paradigm shift in the realm of
recommendations. Amidst the dynamic research landscape, researchers actively
harness the language comprehension and generation capabilities of LLMs to
redefine the foundations of recommendation tasks. The investigation thoroughly
explores the inherent strengths of LLMs within recommendation frameworks,
encompassing nuanced contextual comprehension, seamless transitions across
diverse domains, adoption of unified approaches, holistic learning strategies
leveraging shared data reservoirs, transparent decision-making, and iterative
improvements. Despite their transformative potential, challenges persist,
including sensitivity to input prompts, occasional misinterpretations, and
unforeseen recommendations, necessitating continuous refinement and evolution
in LLM-driven recommender systems.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ WaterVG: Waterway Visual Grounding based on Text-Guided Vision and
  mmWave Radar 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12686v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12686v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Runwei Guan, Liye Jia, Fengyufan Yang, Shanliang Yao, Erick Purwanto, Xiaohui Zhu, Eng Gee Lim, Jeremy Smith, Ka Lok Man, Yutao Yue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The perception of waterways based on human intent holds significant
importance for autonomous navigation and operations of Unmanned Surface
Vehicles (USVs) in water environments. Inspired by visual grounding, in this
paper, we introduce WaterVG, the first visual grounding dataset designed for
USV-based waterway perception based on human intention prompts. WaterVG
encompasses prompts describing multiple targets, with annotations at the
instance level including bounding boxes and masks. Notably, WaterVG includes
11,568 samples with 34,950 referred targets, which integrates both visual and
radar characteristics captured by monocular camera and millimeter-wave (mmWave)
radar, enabling a finer granularity of text prompts. Furthermore, we propose a
novel multi-modal visual grounding model, Potamoi, which is a multi-modal and
multi-task model based on the one-stage paradigm with a designed Phased
Heterogeneous Modality Fusion (PHMF) structure, including Adaptive Radar
Weighting (ARW) and Multi-Head Slim Cross Attention (MHSCA). In specific, MHSCA
is a low-cost and efficient fusion module with a remarkably small parameter
count and FLOPs, elegantly aligning and fusing scenario context information
captured by two sensors with linguistic features, which can effectively address
tasks of referring expression comprehension and segmentation based on
fine-grained prompts. Comprehensive experiments and evaluations have been
conducted on WaterVG, where our Potamoi archives state-of-the-art performances
compared with counterparts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NewsCaption: Named-Entity aware Captioning for Out-of-Context Media 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12618v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12618v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anurag Singh, Shivangi Aneja
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the increasing influence of social media, online misinformation has
grown to become a societal issue. The motivation for our work comes from the
threat caused by cheapfakes, where an unaltered image is described using a news
caption in a new but false-context. The main challenge in detecting such
out-of-context multimedia is the unavailability of large-scale datasets.
Several detection methods employ randomly selected captions to generate
out-of-context training inputs. However, these randomly matched captions are
not truly representative of out-of-context scenarios due to inconsistencies
between the image description and the matched caption. We aim to address these
limitations by introducing a novel task of out-of-context caption generation.
In this work, we propose a new method that generates a realistic out-of-context
caption given visual and textual context. We also demonstrate that the
semantics of the generated captions can be controlled using the textual
context. We also evaluate our method against several baselines and our method
improves over the image captioning baseline by 6.2% BLUE-4, 2.96% CiDEr, 11.5%
ROUGE, and 7.3% METEOR
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mitigating Hallucination in Large Multi-Modal Models via Robust
  Instruction Tuning <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.14565v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.14565v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, Lijuan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the promising progress in multi-modal tasks, current large
multi-modal models (LMMs) are prone to hallucinating inconsistent descriptions
with respect to the associated image and human instructions. This paper
addresses this issue by introducing the first large and diverse visual
instruction tuning dataset, named Large-scale Robust Visual (LRV)-Instruction.
Our dataset comprises 400k visual instructions generated by GPT4, covering 16
vision-and-language tasks with open-ended instructions and answers. Unlike
existing studies that primarily focus on positive instruction samples, we
design LRV-Instruction to include both positive and negative instructions for
more robust visual instruction tuning. Our negative instructions are designed
at three semantic levels: (i) Nonexistent Object Manipulation, (ii) Existent
Object Manipulation and (iii) Knowledge Manipulation. To efficiently measure
the hallucination generated by LMMs, we propose GPT4-Assisted Visual
Instruction Evaluation (GAVIE), a stable approach to evaluate visual
instruction tuning like human experts. GAVIE does not require human-annotated
groundtruth answers and can adapt to diverse instruction formats. We conduct
comprehensive experiments to investigate the hallucination of LMMs. Our results
demonstrate existing LMMs exhibit significant hallucinations when presented
with our negative instructions, particularly Existent Object and Knowledge
Manipulation instructions. Moreover, we successfully mitigate hallucination by
finetuning MiniGPT4 and mPLUG-Owl on LRV-Instruction while improving
performance on several public datasets compared to state-of-the-art methods.
Additionally, we observed that a balanced ratio of positive and negative
instances in the training data leads to a more robust model. Code and data are
available at https://github.com/FuxiaoLiu/LRV-Instruction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>40 pages, 32 figures, ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Feature Extraction and Late Fusion Strategy for Audiovisual
  Emotional Mimicry Intensity Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11757v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11757v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun Yu, Wangyuan Zhu, Jichao Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present the solution to the Emotional Mimicry Intensity
(EMI) Estimation challenge, which is part of 6th Affective Behavior Analysis
in-the-wild (ABAW) Competition.The EMI Estimation challenge task aims to
evaluate the emotional intensity of seed videos by assessing them from a set of
predefined emotion categories (i.e., "Admiration", "Amusement",
"Determination", "Empathic Pain", "Excitement" and "Joy"). To tackle this
challenge, we extracted rich dual-channel visual features based on ResNet18 and
AUs for the video modality and effective single-channel features based on
Wav2Vec2.0 for the audio modality. This allowed us to obtain comprehensive
emotional features for the audiovisual modality. Additionally, leveraging a
late fusion strategy, we averaged the predictions of the visual and acoustic
models, resulting in a more accurate estimation of audiovisual emotional
mimicry intensity. Experimental results validate the effectiveness of our
approach, with the average Pearson's correlation Coefficient($\rho$) across the
6 emotion dimensionson the validation set achieving 0.3288.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AV-SUPERB: A Multi-Task Evaluation Benchmark for Audio-Visual
  Representation Models <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.10787v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.10787v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuan Tseng, Layne Berry, Yi-Ting Chen, I-Hsiang Chiu, Hsuan-Hao Lin, Max Liu, Puyuan Peng, Yi-Jen Shih, Hung-Yu Wang, Haibin Wu, Po-Yao Huang, Chun-Mao Lai, Shang-Wen Li, David Harwath, Yu Tsao, Shinji Watanabe, Abdelrahman Mohamed, Chi-Luen Feng, Hung-yi Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio-visual representation learning aims to develop systems with human-like
perception by utilizing correlation between auditory and visual information.
However, current models often focus on a limited set of tasks, and
generalization abilities of learned representations are unclear. To this end,
we propose the AV-SUPERB benchmark that enables general-purpose evaluation of
unimodal audio/visual and bimodal fusion representations on 7 datasets covering
5 audio-visual tasks in speech and audio processing. We evaluate 5 recent
self-supervised models and show that none of these models generalize to all
tasks, emphasizing the need for future study on improving universal model
performance. In addition, we show that representations may be improved with
intermediate-task fine-tuning and audio event classification with AudioSet
serves as a strong intermediate task. We release our benchmark with evaluation
code and a model submission platform to encourage further research in
audio-visual learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICASSP 2024; Evaluation Code:
  https://github.com/roger-tseng/av-superb Submission Platform:
  https://av.superbbenchmark.org</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>

</body>

<footer>
    <div>
        <time id="build-timestamp" datetime="2024-03-27T05:25:47.399003248Z">
            2024-03-27 05:25:47 UTC
        </time>
    </div>
</footer>
<script src="index.js"></script>
</html>
